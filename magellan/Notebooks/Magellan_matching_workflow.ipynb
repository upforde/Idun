{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.utils import *\n",
    "\n",
    "from dataprep.eda import create_report\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Prelude.</h3>\n",
    "This pipeline consists of the Magellan pipeline with intent of testing data. <br>\n",
    "The pipeline can be adjusted for real data, and hybrid data. <br>\n",
    "It takes into consideration GPT-2 data and Magellan data. <br>\n",
    "With blocking and manual sampling as its own pipeline, this workflow focuses <br>\n",
    "on getting the data to the matcher. Because of this, the blocking step is omitted. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for this pipeline...\n",
    "datasets_dir = r'C:\\Users\\aleks\\Desktop\\Master Thesis\\Py_Magellan\\DataSets\\movies1'\n",
    "\n",
    "\n",
    "# If we are testing synthethic data\n",
    "testing_synth_data = True\n",
    "if testing_synth_data:\n",
    "    # True = GPT\n",
    "    # False = CTGAN\n",
    "    generator_type = False\n",
    "    if generator_type:\n",
    "        name_of_synth_data = \"synth_data.txt\"\n",
    "        name_of_real_data = \"real_data.txt\"\n",
    "    else:\n",
    "        name_of_synth_data = \"synth_data.csv\"\n",
    "        name_of_real_data = \"real_data.csv\"\n",
    "else:\n",
    "    # If we are testing DITTO data\n",
    "    ditto_run = True\n",
    "    train_proportion = 0.7\n",
    "    if ditto_run:\n",
    "        name_of_data = \"data.txt\"\n",
    "    else:\n",
    "        name_of_table_A = \"imdb2.csv\"\n",
    "        name_of_table_B = \"rotten_tomatoes2.csv\"\n",
    "        name_of_table_C = \"sample_set.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load datasets.</h3>\n",
    "First, load two datasets from our dataset path. <br>\n",
    "We also include an ID generator for each line, as some of the datasets come without indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_synth_data:\n",
    "    synth_data_path = datasets_dir + os.sep + name_of_synth_data\n",
    "    real_data_path = datasets_dir + os.sep + name_of_real_data\n",
    "    if generator_type:\n",
    "        with open(synth_data_path, 'r', encoding='utf-8') as file:\n",
    "            synth_data = file.read()\n",
    "\n",
    "        with open(real_data_path, 'r', encoding='utf-8') as file:\n",
    "            real_data = file.read()\n",
    "        \n",
    "        table_A_synth, table_B_synth, truth_synth = ditto_reformater(synth_data)\n",
    "        # table1 = table_A.add_prefix(\"ltable_\")\n",
    "        # table2= table_B.add_prefix(\"rtable_\")\n",
    "        # table = pd.concat([table1, table2, table3], axis=1)\n",
    "        table_A_real, table_B_real, truth_real = ditto_reformater(real_data)\n",
    "\n",
    "        cutoff = len(table_A_synth.index)\n",
    "\n",
    "        table_A = table_A_synth.append(table_A_real, ignore_index=True)\n",
    "        table_B = table_B_synth.append(table_B_real, ignore_index=True)\n",
    "        truth = truth_synth.append(truth_real, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        table_synth = pd.read_csv(synth_data_path)\n",
    "        table_real = pd.read_csv(real_data_path)\n",
    "\n",
    "        cutoff = len(table_synth.index)\n",
    "        split_index = (len(table_synth.columns) - 1) / 2\n",
    "        table = table_synth.append(table_real, ignore_index=True)\n",
    "\n",
    "        truth = table.iloc[:,-1]\n",
    "        table = table.iloc[:,:-1]\n",
    "        table_A = table.iloc[:,:-split_index]\n",
    "        table_B = table.iloc[:,-split_index:]\n",
    "\n",
    "    A_meta = create_index_as_id_for_dataframe(table_A)\n",
    "    B_meta = create_index_as_id_for_dataframe(table_B)\n",
    "    em.set_key(A_meta, 'ID')\n",
    "    em.set_key(B_meta, 'ID')\n",
    "\n",
    "    table1 = A_meta.add_prefix(\"ltable_\")\n",
    "    table2 = B_meta.add_prefix(\"rtable_\")\n",
    "    table = pd.concat([table1, table2, truth], axis=1)\n",
    "else:\n",
    "    if ditto_run:\n",
    "        data_path = datasets_dir + os.sep + name_of_data\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "        \n",
    "        table_A, table_B, truth = ditto_reformater(data)\n",
    "        A_meta = create_index_as_id_for_dataframe(table_A)\n",
    "        B_meta = create_index_as_id_for_dataframe(table_B)\n",
    "        em.set_key(A_meta, 'ID')\n",
    "        em.set_key(B_meta, 'ID')\n",
    "\n",
    "        table1 = A_meta.add_prefix(\"ltable_\")\n",
    "        table2 = B_meta.add_prefix(\"rtable_\")\n",
    "        table = pd.concat([table1, table2, truth], axis=1)\n",
    "    else:\n",
    "        path_A = datasets_dir + os.sep + name_of_table_A\n",
    "        path_B = datasets_dir + os.sep + name_of_table_B\n",
    "\n",
    "        # Might need to remove the file reading from function and just read it here.\n",
    "        create_index_as_id(path_A)\n",
    "        create_index_as_id(path_B)\n",
    "        A_meta = em.read_csv_metadata(path_A, key='ID')\n",
    "        B_meta = em.read_csv_metadata(path_B,  key='ID')\n",
    "        em.set_key(A_meta, 'ID')\n",
    "        em.set_key(B_meta, 'ID')\n",
    "\n",
    "if testing_synth_data:\n",
    "    table_C_path = datasets_dir + os.sep + \"Table_C.csv\"\n",
    "    table.to_csv(table_C_path)\n",
    "else:\n",
    "    if ditto_run:\n",
    "        table_C_path = datasets_dir + os.sep + \"Table_C.csv\"\n",
    "        table.to_csv(table_C_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Down-sizing.</h3>\n",
    "Incase of the datasets being too large, we downsample the datasets before an official run.\n",
    "This can be commented out for production stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_meta, B_meta = em.down_sample(A_meta, B_meta, size=1000, y_param=1, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em._atypes1['Name'], em._atypes1['Genre'], em._atypes1['ReleaseDate'], em._atypes1['Creator'], em._atypes1['Director'])\n",
    "print(em._atypes2['Name'], em._atypes2['Genre'], em._atypes2['ReleaseDate'], em._atypes2['Creator'], em._atypes2['Director'])\n",
    "em._atypes1['Genre'] = 'str_bt_1w_5w'\n",
    "em._atypes2['Genre'] = 'str_bt_1w_5w'\n",
    "atypes1 = em.get_attr_types(A_meta)\n",
    "atypes2 = em.get_attr_types(B_meta)\n",
    "\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "block_c = em.get_attr_corres(A_meta, B_meta)\n",
    "\n",
    "# Then we use the command to see which columns are comparable.\n",
    "print(block_c)\n",
    "feature_table  = em.get_features(A_meta, B_meta, atypes1, atypes2, block_c, block_t, block_s )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4. Sample data to be labelled</h4>\n",
    "For our next step, we need to sample some data and label it accordingly for our matcher. <br>\n",
    "Sample set amount can vary, but these are to be our ground-truth forward. <br> <br> \n",
    "As of 2022, Magellan does not support proper GUI for labelling, so we save the sample set to a filepath and label it manually.  <br>\n",
    "Then, we load the dataset from its origin path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_C = datasets_dir + os.sep + 'Table_C.csv'\n",
    "A_meta.fillna(0)\n",
    "B_meta.fillna(0)\n",
    "C_meta = em.read_csv_metadata(path_C, key='_id',\n",
    "                                    fk_ltable='ltable_ID', fk_rtable='rtable_ID',\n",
    "                                    ltable=A_meta, rtable=B_meta)\n",
    "\n",
    "\n",
    "print(\"Number of matches: \" + str(sum(C_meta['Truth'] > 0)))\n",
    "print(\"Number of non-matches: \" + str(sum(C_meta['Truth'] < 1)))\n",
    "# create_report(C_meta).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5. Selecting and training a matcher on labelled data. </h4>\n",
    "Furthermore, we use the labelled data to train our matcher. <br> <br>\n",
    "\n",
    "1. We start off by splitting our labelled data into a training set, and a test set. <br>\n",
    "2. Secondly, we generate the set of features. For now, we will use py_entitymatchings native function to automatically generate these features. <br>\n",
    "3. Then, we convert the labeled data to feature vectors using the feature table. <br> \n",
    "4. We also handle missing data / NaN values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_synth_data:\n",
    "    dataset_train = C_meta.iloc[:cutoff,:]\n",
    "    dataset_test = C_meta.iloc[cutoff:,:]\n",
    "    # Update catalog\n",
    "    em.catalog.catalog_manager.init_properties(dataset_train)\n",
    "    em.catalog.catalog_manager.copy_properties(C_meta, dataset_train)\n",
    "\n",
    "    em.catalog.catalog_manager.init_properties(dataset_test)\n",
    "    em.catalog.catalog_manager.copy_properties(C_meta, dataset_test)\n",
    "else:\n",
    "    dataset_split = em.split_train_test(C_meta, train_proportion=train_proportion)\n",
    "    dataset_train = dataset_split['train']\n",
    "    dataset_test = dataset_split['test']\n",
    "\n",
    "\n",
    "\n",
    "# dataset_train = C_meta\n",
    "# dataset_test = C_meta2\n",
    "\n",
    "print(\" ========= Training =========\")\n",
    "print(\"Number of matches: \" + str(sum(dataset_train['Truth'] > 0)))\n",
    "print(\"Number of non-matches: \" + str(sum(dataset_train['Truth'] < 1)))\n",
    "print(\" ========= Validation =========\")\n",
    "print(\"Number of matches: \" + str(sum(dataset_test['Truth'] > 0)))\n",
    "print(\"Number of non-matches: \" + str(sum(dataset_test['Truth'] < 1)))\n",
    "\n",
    "feature_table_train = em.get_features_for_matching(A_meta, B_meta, validate_inferred_attr_types=False)\n",
    "\n",
    "# Convert the labeled data to feature vectors using the feature table\n",
    "feature_vector_table_train = em.extract_feature_vecs(dataset_train, \n",
    "                            feature_table=feature_table_train, \n",
    "                            attrs_after='Truth',\n",
    "                            show_progress=False)\n",
    "\n",
    "# Handle missing data. Inplace is required to keep correct object type, and maintaining \"Meta-Data\".\n",
    "feature_vector_table_train.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.1. Selecting the best matcher.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree')\n",
    "svm = em.SVMMatcher(name='SVM')\n",
    "rf = em.RFMatcher(name='RF')\n",
    "lg = em.LogRegMatcher(name='LogReg')\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best ML matcher using Cross-Validation\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=feature_vector_table_train, \n",
    "        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID'],\n",
    "        k=5,\n",
    "        target_attr='Truth', metric_to_select_matcher='precision') # Can be switched out for 'recall'?\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose Random-Forest for testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.2. Training the selected matcher.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RF Matcher\n",
    "rf = em.RFMatcher()\n",
    "\n",
    "# Get the attributes to be projected while training\n",
    "attrs_to_be_excluded = []\n",
    "attrs_to_be_excluded.extend(['_id', 'ltable_ID', 'rtable_ID'])\n",
    "\n",
    "# Train using feature vectors from the labeled data.\n",
    "rf.fit(table=feature_vector_table_train, exclude_attrs=attrs_to_be_excluded, target_attr='Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6. Predict the matches on the evaluation set using the trained matcher.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_table_test = em.extract_feature_vecs(dataset_test, feature_table=feature_table,\n",
    "                             attrs_after='Truth',\n",
    "                             show_progress=False)\n",
    "\n",
    "# Handle NaN values.\n",
    "feature_vector_table_test.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attributes to be excluded while predicting\n",
    "attrs_to_be_excluded = []\n",
    "attrs_to_be_excluded.extend(['_id', 'ltable_ID', 'rtable_ID', 'Truth'])\n",
    "\n",
    "# Predict the matches\n",
    "predictions = rf.predict(table=feature_vector_table_test, exclude_attrs=attrs_to_be_excluded,                          \n",
    "              append=True, target_attr='predicted', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Project the attributes\n",
    "predictions.head(10)\n",
    "\n",
    "# Show only rows which are predicted as \"matched\".\n",
    "df_only_matched = predictions.loc[predictions['predicted'] == 1]\n",
    "df_only_matched.head(100)\n",
    "\n",
    "# Evaluate prediction results.\n",
    "eval_result = em.eval_matches(predictions, 'Truth', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2058fc30687b0128d95a989c948081aa14c6a2edcd16aff3f04d2286aad1f775"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
