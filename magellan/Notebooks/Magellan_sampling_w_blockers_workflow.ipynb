{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.utils import *\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Prelude.</h3>\n",
    "This notebook is for the pipeline which includes EM's blocking and sampling step. <br>\n",
    "In this notebook, we instantiate two tables and load them into Magellans workframe. <br>\n",
    "Thereafter we introduce blockers. Some blockers might be more prevelant than others. <br>\n",
    "Lastly, we save the candidate set to be labelled manually by the developer. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for this pipeline...\n",
    "datasets_dir = r'C:\\Users\\aleks\\Desktop\\Master Thesis\\Py_Magellan\\DataSets\\movies1'\n",
    "name_of_table_A = \"imdb2.csv\"\n",
    "name_of_table_B = \"rotten_tomatoes2.csv\"\n",
    "name_of_sample_set = \"sample_set.csv\"\n",
    "sample_set_size = 450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load datasets.</h3>\n",
    "First, load two datasets from our dataset path. <br>\n",
    "We also include an ID generator for each line, as some of the datasets come without indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the input tables\n",
    "path_A = datasets_dir + os.sep + name_of_table_A\n",
    "path_B = datasets_dir + os.sep + name_of_table_B\n",
    "\n",
    "# If the datasets do not contain a numerical ID, we create it.\n",
    "create_index_as_id(path_A)\n",
    "create_index_as_id(path_B)\n",
    "\n",
    "# We read in the tables data and set the ID column as keys.\n",
    "A_meta = em.read_csv_metadata(path_A, key='ID')\n",
    "B_meta = em.read_csv_metadata(path_B,  key='ID')\n",
    "\n",
    "# To be sure, we set it twice. We see `ID` is the key attribute (since it contains unique values and no value is missing) for the table. We can set this metadata as follows:\n",
    "em.set_key(A_meta, 'ID')\n",
    "em.set_key(B_meta, 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Down-sizing.</h3>\n",
    "Incase of the datasets being too large, we downsample the datasets before a production run.\n",
    "This can be commented out for production stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_meta, B_meta = em.down_sample(A_meta, B_meta, size=1000, y_param=1, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Blockers!</h3>\n",
    "Furthermore, we do blockers to signify which entities do not match by absolution.<br>\n",
    "We will do: <br>\n",
    "Name - Rule-Based Blocker <br>\n",
    "Genre - Overlap Blocker <br>\n",
    "Year R - BlackBox Blocker <br>\n",
    "Director - Attribute Blocker <br>\n",
    "<br><br>\n",
    "To start off, we assure that the attributes we are interested in comparing on are of the same data type. <br> <br>\n",
    "\n",
    "(In hindsight, if we use a Deep Learning method for our Matcher, most blocking steps become redundant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table  = em.get_features_for_blocking(A_meta, B_meta, validate_inferred_attr_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em._atypes1['Name'], em._atypes1['Genre'], em._atypes1['Year'], em._atypes1['Director'])\n",
    "print(em._atypes2['Name'], em._atypes2['Genre'], em._atypes2['Year'], em._atypes2['Director'])\n",
    "em._atypes1['Genre'] = 'str_bt_1w_5w'\n",
    "em._atypes2['Genre'] = 'str_bt_1w_5w'\n",
    "em._atypes1['Year'] = 'str_bt_1w_5w'\n",
    "em._atypes2['Year'] = 'str_bt_1w_5w'\n",
    "print(em._atypes1['Name'], em._atypes1['Genre'], em._atypes1['Year'], em._atypes1['Director'])\n",
    "print(em._atypes2['Name'], em._atypes2['Genre'], em._atypes2['Year'], em._atypes2['Director'])\n",
    "\n",
    "# Then we use the command to see which columns are comparable.\n",
    "print(em._block_c['corres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.1 Rule-Based blocker on \"Name\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rb = em.RuleBasedBlocker()\n",
    "# rb.add_rule(['Name_Name_lev_sim(ltuple, rtuple) < 0.6'], feature_table)\n",
    "# canditate_set_RB = rb.block_tables(A_meta, B_meta, \n",
    "#                     l_output_attrs=['Name', 'Genre', 'Year', 'Director'],\n",
    "#                     r_output_attrs=['Name', 'Genre', 'Year', 'Director'],\n",
    "#                     show_progress=False)\n",
    "\n",
    "\n",
    "# NOTE: Time: >1h for full dataset. 1m 50m for 1000. So we only just an Overlap blocker for now.\n",
    "\n",
    "ob = em.OverlapBlocker()\n",
    "canditate_set_RB = ob.block_tables(A_meta, B_meta, 'Name', 'Name', word_level=True, overlap_size=3, \n",
    "                    l_output_attrs=['Name', 'Genre', 'ReleaseDate', 'Director', 'Creator'], \n",
    "                    r_output_attrs=['Name', 'Genre', 'ReleaseDate', 'Director', 'Creator'],\n",
    "                    show_progress=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2 Overlap blocker on \"Genre\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = em.OverlapBlocker()\n",
    "canditate_set_OB = ob.block_candset(canditate_set_RB, 'Genre', 'Genre', word_level=True, overlap_size=2, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems perfectly fine to stop here, but we add more blockers for show..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.3 Blackbox blocker on \"Year\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = em.BlackBoxBlocker()\n",
    "bb.set_black_box_function(is_year_year)\n",
    "canditate_set_BB = bb.block_candset(canditate_set_RB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.4 Attribute blocker on \"Director\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = em.AttrEquivalenceBlocker()\n",
    "canditate_set_AB = ab.block_candset(canditate_set_BB, l_block_attr='Director', r_block_attr='Director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4. Sample data to be labelled</h4>\n",
    "For our next step, we need to sample some data and label it accordingly for our matcher. <br>\n",
    "Sample set amount can vary, but these are to be our ground-truth forward. <br> <br> \n",
    "As of 2022, Magellan does not support proper GUI for labelling, so we save the sample set to a filepath and label it manually.  <br>\n",
    "Then, we load the dataset from its origin path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Choose the candidate which is the result of your recent blocker. \n",
    "sample_set = em.sample_table(canditate_set_BB, sample_set_size)\n",
    "sample_set.to_csv(datasets_dir + os.sep + name_of_sample_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2058fc30687b0128d95a989c948081aa14c6a2edcd16aff3f04d2286aad1f775"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
