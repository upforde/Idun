cuda
Device: cuda
step: 0, loss: 0.7836191058158875
step: 10, loss: 0.1394299864768982
step: 20, loss: 0.1432913839817047
step: 30, loss: 0.053471148014068604
step: 40, loss: 0.04673295095562935
step: 50, loss: 0.30052074790000916
step: 60, loss: 0.23839877545833588
step: 70, loss: 0.10263822227716446
step: 80, loss: 0.06461740285158157
step: 90, loss: 0.42108309268951416
step: 100, loss: 0.15758958458900452
step: 110, loss: 0.12792275846004486
step: 120, loss: 0.1267126202583313
step: 130, loss: 0.02798686921596527
step: 140, loss: 0.2254079431295395
step: 150, loss: 0.02032845839858055
step: 160, loss: 0.17356622219085693
step: 170, loss: 0.017989439889788628
step: 180, loss: 0.050269730389118195
step: 190, loss: 0.10674630105495453
step: 200, loss: 0.16858620941638947
step: 210, loss: 0.1090771034359932
step: 220, loss: 0.10221906006336212
step: 230, loss: 0.12769296765327454
step: 240, loss: 0.18579211831092834
step: 250, loss: 0.16509364545345306
step: 260, loss: 0.09377750009298325
step: 270, loss: 0.17351901531219482
step: 280, loss: 0.12757712602615356
step: 290, loss: 0.1293257772922516
step: 300, loss: 0.16594010591506958
step: 310, loss: 0.09735049307346344
step: 320, loss: 0.22531521320343018
step: 330, loss: 0.019671300426125526
step: 340, loss: 0.2929892838001251
step: 350, loss: 0.16680099070072174
step: 360, loss: 0.15164269506931305
epoch 1: dev_f1=0.6599999999999999, f1=0.6666666666666666, best_f1=0.6666666666666666
step: 0, loss: 0.025784539058804512
step: 10, loss: 0.12472254782915115
step: 20, loss: 0.04935745894908905
step: 30, loss: 0.28383317589759827
step: 40, loss: 0.1199139654636383
step: 50, loss: 0.2598446011543274
step: 60, loss: 0.21127384901046753
step: 70, loss: 0.10095707327127457
step: 80, loss: 0.043981391936540604
step: 90, loss: 0.17326754331588745
step: 100, loss: 0.07976764440536499
step: 110, loss: 0.1600416749715805
step: 120, loss: 0.13225813210010529
step: 130, loss: 0.08253921568393707
step: 140, loss: 0.08857765048742294
step: 150, loss: 0.04150506854057312
step: 160, loss: 0.23197157680988312
step: 170, loss: 0.04974355548620224
step: 180, loss: 0.13502156734466553
step: 190, loss: 0.20337332785129547
step: 200, loss: 0.05551810562610626
step: 210, loss: 0.17142784595489502
step: 220, loss: 0.15550431609153748
step: 230, loss: 0.30380067229270935
step: 240, loss: 0.0985739678144455
step: 250, loss: 0.08648613095283508
step: 260, loss: 0.158626526594162
step: 270, loss: 0.18805564939975739
step: 280, loss: 0.11608506739139557
step: 290, loss: 0.2730996012687683
step: 300, loss: 0.05011061578989029
step: 310, loss: 0.0907512977719307
step: 320, loss: 0.10494633764028549
step: 330, loss: 0.05733485892415047
step: 340, loss: 0.12096433341503143
step: 350, loss: 0.263213187456131
step: 360, loss: 0.2657625675201416
epoch 2: dev_f1=0.6836734693877551, f1=0.7538461538461538, best_f1=0.7538461538461538
step: 0, loss: 0.07198040187358856
step: 10, loss: 0.06126593425869942
step: 20, loss: 0.06563995033502579
step: 30, loss: 0.042328763753175735
step: 40, loss: 0.06065022572875023
step: 50, loss: 0.004236873704940081
step: 60, loss: 0.15215879678726196
step: 70, loss: 0.028476839885115623
step: 80, loss: 0.020936373621225357
step: 90, loss: 0.09980405122041702
step: 100, loss: 0.09614674746990204
step: 110, loss: 0.04328304901719093
step: 120, loss: 0.03747747093439102
step: 130, loss: 0.14309380948543549
step: 140, loss: 0.13090258836746216
step: 150, loss: 0.07958215475082397
step: 160, loss: 0.210161954164505
step: 170, loss: 0.08408384770154953
step: 180, loss: 0.07439401000738144
step: 190, loss: 0.05008620396256447
step: 200, loss: 0.09998950362205505
step: 210, loss: 0.03540864586830139
step: 220, loss: 0.10253846645355225
step: 230, loss: 0.025271371006965637
step: 240, loss: 0.13650605082511902
step: 250, loss: 0.20302797853946686
step: 260, loss: 0.0013187837321311235
step: 270, loss: 0.013495197519659996
step: 280, loss: 0.15592922270298004
step: 290, loss: 0.07685323059558868
step: 300, loss: 0.16719062626361847
step: 310, loss: 0.014028871431946754
step: 320, loss: 0.11416779458522797
step: 330, loss: 0.08567429333925247
step: 340, loss: 0.03664330393075943
step: 350, loss: 0.0770784243941307
step: 360, loss: 0.07243289798498154
epoch 3: dev_f1=0.7268170426065164, f1=0.7321867321867322, best_f1=0.7321867321867322
step: 0, loss: 0.13632731139659882
step: 10, loss: 0.21614815294742584
step: 20, loss: 0.07282496243715286
step: 30, loss: 0.02471032552421093
step: 40, loss: 0.020095502957701683
step: 50, loss: 0.05919486656785011
step: 60, loss: 0.030983250588178635
step: 70, loss: 0.011353729292750359
step: 80, loss: 0.09221256524324417
step: 90, loss: 0.12588340044021606
step: 100, loss: 0.036000389605760574
step: 110, loss: 0.056239448487758636
step: 120, loss: 0.17450971901416779
step: 130, loss: 0.09414040297269821
step: 140, loss: 0.045890726149082184
step: 150, loss: 0.06411638110876083
step: 160, loss: 0.044160421937704086
step: 170, loss: 0.07436788082122803
step: 180, loss: 0.00828531477600336
step: 190, loss: 0.10674243420362473
step: 200, loss: 0.028109543025493622
step: 210, loss: 0.021460840478539467
step: 220, loss: 0.0952950119972229
step: 230, loss: 0.11965152621269226
step: 240, loss: 0.027883309870958328
step: 250, loss: 0.05090842396020889
step: 260, loss: 0.07616715133190155
step: 270, loss: 0.07434817403554916
step: 280, loss: 0.03261993080377579
step: 290, loss: 0.04739542677998543
step: 300, loss: 0.09809251129627228
step: 310, loss: 0.0751977413892746
step: 320, loss: 0.04583482816815376
step: 330, loss: 0.0976167619228363
step: 340, loss: 0.05354154855012894
step: 350, loss: 0.0494498573243618
step: 360, loss: 0.036453504115343094
epoch 4: dev_f1=0.7132169576059851, f1=0.75, best_f1=0.7321867321867322
step: 0, loss: 0.10012464225292206
step: 10, loss: 0.004329289775341749
step: 20, loss: 0.10716526210308075
step: 30, loss: 0.0014346529496833682
step: 40, loss: 0.021855086088180542
step: 50, loss: 0.01664556935429573
step: 60, loss: 0.12782584130764008
step: 70, loss: 0.06486440449953079
step: 80, loss: 0.07597701251506805
step: 90, loss: 0.00039891537744551897
step: 100, loss: 0.10946048051118851
step: 110, loss: 0.011861204169690609
step: 120, loss: 0.08940710872411728
step: 130, loss: 0.09479634463787079
step: 140, loss: 0.014210068620741367
step: 150, loss: 0.01723671332001686
step: 160, loss: 0.07137634605169296
step: 170, loss: 0.03724659979343414
step: 180, loss: 0.10820776969194412
step: 190, loss: 0.19617310166358948
step: 200, loss: 0.008400254882872105
step: 210, loss: 0.10687675327062607
step: 220, loss: 0.012841307558119297
step: 230, loss: 0.0193696990609169
step: 240, loss: 0.0780695453286171
step: 250, loss: 0.21451304852962494
step: 260, loss: 0.04166744649410248
step: 270, loss: 0.019825957715511322
step: 280, loss: 0.13160741329193115
step: 290, loss: 0.06710889935493469
step: 300, loss: 0.10201909393072128
step: 310, loss: 0.03635095804929733
step: 320, loss: 0.12243232876062393
step: 330, loss: 0.02794075571000576
step: 340, loss: 0.09883172810077667
step: 350, loss: 0.04610421508550644
step: 360, loss: 0.070649154484272
epoch 5: dev_f1=0.7281553398058251, f1=0.7258883248730965, best_f1=0.7258883248730965
step: 0, loss: 0.04968579113483429
step: 10, loss: 0.025809161365032196
step: 20, loss: 0.025099514052271843
step: 30, loss: 0.007987797260284424
step: 40, loss: 0.08272852748632431
step: 50, loss: 0.07660526037216187
step: 60, loss: 0.2355586141347885
step: 70, loss: 0.07094568759202957
step: 80, loss: 0.06391386687755585
step: 90, loss: 0.02523862011730671
step: 100, loss: 0.04742870852351189
step: 110, loss: 0.03254012018442154
step: 120, loss: 0.12039919942617416
step: 130, loss: 0.13002805411815643
step: 140, loss: 0.09906549006700516
step: 150, loss: 0.0019806523341685534
step: 160, loss: 0.047143805772066116
step: 170, loss: 0.023224664852023125
step: 180, loss: 0.059966884553432465
step: 190, loss: 0.03567579388618469
step: 200, loss: 0.01520307082682848
step: 210, loss: 0.016556618735194206
step: 220, loss: 0.08313370496034622
step: 230, loss: 0.022083699703216553
step: 240, loss: 0.008649306371808052
step: 250, loss: 0.11560259759426117
step: 260, loss: 0.02177724801003933
step: 270, loss: 0.06871958076953888
step: 280, loss: 0.10889789462089539
step: 290, loss: 0.06201799213886261
step: 300, loss: 0.1367979794740677
step: 310, loss: 0.0510871484875679
step: 320, loss: 0.1989269256591797
step: 330, loss: 0.041048288345336914
step: 340, loss: 0.016760027036070824
step: 350, loss: 0.07540147006511688
step: 360, loss: 0.06661631166934967
epoch 6: dev_f1=0.7566265060240965, f1=0.7738693467336683, best_f1=0.7738693467336683
step: 0, loss: 0.024008603766560555
step: 10, loss: 0.21747012436389923
step: 20, loss: 0.03793541342020035
step: 30, loss: 0.008630391210317612
step: 40, loss: 0.08509602397680283
step: 50, loss: 0.03381067141890526
step: 60, loss: 0.031222064048051834
step: 70, loss: 0.00873666163533926
step: 80, loss: 0.021070167422294617
step: 90, loss: 0.0957866981625557
step: 100, loss: 0.008380308747291565
step: 110, loss: 0.07656484097242355
step: 120, loss: 0.060431163758039474
step: 130, loss: 0.043445322662591934
step: 140, loss: 0.003254345851019025
step: 150, loss: 0.0709851011633873
step: 160, loss: 0.0001531706948298961
step: 170, loss: 0.12378969043493271
step: 180, loss: 0.03149653226137161
step: 190, loss: 0.0012362261768430471
step: 200, loss: 0.011558222584426403
step: 210, loss: 0.006313086021691561
step: 220, loss: 0.08675092458724976
step: 230, loss: 0.04297485947608948
step: 240, loss: 0.0666801705956459
step: 250, loss: 0.010909322649240494
step: 260, loss: 0.0004127088759560138
step: 270, loss: 0.04273147135972977
step: 280, loss: 0.04146809130907059
step: 290, loss: 0.13197952508926392
step: 300, loss: 0.03722498193383217
step: 310, loss: 0.07600916177034378
step: 320, loss: 0.07282137125730515
step: 330, loss: 0.05092182755470276
step: 340, loss: 0.05755896121263504
step: 350, loss: 0.13342659175395966
step: 360, loss: 0.056108009070158005
epoch 7: dev_f1=0.7526881720430108, f1=0.7648725212464589, best_f1=0.7738693467336683
step: 0, loss: 0.07677671313285828
step: 10, loss: 0.027316473424434662
step: 20, loss: 0.014982230961322784
step: 30, loss: 0.06833679229021072
step: 40, loss: 0.003314671339467168
step: 50, loss: 0.12443127483129501
step: 60, loss: 0.034617602825164795
step: 70, loss: 0.10374059528112411
step: 80, loss: 0.07115960121154785
step: 90, loss: 0.07168090343475342
step: 100, loss: 0.02973259426653385
step: 110, loss: 0.014368587173521519
step: 120, loss: 0.048176757991313934
step: 130, loss: 0.014752981252968311
step: 140, loss: 0.0475015863776207
step: 150, loss: 0.03691994771361351
step: 160, loss: 0.03460317850112915
step: 170, loss: 0.0402376614511013
step: 180, loss: 0.00021655671298503876
step: 190, loss: 0.06635721772909164
step: 200, loss: 0.023842452093958855
step: 210, loss: 0.08833044767379761
step: 220, loss: 0.08020167797803879
step: 230, loss: 0.013344507664442062
step: 240, loss: 0.03725672513246536
step: 250, loss: 0.01066663023084402
step: 260, loss: 0.18658851087093353
step: 270, loss: 0.14264877140522003
step: 280, loss: 0.047571491450071335
step: 290, loss: 0.01371035072952509
step: 300, loss: 0.031043769791722298
step: 310, loss: 0.09043817967176437
step: 320, loss: 0.15059547126293182
step: 330, loss: 0.11011404544115067
step: 340, loss: 0.03349502012133598
step: 350, loss: 0.01650630310177803
step: 360, loss: 0.04993682727217674
epoch 8: dev_f1=0.7567567567567568, f1=0.7727272727272727, best_f1=0.7727272727272727
step: 0, loss: 0.023662809282541275
step: 10, loss: 0.038747552782297134
step: 20, loss: 0.0441146194934845
step: 30, loss: 0.04783031716942787
step: 40, loss: 0.04349151998758316
step: 50, loss: 0.004085161257535219
step: 60, loss: 0.041719838976860046
step: 70, loss: 0.00027971845702268183
step: 80, loss: 0.00025611009914427996
step: 90, loss: 0.04046912491321564
step: 100, loss: 0.000648899469524622
step: 110, loss: 0.30433976650238037
step: 120, loss: 0.025727594271302223
step: 130, loss: 0.0643274188041687
step: 140, loss: 0.0236849095672369
step: 150, loss: 0.04918328672647476
step: 160, loss: 0.012394950725138187
step: 170, loss: 0.027559936046600342
step: 180, loss: 0.022396614775061607
step: 190, loss: 0.014648147858679295
step: 200, loss: 0.0373028926551342
step: 210, loss: 0.05771440267562866
step: 220, loss: 0.008166797459125519
step: 230, loss: 0.013384625315666199
step: 240, loss: 0.11013897508382797
step: 250, loss: 0.014355492778122425
step: 260, loss: 0.05602557212114334
step: 270, loss: 0.09746749699115753
step: 280, loss: 0.04779789224267006
step: 290, loss: 0.04108237102627754
step: 300, loss: 0.008495572954416275
step: 310, loss: 0.07351689785718918
step: 320, loss: 0.06714939326047897
step: 330, loss: 0.05151902511715889
step: 340, loss: 0.003992753569036722
step: 350, loss: 0.0189967080950737
step: 360, loss: 0.09064896404743195
epoch 9: dev_f1=0.7758186397984886, f1=0.75, best_f1=0.75
step: 0, loss: 0.05491901561617851
step: 10, loss: 0.05018361285328865
step: 20, loss: 0.009767777286469936
step: 30, loss: 0.0008122869185172021
step: 40, loss: 0.03477609530091286
step: 50, loss: 0.04420670121908188
step: 60, loss: 0.038229331374168396
step: 70, loss: 0.00016233908536378294
step: 80, loss: 0.04438513144850731
step: 90, loss: 0.09799189120531082
step: 100, loss: 0.0946938693523407
step: 110, loss: 0.06468937546014786
step: 120, loss: 0.14958816766738892
step: 130, loss: 0.04313177987933159
step: 140, loss: 0.023759951815009117
step: 150, loss: 0.029140617698431015
step: 160, loss: 0.05895198509097099
step: 170, loss: 0.03300761431455612
step: 180, loss: 0.02026905119419098
step: 190, loss: 0.062055040150880814
step: 200, loss: 0.03970355540513992
step: 210, loss: 0.010391506366431713
step: 220, loss: 0.13297432661056519
step: 230, loss: 0.0005758178886026144
step: 240, loss: 0.03688497096300125
step: 250, loss: 0.012420090846717358
step: 260, loss: 0.036361195147037506
step: 270, loss: 0.11442399024963379
step: 280, loss: 0.08078654110431671
step: 290, loss: 0.034193143248558044
step: 300, loss: 0.023626191541552544
step: 310, loss: 0.06795883923768997
step: 320, loss: 0.012939989566802979
step: 330, loss: 0.01035749725997448
step: 340, loss: 0.01810082234442234
step: 350, loss: 0.016879471018910408
step: 360, loss: 0.11397674679756165
epoch 10: dev_f1=0.7487437185929648, f1=0.7226463104325699, best_f1=0.75
step: 0, loss: 0.006362790707498789
step: 10, loss: 0.02238348498940468
step: 20, loss: 0.04616547375917435
step: 30, loss: 0.06445593386888504
step: 40, loss: 0.022244052961468697
step: 50, loss: 0.016407333314418793
step: 60, loss: 0.00035399358603172004
step: 70, loss: 0.04002252221107483
step: 80, loss: 0.06552791595458984
step: 90, loss: 0.013253763318061829
step: 100, loss: 0.0161135271191597
step: 110, loss: 0.05127952992916107
step: 120, loss: 0.02244076505303383
step: 130, loss: 0.007060116156935692
step: 140, loss: 0.14920511841773987
step: 150, loss: 0.03514287248253822
step: 160, loss: 0.07218160480260849
step: 170, loss: 0.0038160504773259163
step: 180, loss: 0.2664254307746887
step: 190, loss: 0.16202399134635925
step: 200, loss: 0.009828307665884495
step: 210, loss: 0.020189421251416206
step: 220, loss: 0.019759736955165863
step: 230, loss: 0.05008575692772865
step: 240, loss: 0.06973788142204285
step: 250, loss: 0.14584887027740479
step: 260, loss: 0.014199232682585716
step: 270, loss: 0.06301020085811615
step: 280, loss: 0.0450604110956192
step: 290, loss: 0.004113531205803156
step: 300, loss: 0.032859303057193756
step: 310, loss: 0.03964696824550629
step: 320, loss: 0.03203227370977402
step: 330, loss: 0.025070570409297943
step: 340, loss: 0.05582250654697418
step: 350, loss: 0.002151773776859045
step: 360, loss: 0.028648806735873222
epoch 11: dev_f1=0.7386091127098321, f1=0.7381546134663343, best_f1=0.75
step: 0, loss: 0.028425557538866997
step: 10, loss: 0.06155967712402344
step: 20, loss: 0.02551618590950966
step: 30, loss: 0.00700385170057416
step: 40, loss: 0.018957538530230522
step: 50, loss: 0.05414875969290733
step: 60, loss: 0.012952394783496857
step: 70, loss: 0.00017614694661460817
step: 80, loss: 0.012524505145847797
step: 90, loss: 0.05707370117306709
step: 100, loss: 0.02605968900024891
step: 110, loss: 0.05942455306649208
step: 120, loss: 0.1317109316587448
step: 130, loss: 0.16570237278938293
step: 140, loss: 0.020622622221708298
step: 150, loss: 6.64692634018138e-05
step: 160, loss: 0.009454612620174885
step: 170, loss: 0.02170378714799881
step: 180, loss: 6.702323298668489e-05
step: 190, loss: 0.00976795144379139
step: 200, loss: 0.020232517272233963
step: 210, loss: 0.007208466064184904
step: 220, loss: 0.00014304841170087457
step: 230, loss: 0.030313169583678246
step: 240, loss: 0.06768687069416046
step: 250, loss: 0.06742764264345169
step: 260, loss: 0.021645719185471535
step: 270, loss: 0.02377275563776493
step: 280, loss: 0.0009715604246594012
step: 290, loss: 0.004858349449932575
step: 300, loss: 0.013194927014410496
step: 310, loss: 0.271525114774704
step: 320, loss: 0.006844938267022371
step: 330, loss: 0.07145792990922928
step: 340, loss: 0.004579304251819849
step: 350, loss: 0.047358438372612
step: 360, loss: 0.04320322349667549
epoch 12: dev_f1=0.7648578811369509, f1=0.7596899224806202, best_f1=0.75
step: 0, loss: 0.07990889251232147
step: 10, loss: 0.004340350162237883
step: 20, loss: 0.02207929827272892
step: 30, loss: 0.007393370382487774
step: 40, loss: 0.025074491277337074
step: 50, loss: 0.0996895283460617
step: 60, loss: 0.024489138275384903
step: 70, loss: 4.2596184357535094e-05
step: 80, loss: 0.14544832706451416
step: 90, loss: 0.008535557426512241
step: 100, loss: 0.08373317122459412
step: 110, loss: 0.04133647307753563
step: 120, loss: 0.002067980356514454
step: 130, loss: 0.02069748565554619
step: 140, loss: 0.05183880031108856
step: 150, loss: 0.022837858647108078
step: 160, loss: 6.659829523414373e-05
step: 170, loss: 0.08059775829315186
step: 180, loss: 0.0036365000996738672
step: 190, loss: 0.00402381457388401
step: 200, loss: 0.014726124703884125
step: 210, loss: 0.0013733715750277042
step: 220, loss: 0.06849575787782669
step: 230, loss: 0.022707857191562653
step: 240, loss: 0.020677149295806885
step: 250, loss: 0.0912780687212944
step: 260, loss: 0.006564841605722904
step: 270, loss: 0.016382236033678055
step: 280, loss: 8.79418512340635e-05
step: 290, loss: 0.060036107897758484
step: 300, loss: 0.00318354950286448
step: 310, loss: 0.0439273826777935
step: 320, loss: 0.022166959941387177
step: 330, loss: 0.008836341090500355
step: 340, loss: 0.00045066719758324325
step: 350, loss: 0.0002094725496135652
step: 360, loss: 0.04369997978210449
epoch 13: dev_f1=0.73, f1=0.7135416666666666, best_f1=0.75
step: 0, loss: 0.017994167283177376
step: 10, loss: 0.08150167763233185
step: 20, loss: 0.024912508204579353
step: 30, loss: 0.013213057070970535
step: 40, loss: 0.030315542593598366
step: 50, loss: 0.06456801295280457
step: 60, loss: 0.02971825934946537
step: 70, loss: 0.11236622184515
step: 80, loss: 0.08622467517852783
step: 90, loss: 0.022547144442796707
step: 100, loss: 0.017669055610895157
step: 110, loss: 0.0002440308453515172
step: 120, loss: 0.001685730298049748
step: 130, loss: 0.02887985110282898
step: 140, loss: 0.0007578985532745719
step: 150, loss: 0.005502929445356131
step: 160, loss: 0.02248997427523136
step: 170, loss: 0.02441292814910412
step: 180, loss: 0.07912667095661163
step: 190, loss: 0.06833450496196747
step: 200, loss: 0.08449813723564148
step: 210, loss: 0.047055184841156006
step: 220, loss: 0.02796161361038685
step: 230, loss: 0.02969410829246044
step: 240, loss: 0.056482166051864624
step: 250, loss: 0.01899089105427265
step: 260, loss: 0.02166924625635147
step: 270, loss: 0.0013618816155940294
step: 280, loss: 0.006357158534228802
step: 290, loss: 0.00011656067363219336
step: 300, loss: 0.08757814019918442
step: 310, loss: 0.04357040673494339
step: 320, loss: 0.11024616658687592
step: 330, loss: 0.05395621806383133
step: 340, loss: 0.0008556161774322391
step: 350, loss: 0.03650495037436485
step: 360, loss: 0.0038037439808249474
epoch 14: dev_f1=0.7412935323383084, f1=0.717948717948718, best_f1=0.75
step: 0, loss: 0.0009679100476205349
step: 10, loss: 0.0026771980337798595
step: 20, loss: 0.021074604243040085
step: 30, loss: 0.05104143172502518
step: 40, loss: 0.02056039683520794
step: 50, loss: 0.0692182332277298
step: 60, loss: 0.026031706482172012
step: 70, loss: 0.0031828545033931732
step: 80, loss: 0.042199283838272095
step: 90, loss: 0.10978186130523682
step: 100, loss: 0.003042611526325345
step: 110, loss: 0.08367623388767242
step: 120, loss: 0.02195822447538376
step: 130, loss: 0.02547399327158928
step: 140, loss: 0.02259242534637451
step: 150, loss: 0.02358534373342991
step: 160, loss: 0.0002399903314653784
step: 170, loss: 0.03286221995949745
step: 180, loss: 0.036812249571084976
step: 190, loss: 3.420414577703923e-05
step: 200, loss: 0.00035457490594126284
step: 210, loss: 0.00403897138312459
step: 220, loss: 0.014180661179125309
step: 230, loss: 0.15957164764404297
step: 240, loss: 0.014476578682661057
step: 250, loss: 0.03770226985216141
step: 260, loss: 0.002122719306498766
step: 270, loss: 0.045775916427373886
step: 280, loss: 0.022035745903849602
step: 290, loss: 0.0019074666779488325
step: 300, loss: 0.0001557925425004214
step: 310, loss: 0.0006868192576803267
step: 320, loss: 0.026936767622828484
step: 330, loss: 0.0005316444439813495
step: 340, loss: 0.02383124828338623
step: 350, loss: 0.06911353766918182
step: 360, loss: 0.009750496596097946
epoch 15: dev_f1=0.7611548556430445, f1=0.7473118279569892, best_f1=0.75
step: 0, loss: 0.03610231354832649
step: 10, loss: 0.09527211636304855
step: 20, loss: 0.06908242404460907
step: 30, loss: 0.07447701692581177
step: 40, loss: 0.1846933215856552
step: 50, loss: 0.020452609285712242
step: 60, loss: 0.009025104343891144
step: 70, loss: 0.11665335297584534
step: 80, loss: 0.03066212125122547
step: 90, loss: 0.075375035405159
step: 100, loss: 0.0005401686066761613
step: 110, loss: 0.027514822781085968
step: 120, loss: 0.018491476774215698
step: 130, loss: 0.00014777995238546282
step: 140, loss: 0.0001256421091966331
step: 150, loss: 0.029540421441197395
step: 160, loss: 0.03332590311765671
step: 170, loss: 0.062564916908741
step: 180, loss: 0.09772442281246185
step: 190, loss: 0.006793381180614233
step: 200, loss: 0.0021917398553341627
step: 210, loss: 0.0010904287919402122
step: 220, loss: 0.03391939029097557
step: 230, loss: 0.002051134128123522
step: 240, loss: 0.04224210977554321
step: 250, loss: 0.0548003688454628
step: 260, loss: 0.011077485978603363
step: 270, loss: 0.05146251991391182
step: 280, loss: 0.025114908814430237
step: 290, loss: 0.0023903646506369114
step: 300, loss: 0.08745121955871582
step: 310, loss: 0.10673007369041443
step: 320, loss: 0.03603364899754524
step: 330, loss: 0.00853895116597414
step: 340, loss: 0.024692922830581665
step: 350, loss: 0.01851455681025982
step: 360, loss: 0.08767141401767731
epoch 16: dev_f1=0.7424242424242425, f1=0.7346938775510203, best_f1=0.75
step: 0, loss: 0.060279641300439835
step: 10, loss: 0.021950917318463326
step: 20, loss: 0.023739073425531387
step: 30, loss: 0.04492665082216263
step: 40, loss: 0.022140564396977425
step: 50, loss: 0.017982684075832367
step: 60, loss: 0.017924325540661812
step: 70, loss: 0.01842222362756729
step: 80, loss: 0.027248090133070946
step: 90, loss: 0.054973188787698746
step: 100, loss: 0.0031530424021184444
step: 110, loss: 0.024722574278712273
step: 120, loss: 0.0316564217209816
step: 130, loss: 0.06749328225851059
step: 140, loss: 0.02056645229458809
step: 150, loss: 0.000388109969208017
step: 160, loss: 0.032850008457899094
step: 170, loss: 0.016740355640649796
step: 180, loss: 0.00016606035933364183
step: 190, loss: 0.031002987176179886
step: 200, loss: 0.014275072142481804
step: 210, loss: 0.05667131021618843
step: 220, loss: 0.029600055888295174
step: 230, loss: 0.00013612610928248614
step: 240, loss: 0.0007565027917735279
step: 250, loss: 0.04366206377744675
step: 260, loss: 0.00013237209350336343
step: 270, loss: 0.05675183981657028
step: 280, loss: 0.020282898098230362
step: 290, loss: 0.0017633233219385147
step: 300, loss: 0.04504019021987915
step: 310, loss: 9.75475340965204e-05
step: 320, loss: 0.012567568570375443
step: 330, loss: 0.022630352526903152
step: 340, loss: 0.030251121148467064
step: 350, loss: 0.025894148275256157
step: 360, loss: 0.08402951806783676
epoch 17: dev_f1=0.7526315789473683, f1=0.7452054794520547, best_f1=0.75
step: 0, loss: 0.047987859696149826
step: 10, loss: 0.03191012144088745
step: 20, loss: 0.021492036059498787
step: 30, loss: 0.00021146272774785757
step: 40, loss: 0.019033264368772507
step: 50, loss: 0.00012269614671822637
step: 60, loss: 0.0024725294206291437
step: 70, loss: 0.06336601823568344
step: 80, loss: 0.0006141028134152293
step: 90, loss: 0.04886344447731972
step: 100, loss: 0.07971000671386719
step: 110, loss: 0.0017984233563765883
step: 120, loss: 0.00024923201999627054
step: 130, loss: 0.018401196226477623
step: 140, loss: 0.0005333725712262094
step: 150, loss: 0.001437499071471393
step: 160, loss: 0.05758778378367424
step: 170, loss: 3.098217712249607e-05
step: 180, loss: 0.003073094878345728
step: 190, loss: 0.004998323041945696
step: 200, loss: 0.002209572121500969
step: 210, loss: 0.010995802469551563
step: 220, loss: 0.0335313081741333
step: 230, loss: 0.0008489988395012915
step: 240, loss: 0.0010807611979544163
step: 250, loss: 0.01171647384762764
step: 260, loss: 0.00010770662629511207
step: 270, loss: 0.0281987227499485
step: 280, loss: 0.008259817026555538
step: 290, loss: 0.022113194689154625
step: 300, loss: 0.0002265485964016989
step: 310, loss: 4.365401400718838e-05
step: 320, loss: 0.03359822556376457
step: 330, loss: 9.712161408970132e-05
step: 340, loss: 0.0021803786512464285
step: 350, loss: 0.00017013760225381702
step: 360, loss: 0.038612041622400284
epoch 18: dev_f1=0.7474226804123711, f1=0.7393617021276595, best_f1=0.75
step: 0, loss: 0.016850028187036514
step: 10, loss: 0.0002386723062954843
step: 20, loss: 0.012257394380867481
step: 30, loss: 0.0002693976566661149
step: 40, loss: 5.98408114456106e-05
step: 50, loss: 8.154885290423408e-05
step: 60, loss: 0.04939901828765869
step: 70, loss: 0.0001946251722984016
step: 80, loss: 0.016081683337688446
step: 90, loss: 0.02280193753540516
step: 100, loss: 0.03451590612530708
step: 110, loss: 0.04436258226633072
step: 120, loss: 0.046493180096149445
step: 130, loss: 0.034091196954250336
step: 140, loss: 0.05057075619697571
step: 150, loss: 0.02710568532347679
step: 160, loss: 0.09364619106054306
step: 170, loss: 0.004521952010691166
step: 180, loss: 0.004557493142783642
step: 190, loss: 0.01489375252276659
step: 200, loss: 0.005726430099457502
step: 210, loss: 0.00021012641082052141
step: 220, loss: 0.009247275069355965
step: 230, loss: 0.045497942715883255
step: 240, loss: 5.784500899608247e-05
step: 250, loss: 0.0909862145781517
step: 260, loss: 0.0006859783898107708
step: 270, loss: 0.021472811698913574
step: 280, loss: 0.02707197144627571
step: 290, loss: 0.018969837576150894
step: 300, loss: 0.026557454839348793
step: 310, loss: 0.0024503814056515694
step: 320, loss: 0.04027314484119415
step: 330, loss: 0.06098959594964981
step: 340, loss: 0.006018069572746754
step: 350, loss: 0.034097421914339066
step: 360, loss: 0.03574208542704582
epoch 19: dev_f1=0.7424242424242425, f1=0.7315789473684211, best_f1=0.75
step: 0, loss: 0.07321050763130188
step: 10, loss: 0.007774684112519026
step: 20, loss: 0.004639748018234968
step: 30, loss: 0.0004158958909101784
step: 40, loss: 0.06509287655353546
step: 50, loss: 0.046260666102170944
step: 60, loss: 0.0006436063558794558
step: 70, loss: 0.024273138493299484
step: 80, loss: 0.023385651409626007
step: 90, loss: 0.07127341628074646
step: 100, loss: 0.0028958795592188835
step: 110, loss: 0.04563051462173462
step: 120, loss: 0.0001955724583240226
step: 130, loss: 0.00022297847317531705
step: 140, loss: 0.04674777388572693
step: 150, loss: 0.002331949071958661
step: 160, loss: 3.099264722550288e-05
step: 170, loss: 6.729368033120409e-05
step: 180, loss: 0.017928697168827057
step: 190, loss: 0.02310751937329769
step: 200, loss: 0.005699860397726297
step: 210, loss: 0.04281911253929138
step: 220, loss: 2.7119431251776405e-05
step: 230, loss: 0.027948502451181412
step: 240, loss: 0.028598727658391
step: 250, loss: 0.15729643404483795
step: 260, loss: 0.04231205955147743
step: 270, loss: 0.00020112344645895064
step: 280, loss: 0.01580696925520897
step: 290, loss: 0.00013345367915462703
step: 300, loss: 0.01984538696706295
step: 310, loss: 0.019111337140202522
step: 320, loss: 0.02329300157725811
step: 330, loss: 0.02035868540406227
step: 340, loss: 0.015465577132999897
step: 350, loss: 0.036671653389930725
step: 360, loss: 0.0006849021301604807
epoch 20: dev_f1=0.7384615384615385, f1=0.7326203208556149, best_f1=0.75
