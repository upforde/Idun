cuda
Device: cuda
step: 0, loss: 0.4855365455150604
step: 10, loss: 0.3150831460952759
step: 20, loss: 0.1596144586801529
step: 30, loss: 0.2563060224056244
step: 40, loss: 0.31808897852897644
step: 50, loss: 0.5049325823783875
step: 60, loss: 0.04366512969136238
step: 70, loss: 0.14925676584243774
step: 80, loss: 0.5858469605445862
step: 90, loss: 0.14439629018306732
step: 100, loss: 0.26028022170066833
step: 110, loss: 0.1430269330739975
step: 120, loss: 0.14374278485774994
step: 130, loss: 0.14134056866168976
step: 140, loss: 0.13825133442878723
step: 150, loss: 0.13063666224479675
step: 160, loss: 0.15028060972690582
step: 170, loss: 0.19608047604560852
step: 180, loss: 0.1105312779545784
step: 190, loss: 0.20147724449634552
step: 200, loss: 0.24386198818683624
step: 210, loss: 0.12182968854904175
step: 220, loss: 0.016914037987589836
step: 230, loss: 0.1188403069972992
step: 240, loss: 0.07514087110757828
step: 250, loss: 0.1191178485751152
step: 260, loss: 0.14903895556926727
step: 270, loss: 0.007353348191827536
step: 280, loss: 0.19429972767829895
step: 290, loss: 0.012624828144907951
step: 300, loss: 0.03985283896327019
step: 310, loss: 0.10934305191040039
step: 320, loss: 0.04096265137195587
step: 330, loss: 0.018765894696116447
step: 340, loss: 0.17153139412403107
step: 350, loss: 0.07962922751903534
step: 360, loss: 0.099788598716259
epoch 1: dev_f1=0.5922746781115881, f1=0.5664488017429193, best_f1=0.5664488017429193
step: 0, loss: 0.04085024818778038
step: 10, loss: 0.2857045829296112
step: 20, loss: 0.07982183247804642
step: 30, loss: 0.21547938883304596
step: 40, loss: 0.24278607964515686
step: 50, loss: 0.2895280718803406
step: 60, loss: 0.056622084230184555
step: 70, loss: 0.2531164586544037
step: 80, loss: 0.204403817653656
step: 90, loss: 0.15422125160694122
step: 100, loss: 0.10289128124713898
step: 110, loss: 0.06883643567562103
step: 120, loss: 0.16202926635742188
step: 130, loss: 0.08710143715143204
step: 140, loss: 0.20966888964176178
step: 150, loss: 0.18473613262176514
step: 160, loss: 0.14474792778491974
step: 170, loss: 0.17531903088092804
step: 180, loss: 0.17321376502513885
step: 190, loss: 0.1571379154920578
step: 200, loss: 0.1883627474308014
step: 210, loss: 0.1225576102733612
step: 220, loss: 0.2735775113105774
step: 230, loss: 0.05294942855834961
step: 240, loss: 0.025767464190721512
step: 250, loss: 0.03163198009133339
step: 260, loss: 0.15971192717552185
step: 270, loss: 0.017610087990760803
step: 280, loss: 0.03290608897805214
step: 290, loss: 0.21522605419158936
step: 300, loss: 0.10295481979846954
step: 310, loss: 0.07720021158456802
step: 320, loss: 0.024660781025886536
step: 330, loss: 0.047257717698812485
step: 340, loss: 0.22187909483909607
step: 350, loss: 0.05570928752422333
step: 360, loss: 0.12483461946249008
epoch 2: dev_f1=0.72, f1=0.7184466019417476, best_f1=0.7184466019417476
step: 0, loss: 0.03903729096055031
step: 10, loss: 0.025117795914411545
step: 20, loss: 0.06509958207607269
step: 30, loss: 0.15766257047653198
step: 40, loss: 0.054985225200653076
step: 50, loss: 0.04565677046775818
step: 60, loss: 0.1282401829957962
step: 70, loss: 0.14966124296188354
step: 80, loss: 0.06568074971437454
step: 90, loss: 0.034900885075330734
step: 100, loss: 0.06256582587957382
step: 110, loss: 0.20219893753528595
step: 120, loss: 0.08027945458889008
step: 130, loss: 0.11882498860359192
step: 140, loss: 0.03948960080742836
step: 150, loss: 0.27314049005508423
step: 160, loss: 0.05174519494175911
step: 170, loss: 0.11910784244537354
step: 180, loss: 0.06897604465484619
step: 190, loss: 0.029156414791941643
step: 200, loss: 0.23358166217803955
step: 210, loss: 0.04803062230348587
step: 220, loss: 0.23751208186149597
step: 230, loss: 0.025557899847626686
step: 240, loss: 0.14253656566143036
step: 250, loss: 0.12115316838026047
step: 260, loss: 0.12027732282876968
step: 270, loss: 0.10615967959165573
step: 280, loss: 0.07470337301492691
step: 290, loss: 0.0025681843981146812
step: 300, loss: 0.07046761363744736
step: 310, loss: 0.1745493859052658
step: 320, loss: 0.06240614503622055
step: 330, loss: 0.07836674153804779
step: 340, loss: 0.056390516459941864
step: 350, loss: 0.049288515001535416
step: 360, loss: 0.13891755044460297
epoch 3: dev_f1=0.7577319587628867, f1=0.7563451776649746, best_f1=0.7563451776649746
step: 0, loss: 0.04694471135735512
step: 10, loss: 0.02354668453335762
step: 20, loss: 0.040606483817100525
step: 30, loss: 0.21112491190433502
step: 40, loss: 0.08975883573293686
step: 50, loss: 0.06250081956386566
step: 60, loss: 0.06759191304445267
step: 70, loss: 0.1972324699163437
step: 80, loss: 0.09324119985103607
step: 90, loss: 0.10845138877630234
step: 100, loss: 0.052527520805597305
step: 110, loss: 0.03050907328724861
step: 120, loss: 0.025211403146386147
step: 130, loss: 0.08881690353155136
step: 140, loss: 0.136764794588089
step: 150, loss: 0.015972057357430458
step: 160, loss: 0.056633852422237396
step: 170, loss: 0.002587901894003153
step: 180, loss: 0.02529681846499443
step: 190, loss: 0.19298633933067322
step: 200, loss: 0.017862673848867416
step: 210, loss: 0.038925886154174805
step: 220, loss: 0.0717574805021286
step: 230, loss: 0.06114572659134865
step: 240, loss: 0.06767633557319641
step: 250, loss: 0.13063865900039673
step: 260, loss: 0.08019817620515823
step: 270, loss: 0.0781569629907608
step: 280, loss: 0.18551236391067505
step: 290, loss: 0.05248548462986946
step: 300, loss: 0.02242533676326275
step: 310, loss: 0.023400070145726204
step: 320, loss: 0.11659551411867142
step: 330, loss: 0.034509338438510895
step: 340, loss: 0.04004131257534027
step: 350, loss: 0.15962707996368408
step: 360, loss: 0.1146773099899292
epoch 4: dev_f1=0.7708333333333334, f1=0.7465940054495912, best_f1=0.7465940054495912
step: 0, loss: 0.014287255704402924
step: 10, loss: 0.06664275377988815
step: 20, loss: 0.02429911121726036
step: 30, loss: 0.04291388764977455
step: 40, loss: 0.04592149704694748
step: 50, loss: 0.06029125303030014
step: 60, loss: 0.0004927720292471349
step: 70, loss: 0.07332948595285416
step: 80, loss: 0.1408042311668396
step: 90, loss: 0.0656883642077446
step: 100, loss: 0.17924204468727112
step: 110, loss: 0.01688951440155506
step: 120, loss: 0.08006849139928818
step: 130, loss: 0.012731538154184818
step: 140, loss: 0.04046619310975075
step: 150, loss: 0.015595270320773125
step: 160, loss: 0.14514467120170593
step: 170, loss: 0.05105948448181152
step: 180, loss: 0.015169003047049046
step: 190, loss: 0.030317822471261024
step: 200, loss: 0.0224330835044384
step: 210, loss: 0.2533196806907654
step: 220, loss: 0.03371335566043854
step: 230, loss: 0.017172791063785553
step: 240, loss: 0.04727791249752045
step: 250, loss: 0.04101661592721939
step: 260, loss: 0.11350351572036743
step: 270, loss: 0.055109817534685135
step: 280, loss: 0.09145383536815643
step: 290, loss: 0.031470343470573425
step: 300, loss: 0.08383920043706894
step: 310, loss: 0.1472017914056778
step: 320, loss: 0.07410129904747009
step: 330, loss: 0.017118703573942184
step: 340, loss: 0.0347432941198349
step: 350, loss: 0.030519068241119385
step: 360, loss: 0.05299478396773338
epoch 5: dev_f1=0.7566265060240965, f1=0.7405541561712846, best_f1=0.7465940054495912
step: 0, loss: 0.04972059279680252
step: 10, loss: 0.0740722194314003
step: 20, loss: 0.01738717220723629
step: 30, loss: 0.12302125245332718
step: 40, loss: 0.13694638013839722
step: 50, loss: 0.08146485686302185
step: 60, loss: 0.07169865071773529
step: 70, loss: 0.09032658487558365
step: 80, loss: 0.33886873722076416
step: 90, loss: 0.0022674964275211096
step: 100, loss: 0.08998676389455795
step: 110, loss: 0.021286796778440475
step: 120, loss: 0.01943281665444374
step: 130, loss: 0.0794287845492363
step: 140, loss: 0.08240781724452972
step: 150, loss: 0.004121236503124237
step: 160, loss: 0.013257279060781002
step: 170, loss: 0.08760319650173187
step: 180, loss: 0.07327152043581009
step: 190, loss: 0.03772485628724098
step: 200, loss: 0.041633427143096924
step: 210, loss: 0.0011981066782027483
step: 220, loss: 0.022647708654403687
step: 230, loss: 0.08365636318922043
step: 240, loss: 0.029887093231081963
step: 250, loss: 0.07035468518733978
step: 260, loss: 0.11869895458221436
step: 270, loss: 0.0008146627224050462
step: 280, loss: 0.00016292056534439325
step: 290, loss: 0.09731519222259521
step: 300, loss: 0.0002711325651034713
step: 310, loss: 0.1498919278383255
step: 320, loss: 0.1412479430437088
step: 330, loss: 0.05454757809638977
step: 340, loss: 0.02610497735440731
step: 350, loss: 0.039212219417095184
step: 360, loss: 0.04418206959962845
epoch 6: dev_f1=0.7769028871391076, f1=0.7380281690140845, best_f1=0.7380281690140845
step: 0, loss: 0.08958163112401962
step: 10, loss: 0.015325185842812061
step: 20, loss: 0.15995417535305023
step: 30, loss: 0.004055241122841835
step: 40, loss: 0.0286409854888916
step: 50, loss: 0.014557668939232826
step: 60, loss: 0.06331578642129898
step: 70, loss: 0.009492183104157448
step: 80, loss: 0.011851540766656399
step: 90, loss: 0.08698944747447968
step: 100, loss: 0.04308732599020004
step: 110, loss: 0.07658098638057709
step: 120, loss: 0.10419311374425888
step: 130, loss: 0.16158851981163025
step: 140, loss: 0.08847276866436005
step: 150, loss: 0.0036446901503950357
step: 160, loss: 0.015505773015320301
step: 170, loss: 0.04631226509809494
step: 180, loss: 0.00820996891707182
step: 190, loss: 0.011886374093592167
step: 200, loss: 0.025692474097013474
step: 210, loss: 0.03918745741248131
step: 220, loss: 0.05448398366570473
step: 230, loss: 0.01393425464630127
step: 240, loss: 0.09439599514007568
step: 250, loss: 0.008382081054151058
step: 260, loss: 0.001809971872717142
step: 270, loss: 0.08988801389932632
step: 280, loss: 0.09650929272174835
step: 290, loss: 0.04875452071428299
step: 300, loss: 0.0004236403328832239
step: 310, loss: 0.0901012271642685
step: 320, loss: 0.0749070793390274
step: 330, loss: 0.07855541259050369
step: 340, loss: 0.04040314257144928
step: 350, loss: 0.06085817515850067
step: 360, loss: 0.0014468791196122766
epoch 7: dev_f1=0.7598944591029023, f1=0.7493261455525607, best_f1=0.7380281690140845
step: 0, loss: 0.0170719176530838
step: 10, loss: 0.002004258567467332
step: 20, loss: 0.011985712684690952
step: 30, loss: 0.007631534710526466
step: 40, loss: 0.041980329900979996
step: 50, loss: 0.03327105939388275
step: 60, loss: 0.027716394513845444
step: 70, loss: 0.06309162080287933
step: 80, loss: 0.08256854116916656
step: 90, loss: 0.0930110439658165
step: 100, loss: 0.029671667143702507
step: 110, loss: 0.043298132717609406
step: 120, loss: 0.15854987502098083
step: 130, loss: 0.05441749840974808
step: 140, loss: 0.10052258521318436
step: 150, loss: 0.034428201615810394
step: 160, loss: 0.02843432128429413
step: 170, loss: 0.030968934297561646
step: 180, loss: 0.0005110326455906034
step: 190, loss: 0.0003255888295825571
step: 200, loss: 0.0013141243252903223
step: 210, loss: 0.12022101879119873
step: 220, loss: 0.026686454191803932
step: 230, loss: 0.08986469358205795
step: 240, loss: 0.06486931443214417
step: 250, loss: 0.0756998360157013
step: 260, loss: 0.030046945437788963
step: 270, loss: 0.06035267561674118
step: 280, loss: 0.06875021755695343
step: 290, loss: 0.01305387169122696
step: 300, loss: 0.050486885011196136
step: 310, loss: 0.10173782706260681
step: 320, loss: 0.005886704195290804
step: 330, loss: 0.043462734669446945
step: 340, loss: 0.04862175136804581
step: 350, loss: 0.042131103575229645
step: 360, loss: 0.03163542598485947
epoch 8: dev_f1=0.7704485488126649, f1=0.736, best_f1=0.7380281690140845
step: 0, loss: 0.03843335434794426
step: 10, loss: 0.029338020831346512
step: 20, loss: 0.013813029043376446
step: 30, loss: 0.05848843976855278
step: 40, loss: 0.007954600267112255
step: 50, loss: 0.0351722277700901
step: 60, loss: 0.12454824894666672
step: 70, loss: 0.07396604865789413
step: 80, loss: 0.018578141927719116
step: 90, loss: 0.008451665751636028
step: 100, loss: 0.0582975298166275
step: 110, loss: 0.018690725788474083
step: 120, loss: 0.0019600912928581238
step: 130, loss: 0.006163209676742554
step: 140, loss: 0.025910792872309685
step: 150, loss: 0.0821111798286438
step: 160, loss: 0.06380296498537064
step: 170, loss: 0.0723244696855545
step: 180, loss: 0.007159315049648285
step: 190, loss: 0.040013205260038376
step: 200, loss: 0.02215469256043434
step: 210, loss: 0.08047513663768768
step: 220, loss: 0.005455110687762499
step: 230, loss: 0.049751896411180496
step: 240, loss: 0.02924872748553753
step: 250, loss: 0.03431720659136772
step: 260, loss: 0.10555924475193024
step: 270, loss: 0.047459185123443604
step: 280, loss: 0.017354754731059074
step: 290, loss: 0.06827596575021744
step: 300, loss: 0.053553901612758636
step: 310, loss: 0.0010936567559838295
step: 320, loss: 0.02672480046749115
step: 330, loss: 0.023843254894018173
step: 340, loss: 0.00649259053170681
step: 350, loss: 0.058452505618333817
step: 360, loss: 0.07027456164360046
epoch 9: dev_f1=0.761904761904762, f1=0.7390180878552972, best_f1=0.7380281690140845
step: 0, loss: 0.06422151625156403
step: 10, loss: 0.044828955084085464
step: 20, loss: 0.020445382222533226
step: 30, loss: 0.10742305964231491
step: 40, loss: 0.04314982518553734
step: 50, loss: 0.06386810541152954
step: 60, loss: 0.22311565279960632
step: 70, loss: 0.011024190112948418
step: 80, loss: 0.015327654778957367
step: 90, loss: 0.09601651132106781
step: 100, loss: 0.011272808536887169
step: 110, loss: 0.007840939797461033
step: 120, loss: 0.0365540087223053
step: 130, loss: 0.11722337454557419
step: 140, loss: 0.013470543548464775
step: 150, loss: 0.041183557361364365
step: 160, loss: 0.009011144749820232
step: 170, loss: 0.07116550952196121
step: 180, loss: 0.0832616463303566
step: 190, loss: 0.011932329274713993
step: 200, loss: 0.14394578337669373
step: 210, loss: 0.023157401010394096
step: 220, loss: 0.020891815423965454
step: 230, loss: 0.07088129222393036
step: 240, loss: 0.09207526594400406
step: 250, loss: 0.03320099040865898
step: 260, loss: 0.01989760436117649
step: 270, loss: 0.0052920072339475155
step: 280, loss: 0.10452228039503098
step: 290, loss: 0.030287545174360275
step: 300, loss: 0.020617924630641937
step: 310, loss: 0.12651652097702026
step: 320, loss: 0.005254268646240234
step: 330, loss: 0.10856504738330841
step: 340, loss: 0.002970946952700615
step: 350, loss: 0.05111915245652199
step: 360, loss: 0.005566948559135199
epoch 10: dev_f1=0.7760416666666666, f1=0.7391304347826086, best_f1=0.7380281690140845
step: 0, loss: 0.012107194401323795
step: 10, loss: 0.006425430998206139
step: 20, loss: 0.1196427270770073
step: 30, loss: 0.06460057199001312
step: 40, loss: 0.062335990369319916
step: 50, loss: 0.035683341324329376
step: 60, loss: 9.217514161719009e-05
step: 70, loss: 0.01586996018886566
step: 80, loss: 0.05443725362420082
step: 90, loss: 0.0007105718832463026
step: 100, loss: 0.008756098337471485
step: 110, loss: 0.004361405503004789
step: 120, loss: 0.04960635304450989
step: 130, loss: 0.05607305094599724
step: 140, loss: 0.006214183755218983
step: 150, loss: 0.05607035383582115
step: 160, loss: 0.00264841690659523
step: 170, loss: 0.010196847841143608
step: 180, loss: 0.013124793767929077
step: 190, loss: 0.026579905301332474
step: 200, loss: 0.010589972138404846
step: 210, loss: 0.0584522970020771
step: 220, loss: 0.00021585047943517566
step: 230, loss: 0.09010329842567444
step: 240, loss: 0.021830201148986816
step: 250, loss: 0.12365566194057465
step: 260, loss: 0.020747270435094833
step: 270, loss: 0.04857424274086952
step: 280, loss: 0.02045467123389244
step: 290, loss: 0.01860946975648403
step: 300, loss: 0.06423714011907578
step: 310, loss: 0.0017014550976455212
step: 320, loss: 0.011006915010511875
step: 330, loss: 0.07307273894548416
step: 340, loss: 0.08136975765228271
step: 350, loss: 0.01233681757003069
step: 360, loss: 0.0013034518342465162
epoch 11: dev_f1=0.7789473684210525, f1=0.7520435967302452, best_f1=0.7520435967302452
step: 0, loss: 0.009883577935397625
step: 10, loss: 0.0038552104961127043
step: 20, loss: 0.0718861073255539
step: 30, loss: 0.04178816080093384
step: 40, loss: 0.0002599938597995788
step: 50, loss: 0.02223515883088112
step: 60, loss: 0.0007201556582003832
step: 70, loss: 0.008286012336611748
step: 80, loss: 0.0022242534905672073
step: 90, loss: 0.022547733038663864
step: 100, loss: 0.03823825344443321
step: 110, loss: 0.11782380193471909
step: 120, loss: 0.04333552345633507
step: 130, loss: 0.040899477899074554
step: 140, loss: 0.0016113024903461337
step: 150, loss: 0.020838335156440735
step: 160, loss: 0.020117124542593956
step: 170, loss: 0.0011444087140262127
step: 180, loss: 0.020898088812828064
step: 190, loss: 0.08313360065221786
step: 200, loss: 0.04259570688009262
step: 210, loss: 0.021140392869710922
step: 220, loss: 0.021256832405924797
step: 230, loss: 0.0589461587369442
step: 240, loss: 0.012430937960743904
step: 250, loss: 0.012572858482599258
step: 260, loss: 0.03448459878563881
step: 270, loss: 0.08843673765659332
step: 280, loss: 0.004921407904475927
step: 290, loss: 0.09737230837345123
step: 300, loss: 0.025486186146736145
step: 310, loss: 0.053281672298908234
step: 320, loss: 0.05223522335290909
step: 330, loss: 0.16840125620365143
step: 340, loss: 0.0024598226882517338
step: 350, loss: 0.010832125321030617
step: 360, loss: 0.028482211753726006
epoch 12: dev_f1=0.7874396135265701, f1=0.7462686567164178, best_f1=0.7462686567164178
step: 0, loss: 0.0033800427336245775
step: 10, loss: 0.07137908786535263
step: 20, loss: 0.08462480455636978
step: 30, loss: 0.06755460053682327
step: 40, loss: 0.08259933441877365
step: 50, loss: 0.004547822754830122
step: 60, loss: 0.026538509875535965
step: 70, loss: 0.004025366622954607
step: 80, loss: 0.0008784073288552463
step: 90, loss: 5.125069583300501e-05
step: 100, loss: 0.007848070003092289
step: 110, loss: 0.00010375526471761987
step: 120, loss: 0.0070241778157651424
step: 130, loss: 0.0037922891788184643
step: 140, loss: 6.623053923249245e-05
step: 150, loss: 0.0018908418715000153
step: 160, loss: 0.11176467686891556
step: 170, loss: 0.009283745661377907
step: 180, loss: 0.07370835542678833
step: 190, loss: 0.027540404349565506
step: 200, loss: 0.0018023698357865214
step: 210, loss: 0.09939247369766235
step: 220, loss: 0.04227130487561226
step: 230, loss: 0.005723351147025824
step: 240, loss: 0.024521727114915848
step: 250, loss: 0.013310261070728302
step: 260, loss: 0.008068175055086613
step: 270, loss: 0.0001394103019265458
step: 280, loss: 0.039528362452983856
step: 290, loss: 0.08780746906995773
step: 300, loss: 0.007339726202189922
step: 310, loss: 0.13093958795070648
step: 320, loss: 0.0054587372578680515
step: 330, loss: 0.024031074717640877
step: 340, loss: 0.021024106070399284
step: 350, loss: 0.029894134029746056
step: 360, loss: 0.08120235055685043
epoch 13: dev_f1=0.7573333333333333, f1=0.7197802197802198, best_f1=0.7462686567164178
step: 0, loss: 0.001207804656587541
step: 10, loss: 0.00147708086296916
step: 20, loss: 0.0078000943176448345
step: 30, loss: 0.018768971785902977
step: 40, loss: 0.0024312646128237247
step: 50, loss: 0.011519676074385643
step: 60, loss: 0.024277592077851295
step: 70, loss: 0.00959776435047388
step: 80, loss: 0.040815673768520355
step: 90, loss: 0.0590454638004303
step: 100, loss: 0.02455126866698265
step: 110, loss: 0.04275549575686455
step: 120, loss: 0.08276192098855972
step: 130, loss: 0.08248205482959747
step: 140, loss: 0.004166676662862301
step: 150, loss: 0.010431989096105099
step: 160, loss: 0.026721391826868057
step: 170, loss: 0.03094465844333172
step: 180, loss: 0.001859955838881433
step: 190, loss: 0.018830474466085434
step: 200, loss: 0.00032415511668659747
step: 210, loss: 0.006763183046132326
step: 220, loss: 0.008841920644044876
step: 230, loss: 0.10884684324264526
step: 240, loss: 0.15018340945243835
step: 250, loss: 0.002498797606676817
step: 260, loss: 0.04176103323698044
step: 270, loss: 0.03933185338973999
step: 280, loss: 0.002214801963418722
step: 290, loss: 0.016125639900565147
step: 300, loss: 0.006206739693880081
step: 310, loss: 0.00837866310030222
step: 320, loss: 0.031601350754499435
step: 330, loss: 0.034952688962221146
step: 340, loss: 0.007399419788271189
step: 350, loss: 0.06628523766994476
step: 360, loss: 0.0008813341846689582
epoch 14: dev_f1=0.7587939698492463, f1=0.7352185089974292, best_f1=0.7462686567164178
step: 0, loss: 0.005383155308663845
step: 10, loss: 0.006841941736638546
step: 20, loss: 0.025824375450611115
step: 30, loss: 0.0002743528748396784
step: 40, loss: 0.00030159001471474767
step: 50, loss: 3.8238194974837825e-05
step: 60, loss: 0.000937321747187525
step: 70, loss: 0.00038893154123798013
step: 80, loss: 0.000350831396644935
step: 90, loss: 0.0011609041830524802
step: 100, loss: 0.07589183002710342
step: 110, loss: 0.002318776212632656
step: 120, loss: 0.008667008019983768
step: 130, loss: 0.07302550226449966
step: 140, loss: 0.06875711679458618
step: 150, loss: 0.034316737204790115
step: 160, loss: 0.002807050943374634
step: 170, loss: 0.0007637592498213053
step: 180, loss: 0.07518218457698822
step: 190, loss: 0.01622745208442211
step: 200, loss: 0.042866215109825134
step: 210, loss: 0.0010215475922450423
step: 220, loss: 0.013028011657297611
step: 230, loss: 0.005923574324697256
step: 240, loss: 0.00016464439977426082
step: 250, loss: 0.023144090548157692
step: 260, loss: 0.001501085003837943
step: 270, loss: 0.02889386937022209
step: 280, loss: 0.008249465376138687
step: 290, loss: 0.12114311009645462
step: 300, loss: 0.0010406257351860404
step: 310, loss: 0.0014480691170319915
step: 320, loss: 0.0014385709073394537
step: 330, loss: 0.004840000066906214
step: 340, loss: 0.011724003590643406
step: 350, loss: 0.07796985656023026
step: 360, loss: 0.01863952726125717
epoch 15: dev_f1=0.7700534759358288, f1=0.7342465753424658, best_f1=0.7462686567164178
step: 0, loss: 0.001285879174247384
step: 10, loss: 0.10412722826004028
step: 20, loss: 0.004832643549889326
step: 30, loss: 0.016542766243219376
step: 40, loss: 0.08442800492048264
step: 50, loss: 0.02382856234908104
step: 60, loss: 0.00016054941806942225
step: 70, loss: 0.04320141673088074
step: 80, loss: 0.021420413628220558
step: 90, loss: 0.26092270016670227
step: 100, loss: 7.087159610819072e-05
step: 110, loss: 0.003583426121622324
step: 120, loss: 0.001932172104716301
step: 130, loss: 0.004014011938124895
step: 140, loss: 0.0036061981227248907
step: 150, loss: 0.0006919686566106975
step: 160, loss: 0.01632360741496086
step: 170, loss: 0.06825890392065048
step: 180, loss: 0.0031312652863562107
step: 190, loss: 0.039353612810373306
step: 200, loss: 0.040572233498096466
step: 210, loss: 4.0185834222938865e-05
step: 220, loss: 0.0048139519058167934
step: 230, loss: 0.023260053247213364
step: 240, loss: 0.03287918493151665
step: 250, loss: 0.0005340413772501051
step: 260, loss: 0.00017057334480341524
step: 270, loss: 0.008297448046505451
step: 280, loss: 0.04113300144672394
step: 290, loss: 0.0012613587314262986
step: 300, loss: 0.05953994020819664
step: 310, loss: 0.011765168979763985
step: 320, loss: 0.0007998296641744673
step: 330, loss: 0.001051824539899826
step: 340, loss: 0.017876755446195602
step: 350, loss: 0.03155716881155968
step: 360, loss: 0.0004229465557727963
epoch 16: dev_f1=0.7560975609756098, f1=0.7341772151898734, best_f1=0.7462686567164178
step: 0, loss: 0.03243446722626686
step: 10, loss: 0.029974645003676414
step: 20, loss: 9.841976861935109e-05
step: 30, loss: 0.004236765205860138
step: 40, loss: 0.015867164358496666
step: 50, loss: 0.04133479297161102
step: 60, loss: 0.0837358757853508
step: 70, loss: 0.04343651980161667
step: 80, loss: 0.002289833500981331
step: 90, loss: 0.014262464828789234
step: 100, loss: 3.674764957395382e-05
step: 110, loss: 0.0008014087798073888
step: 120, loss: 0.04446794465184212
step: 130, loss: 0.04173095524311066
step: 140, loss: 0.024387914687395096
step: 150, loss: 0.041350435465574265
step: 160, loss: 0.03288818523287773
step: 170, loss: 0.03875041753053665
step: 180, loss: 0.03209180012345314
step: 190, loss: 0.047512371093034744
step: 200, loss: 0.0342227965593338
step: 210, loss: 0.015732405707240105
step: 220, loss: 8.876554056769237e-05
step: 230, loss: 1.9825769413728267e-05
step: 240, loss: 0.001379117602482438
step: 250, loss: 0.07479524612426758
step: 260, loss: 0.0005770822754129767
step: 270, loss: 0.024199752137064934
step: 280, loss: 0.0020958802197128534
step: 290, loss: 0.00013096501061227173
step: 300, loss: 0.002066045068204403
step: 310, loss: 0.04790142551064491
step: 320, loss: 0.03349826857447624
step: 330, loss: 0.041065942496061325
step: 340, loss: 0.011789403855800629
step: 350, loss: 4.554181941784918e-05
step: 360, loss: 0.0014326178934425116
epoch 17: dev_f1=0.7476190476190476, f1=0.7350000000000001, best_f1=0.7462686567164178
step: 0, loss: 0.02806214615702629
step: 10, loss: 2.7547366698854603e-05
step: 20, loss: 0.012783489190042019
step: 30, loss: 0.0008636877173557878
step: 40, loss: 0.0738220363855362
step: 50, loss: 0.0019337700214236975
step: 60, loss: 6.983547064010054e-05
step: 70, loss: 0.002361773978918791
step: 80, loss: 7.127506250981241e-05
step: 90, loss: 0.03219261392951012
step: 100, loss: 0.009557738900184631
step: 110, loss: 0.03904842212796211
step: 120, loss: 0.04821287468075752
step: 130, loss: 0.06911929696798325
step: 140, loss: 2.669480090844445e-05
step: 150, loss: 0.03186088427901268
step: 160, loss: 0.017184719443321228
step: 170, loss: 0.04933859407901764
step: 180, loss: 0.02967778593301773
step: 190, loss: 0.050824910402297974
step: 200, loss: 0.0005200519226491451
step: 210, loss: 0.02661350555717945
step: 220, loss: 0.009960423223674297
step: 230, loss: 0.0015333052724599838
step: 240, loss: 0.1390695422887802
step: 250, loss: 0.03752366825938225
step: 260, loss: 0.05007993429899216
step: 270, loss: 0.04480064660310745
step: 280, loss: 0.03993542864918709
step: 290, loss: 0.02367715910077095
step: 300, loss: 0.12741489708423615
step: 310, loss: 0.004650184419006109
step: 320, loss: 0.00012727266584988683
step: 330, loss: 6.442653830163181e-05
step: 340, loss: 0.0039036236703395844
step: 350, loss: 8.445794810540974e-05
step: 360, loss: 0.026145197451114655
epoch 18: dev_f1=0.7323232323232323, f1=0.7454068241469817, best_f1=0.7462686567164178
step: 0, loss: 0.026636909693479538
step: 10, loss: 0.10990926623344421
step: 20, loss: 0.010893224738538265
step: 30, loss: 0.025579864159226418
step: 40, loss: 0.0029472054447978735
step: 50, loss: 0.00843669194728136
step: 60, loss: 0.00017250778910238296
step: 70, loss: 0.007820404134690762
step: 80, loss: 0.0007766521302983165
step: 90, loss: 0.0035510393790900707
step: 100, loss: 0.00013612928160000592
step: 110, loss: 2.65303115156712e-05
step: 120, loss: 0.0005983167793601751
step: 130, loss: 8.098618855001405e-05
step: 140, loss: 0.00017276860307902098
step: 150, loss: 4.55187946499791e-05
step: 160, loss: 0.032698456197977066
step: 170, loss: 1.907699515868444e-05
step: 180, loss: 0.00012080537271685898
step: 190, loss: 0.00024133628176059574
step: 200, loss: 0.00027138146106153727
step: 210, loss: 0.004598196130245924
step: 220, loss: 0.06208816170692444
step: 230, loss: 0.016069946810603142
step: 240, loss: 0.019347641617059708
step: 250, loss: 0.0007880674675107002
step: 260, loss: 0.02612321823835373
step: 270, loss: 0.04501327499747276
step: 280, loss: 0.031018858775496483
step: 290, loss: 0.009931286796927452
step: 300, loss: 0.013728865422308445
step: 310, loss: 0.00013680936535820365
step: 320, loss: 0.017281144857406616
step: 330, loss: 3.802076025749557e-05
step: 340, loss: 0.009736359119415283
step: 350, loss: 0.00028597720665857196
step: 360, loss: 0.016641899943351746
epoch 19: dev_f1=0.7386934673366835, f1=0.7362924281984334, best_f1=0.7462686567164178
step: 0, loss: 3.056444620597176e-05
step: 10, loss: 0.016920164227485657
step: 20, loss: 0.0760098248720169
step: 30, loss: 0.023519501090049744
step: 40, loss: 0.03519506752490997
step: 50, loss: 0.018494214862585068
step: 60, loss: 0.0006480325246229768
step: 70, loss: 0.0012192222056910396
step: 80, loss: 3.406453106435947e-05
step: 90, loss: 0.026381174102425575
step: 100, loss: 0.0002486924931872636
step: 110, loss: 7.33275810489431e-05
step: 120, loss: 0.04358900710940361
step: 130, loss: 0.001290991436690092
step: 140, loss: 6.12644143984653e-05
step: 150, loss: 0.0009479634463787079
step: 160, loss: 0.0029603703878819942
step: 170, loss: 0.0003032510285265744
step: 180, loss: 0.0010808284860104322
step: 190, loss: 0.0024247111286967993
step: 200, loss: 0.031314417719841
step: 210, loss: 0.04769609123468399
step: 220, loss: 0.000144932433613576
step: 230, loss: 0.011433426290750504
step: 240, loss: 0.04745461419224739
step: 250, loss: 0.02097383327782154
step: 260, loss: 0.015091311186552048
step: 270, loss: 0.004403603263199329
step: 280, loss: 0.00018956948770210147
step: 290, loss: 0.018804943189024925
step: 300, loss: 0.04338417574763298
step: 310, loss: 0.07457861304283142
step: 320, loss: 0.00163729686755687
step: 330, loss: 0.0002927067980635911
step: 340, loss: 0.008251016028225422
step: 350, loss: 0.04373825713992119
step: 360, loss: 0.03323015943169594
epoch 20: dev_f1=0.7407407407407407, f1=0.7422680412371134, best_f1=0.7462686567164178
