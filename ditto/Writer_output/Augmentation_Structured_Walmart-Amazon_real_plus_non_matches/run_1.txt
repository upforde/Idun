cuda
Device: cuda
step: 0, loss: 0.624112606048584
step: 10, loss: 0.13812415301799774
step: 20, loss: 0.16095857322216034
step: 30, loss: 0.23695765435695648
step: 40, loss: 0.3067992329597473
step: 50, loss: 0.44108060002326965
step: 60, loss: 0.14451156556606293
step: 70, loss: 0.14063312113285065
step: 80, loss: 0.31662964820861816
step: 90, loss: 0.27176839113235474
step: 100, loss: 0.1338099241256714
step: 110, loss: 0.13725171983242035
step: 120, loss: 0.23030656576156616
step: 130, loss: 0.30662497878074646
step: 140, loss: 0.24055011570453644
step: 150, loss: 0.014530905522406101
step: 160, loss: 0.3050055205821991
step: 170, loss: 0.08375921100378036
step: 180, loss: 0.25237593054771423
step: 190, loss: 0.05700037255883217
step: 200, loss: 0.2506978511810303
step: 210, loss: 0.027404919266700745
step: 220, loss: 0.07533044368028641
step: 230, loss: 0.08540835976600647
step: 240, loss: 0.35621771216392517
step: 250, loss: 0.37090247869491577
step: 260, loss: 0.09217406064271927
step: 270, loss: 0.24215716123580933
step: 280, loss: 0.048761848360300064
step: 290, loss: 0.2768086791038513
step: 300, loss: 0.1760810911655426
step: 310, loss: 0.029461652040481567
step: 320, loss: 0.12929122149944305
step: 330, loss: 0.17764367163181305
step: 340, loss: 0.2774631679058075
step: 350, loss: 0.07972514629364014
step: 360, loss: 0.04058538004755974
epoch 1: dev_f1=0.654911838790932, f1=0.668354430379747, best_f1=0.668354430379747
step: 0, loss: 0.03095814771950245
step: 10, loss: 0.1124170571565628
step: 20, loss: 0.21969938278198242
step: 30, loss: 0.037989985197782516
step: 40, loss: 0.2572738528251648
step: 50, loss: 0.24924641847610474
step: 60, loss: 0.14775286614894867
step: 70, loss: 0.10500738024711609
step: 80, loss: 0.249508798122406
step: 90, loss: 0.1839352548122406
step: 100, loss: 0.06299718469381332
step: 110, loss: 0.07650292664766312
step: 120, loss: 0.1404123157262802
step: 130, loss: 0.18911176919937134
step: 140, loss: 0.15541160106658936
step: 150, loss: 0.08371514827013016
step: 160, loss: 0.17247317731380463
step: 170, loss: 0.10464143753051758
step: 180, loss: 0.24667122960090637
step: 190, loss: 0.03162003308534622
step: 200, loss: 0.12621860206127167
step: 210, loss: 0.2612725496292114
step: 220, loss: 0.1066083163022995
step: 230, loss: 0.4940303564071655
step: 240, loss: 0.04038175940513611
step: 250, loss: 0.10401637107133865
step: 260, loss: 0.19301210343837738
step: 270, loss: 0.017080675810575485
step: 280, loss: 0.15826405584812164
step: 290, loss: 0.2254176288843155
step: 300, loss: 0.1167936772108078
step: 310, loss: 0.20785212516784668
step: 320, loss: 0.03283272683620453
step: 330, loss: 0.1771261841058731
step: 340, loss: 0.05859728902578354
step: 350, loss: 0.11304005235433578
step: 360, loss: 0.06145070493221283
epoch 2: dev_f1=0.6880000000000001, f1=0.7340425531914895, best_f1=0.7340425531914895
step: 0, loss: 0.4194481074810028
step: 10, loss: 0.1417139172554016
step: 20, loss: 0.0867377445101738
step: 30, loss: 0.30417919158935547
step: 40, loss: 0.05987630784511566
step: 50, loss: 0.05133215710520744
step: 60, loss: 0.11982863396406174
step: 70, loss: 0.09534432739019394
step: 80, loss: 0.034784238785505295
step: 90, loss: 0.0937356948852539
step: 100, loss: 0.11133980005979538
step: 110, loss: 0.03793378919363022
step: 120, loss: 0.07914494723081589
step: 130, loss: 0.10722803324460983
step: 140, loss: 0.08850949257612228
step: 150, loss: 0.01099085621535778
step: 160, loss: 0.017524568364024162
step: 170, loss: 0.09803532063961029
step: 180, loss: 0.13535505533218384
step: 190, loss: 0.06953885406255722
step: 200, loss: 0.06366488337516785
step: 210, loss: 0.044021524488925934
step: 220, loss: 0.0023869334254413843
step: 230, loss: 0.0843951627612114
step: 240, loss: 0.09609755128622055
step: 250, loss: 0.13238948583602905
step: 260, loss: 0.06735202670097351
step: 270, loss: 0.07556131482124329
step: 280, loss: 0.12540386617183685
step: 290, loss: 0.10074248164892197
step: 300, loss: 0.07997206598520279
step: 310, loss: 0.11244430392980576
step: 320, loss: 0.22476044297218323
step: 330, loss: 0.06356066465377808
step: 340, loss: 0.03426704183220863
step: 350, loss: 0.11017323285341263
step: 360, loss: 0.10007299482822418
epoch 3: dev_f1=0.7178217821782177, f1=0.779746835443038, best_f1=0.779746835443038
step: 0, loss: 0.060129545629024506
step: 10, loss: 0.03633425012230873
step: 20, loss: 0.05393185466527939
step: 30, loss: 0.019167352467775345
step: 40, loss: 0.09240108728408813
step: 50, loss: 0.026259394362568855
step: 60, loss: 0.025139303877949715
step: 70, loss: 0.12799717485904694
step: 80, loss: 0.027142563834786415
step: 90, loss: 0.06690196692943573
step: 100, loss: 0.13751941919326782
step: 110, loss: 0.06799416989088058
step: 120, loss: 0.04186683148145676
step: 130, loss: 0.09468656033277512
step: 140, loss: 0.02069241553544998
step: 150, loss: 0.06401801854372025
step: 160, loss: 0.049517180770635605
step: 170, loss: 0.2253788709640503
step: 180, loss: 0.17293207347393036
step: 190, loss: 0.0518803708255291
step: 200, loss: 0.09503640234470367
step: 210, loss: 0.07310977578163147
step: 220, loss: 0.1078294888138771
step: 230, loss: 0.09524473547935486
step: 240, loss: 0.05410582199692726
step: 250, loss: 0.14747501909732819
step: 260, loss: 0.05865688994526863
step: 270, loss: 0.003583054756745696
step: 280, loss: 0.021533828228712082
step: 290, loss: 0.10476115345954895
step: 300, loss: 0.04404507577419281
step: 310, loss: 0.11016017198562622
step: 320, loss: 0.0577971413731575
step: 330, loss: 0.05410848557949066
step: 340, loss: 0.044749606400728226
step: 350, loss: 0.05132066458463669
step: 360, loss: 0.1735636293888092
epoch 4: dev_f1=0.7345844504021448, f1=0.7835616438356164, best_f1=0.7835616438356164
step: 0, loss: 0.05661838501691818
step: 10, loss: 0.034668177366256714
step: 20, loss: 0.03604941815137863
step: 30, loss: 0.1577049344778061
step: 40, loss: 0.09710311144590378
step: 50, loss: 0.019180787727236748
step: 60, loss: 0.002127604326233268
step: 70, loss: 0.06187232583761215
step: 80, loss: 0.045036736875772476
step: 90, loss: 0.0011904760031029582
step: 100, loss: 0.06354445964097977
step: 110, loss: 0.1283821165561676
step: 120, loss: 0.0752560943365097
step: 130, loss: 0.06957641243934631
step: 140, loss: 0.07700612396001816
step: 150, loss: 0.05905377119779587
step: 160, loss: 0.15298371016979218
step: 170, loss: 0.040310777723789215
step: 180, loss: 0.04009539633989334
step: 190, loss: 0.10749568790197372
step: 200, loss: 0.014437391422688961
step: 210, loss: 0.042345114052295685
step: 220, loss: 0.07942476123571396
step: 230, loss: 0.05287765711545944
step: 240, loss: 0.02502412348985672
step: 250, loss: 0.04678303375840187
step: 260, loss: 0.094493567943573
step: 270, loss: 0.16989916563034058
step: 280, loss: 0.06510715186595917
step: 290, loss: 0.013257290236651897
step: 300, loss: 0.09723398834466934
step: 310, loss: 0.08222789317369461
step: 320, loss: 0.0049778916873037815
step: 330, loss: 0.1457948237657547
step: 340, loss: 0.1460210084915161
step: 350, loss: 0.06411571055650711
step: 360, loss: 0.08810650557279587
epoch 5: dev_f1=0.7338129496402879, f1=0.7741935483870968, best_f1=0.7835616438356164
step: 0, loss: 0.023513058200478554
step: 10, loss: 0.06311194598674774
step: 20, loss: 0.07775337994098663
step: 30, loss: 0.012579704634845257
step: 40, loss: 0.04755854979157448
step: 50, loss: 0.03807961940765381
step: 60, loss: 0.08594323694705963
step: 70, loss: 0.044758860021829605
step: 80, loss: 0.02097298763692379
step: 90, loss: 0.08185038715600967
step: 100, loss: 0.0004252749204169959
step: 110, loss: 0.07570362091064453
step: 120, loss: 0.08583246916532516
step: 130, loss: 0.01973324827849865
step: 140, loss: 0.06218821182847023
step: 150, loss: 0.03686196729540825
step: 160, loss: 0.05785314738750458
step: 170, loss: 0.028357449918985367
step: 180, loss: 0.16457238793373108
step: 190, loss: 0.1152399554848671
step: 200, loss: 0.038953669369220734
step: 210, loss: 0.02164030261337757
step: 220, loss: 0.06290464848279953
step: 230, loss: 0.307826966047287
step: 240, loss: 0.23168210685253143
step: 250, loss: 0.04146739840507507
step: 260, loss: 0.06150678172707558
step: 270, loss: 0.00431067543104291
step: 280, loss: 0.002778853988274932
step: 290, loss: 0.07357024401426315
step: 300, loss: 0.0875605046749115
step: 310, loss: 0.05589605122804642
step: 320, loss: 0.15917272865772247
step: 330, loss: 0.1026153564453125
step: 340, loss: 0.06845267117023468
step: 350, loss: 0.11347706615924835
step: 360, loss: 0.0508689284324646
epoch 6: dev_f1=0.7238605898123326, f1=0.7554347826086956, best_f1=0.7835616438356164
step: 0, loss: 0.03144143894314766
step: 10, loss: 0.025846494361758232
step: 20, loss: 0.08333510905504227
step: 30, loss: 0.017553288489580154
step: 40, loss: 0.045008335262537
step: 50, loss: 0.05514833331108093
step: 60, loss: 0.02009551413357258
step: 70, loss: 0.018819503486156464
step: 80, loss: 0.040448982268571854
step: 90, loss: 0.0791427493095398
step: 100, loss: 0.00019842603069264442
step: 110, loss: 0.0068142712116241455
step: 120, loss: 0.07579460740089417
step: 130, loss: 0.03376605361700058
step: 140, loss: 0.017722779884934425
step: 150, loss: 0.009192614816129208
step: 160, loss: 0.15397909283638
step: 170, loss: 0.05788557976484299
step: 180, loss: 0.10480781644582748
step: 190, loss: 0.05712633207440376
step: 200, loss: 0.006908603012561798
step: 210, loss: 0.02893432416021824
step: 220, loss: 0.06018050014972687
step: 230, loss: 0.01912005804479122
step: 240, loss: 0.07469704747200012
step: 250, loss: 0.06165621802210808
step: 260, loss: 0.039977848529815674
step: 270, loss: 0.2424996942281723
step: 280, loss: 0.07758279889822006
step: 290, loss: 0.019819261506199837
step: 300, loss: 0.022838033735752106
step: 310, loss: 0.087239570915699
step: 320, loss: 0.06808154284954071
step: 330, loss: 0.04463600739836693
step: 340, loss: 0.020650938153266907
step: 350, loss: 0.010725583881139755
step: 360, loss: 0.03890308365225792
epoch 7: dev_f1=0.7315789473684211, f1=0.7700831024930748, best_f1=0.7835616438356164
step: 0, loss: 0.019744763150811195
step: 10, loss: 0.03204670175909996
step: 20, loss: 0.06133370101451874
step: 30, loss: 0.05054047331213951
step: 40, loss: 0.10129531472921371
step: 50, loss: 0.04267360270023346
step: 60, loss: 0.014601778239011765
step: 70, loss: 0.08355602622032166
step: 80, loss: 0.008973533287644386
step: 90, loss: 0.03615625947713852
step: 100, loss: 0.02653559483587742
step: 110, loss: 0.0677788034081459
step: 120, loss: 0.09675505012273788
step: 130, loss: 0.07422485947608948
step: 140, loss: 0.07586006820201874
step: 150, loss: 0.0016292947111651301
step: 160, loss: 0.05621911585330963
step: 170, loss: 0.012718413025140762
step: 180, loss: 0.0021792829502373934
step: 190, loss: 0.07945377379655838
step: 200, loss: 0.08109435439109802
step: 210, loss: 0.015819184482097626
step: 220, loss: 0.014301102608442307
step: 230, loss: 0.02433585189282894
step: 240, loss: 0.013558613136410713
step: 250, loss: 0.023678047582507133
step: 260, loss: 0.09732895344495773
step: 270, loss: 0.1058829128742218
step: 280, loss: 0.02597411349415779
step: 290, loss: 0.04078563302755356
step: 300, loss: 0.023052822798490524
step: 310, loss: 0.05005892738699913
step: 320, loss: 0.08945801854133606
step: 330, loss: 0.08042202144861221
step: 340, loss: 0.05859250947833061
step: 350, loss: 0.01896575465798378
step: 360, loss: 0.033987857401371
epoch 8: dev_f1=0.7228260869565217, f1=0.7840909090909091, best_f1=0.7835616438356164
step: 0, loss: 0.05444835498929024
step: 10, loss: 0.020882602781057358
step: 20, loss: 0.00015533113037236035
step: 30, loss: 0.01919320784509182
step: 40, loss: 0.013761269859969616
step: 50, loss: 0.14370420575141907
step: 60, loss: 0.014260971918702126
step: 70, loss: 0.015129368752241135
step: 80, loss: 0.11784268915653229
step: 90, loss: 0.001624362193979323
step: 100, loss: 0.06529063731431961
step: 110, loss: 0.001408897340297699
step: 120, loss: 0.019029174000024796
step: 130, loss: 0.07471248507499695
step: 140, loss: 0.12807638943195343
step: 150, loss: 0.027409860864281654
step: 160, loss: 0.01771995611488819
step: 170, loss: 0.1996065378189087
step: 180, loss: 0.00631803460419178
step: 190, loss: 0.04958268254995346
step: 200, loss: 0.1870630979537964
step: 210, loss: 0.08894804120063782
step: 220, loss: 0.03161057084798813
step: 230, loss: 0.026135453954339027
step: 240, loss: 0.09652558714151382
step: 250, loss: 0.08446909487247467
step: 260, loss: 0.08352535963058472
step: 270, loss: 0.06331045180559158
step: 280, loss: 0.030628452077507973
step: 290, loss: 0.051693741232156754
step: 300, loss: 0.03169623017311096
step: 310, loss: 0.03835155814886093
step: 320, loss: 0.0522502101957798
step: 330, loss: 0.05604521557688713
step: 340, loss: 0.01912124827504158
step: 350, loss: 0.08008798211812973
step: 360, loss: 0.07122233510017395
epoch 9: dev_f1=0.7446808510638299, f1=0.7555555555555554, best_f1=0.7555555555555554
step: 0, loss: 0.025562047958374023
step: 10, loss: 0.05419311299920082
step: 20, loss: 0.03775303065776825
step: 30, loss: 0.05369050055742264
step: 40, loss: 0.05033313110470772
step: 50, loss: 0.008759468793869019
step: 60, loss: 0.014869905076920986
step: 70, loss: 0.028414994478225708
step: 80, loss: 0.044081155210733414
step: 90, loss: 0.03549377992749214
step: 100, loss: 0.08908510953187943
step: 110, loss: 0.0002074333606287837
step: 120, loss: 0.036296214908361435
step: 130, loss: 0.05954410880804062
step: 140, loss: 0.07237175852060318
step: 150, loss: 0.01089921873062849
step: 160, loss: 0.032203204929828644
step: 170, loss: 0.08963816612958908
step: 180, loss: 0.012263997457921505
step: 190, loss: 0.015248817391693592
step: 200, loss: 0.08600595593452454
step: 210, loss: 0.02675052545964718
step: 220, loss: 0.108609639108181
step: 230, loss: 0.04120844975113869
step: 240, loss: 0.027688410133123398
step: 250, loss: 0.032412994652986526
step: 260, loss: 0.01062417309731245
step: 270, loss: 0.07417627424001694
step: 280, loss: 0.0033471807837486267
step: 290, loss: 0.02188919484615326
step: 300, loss: 0.0548907145857811
step: 310, loss: 0.011215879581868649
step: 320, loss: 0.045650240033864975
step: 330, loss: 0.029756799340248108
step: 340, loss: 0.005056732799857855
step: 350, loss: 0.03708649054169655
step: 360, loss: 0.022711187601089478
epoch 10: dev_f1=0.724935732647815, f1=0.7559055118110236, best_f1=0.7555555555555554
step: 0, loss: 0.045534662902355194
step: 10, loss: 0.021814260631799698
step: 20, loss: 0.0075879814103245735
step: 30, loss: 0.04992489889264107
step: 40, loss: 0.0011967261089012027
step: 50, loss: 0.020134495571255684
step: 60, loss: 0.0021639945916831493
step: 70, loss: 0.02555268444120884
step: 80, loss: 0.1180315688252449
step: 90, loss: 0.008906080387532711
step: 100, loss: 0.003907959908246994
step: 110, loss: 0.15281152725219727
step: 120, loss: 0.01666070893406868
step: 130, loss: 0.06383604556322098
step: 140, loss: 0.00025004500639624894
step: 150, loss: 0.22949610650539398
step: 160, loss: 0.08000289648771286
step: 170, loss: 0.02558518946170807
step: 180, loss: 0.007115633692592382
step: 190, loss: 0.0002858780790120363
step: 200, loss: 0.11788366734981537
step: 210, loss: 0.14468874037265778
step: 220, loss: 0.016020214185118675
step: 230, loss: 0.07652981579303741
step: 240, loss: 0.06395213305950165
step: 250, loss: 0.03415639325976372
step: 260, loss: 0.024838130921125412
step: 270, loss: 0.015091429464519024
step: 280, loss: 0.024707067757844925
step: 290, loss: 0.01334658358246088
step: 300, loss: 0.00019944137602578849
step: 310, loss: 7.10930980858393e-05
step: 320, loss: 0.08610711246728897
step: 330, loss: 0.01205905806273222
step: 340, loss: 0.009086268953979015
step: 350, loss: 0.0188408512622118
step: 360, loss: 0.031710006296634674
epoch 11: dev_f1=0.7575757575757577, f1=0.7636363636363637, best_f1=0.7636363636363637
step: 0, loss: 0.004975263029336929
step: 10, loss: 0.04392681643366814
step: 20, loss: 0.03148733824491501
step: 30, loss: 0.0939704105257988
step: 40, loss: 0.01112847588956356
step: 50, loss: 0.06851671636104584
step: 60, loss: 0.05193950980901718
step: 70, loss: 0.04816630855202675
step: 80, loss: 0.03424612805247307
step: 90, loss: 0.03621901571750641
step: 100, loss: 0.03761792927980423
step: 110, loss: 0.018904909491539
step: 120, loss: 0.033266425132751465
step: 130, loss: 0.0025363899767398834
step: 140, loss: 0.0033318561036139727
step: 150, loss: 0.031871598213911057
step: 160, loss: 0.0934567004442215
step: 170, loss: 0.02715594507753849
step: 180, loss: 0.01853281818330288
step: 190, loss: 0.025498420000076294
step: 200, loss: 0.003228765679523349
step: 210, loss: 0.010711965151131153
step: 220, loss: 0.03852224349975586
step: 230, loss: 0.0713360384106636
step: 240, loss: 0.09831945598125458
step: 250, loss: 0.024825094267725945
step: 260, loss: 0.08257351815700531
step: 270, loss: 0.08945746719837189
step: 280, loss: 0.09903255850076675
step: 290, loss: 0.10609818249940872
step: 300, loss: 0.024923941120505333
step: 310, loss: 0.09465227276086807
step: 320, loss: 0.023018278181552887
step: 330, loss: 0.10080049932003021
step: 340, loss: 0.004554940387606621
step: 350, loss: 0.05890240892767906
step: 360, loss: 0.04100771248340607
epoch 12: dev_f1=0.7296137339055794, f1=0.7301587301587301, best_f1=0.7636363636363637
step: 0, loss: 0.09434091299772263
step: 10, loss: 0.03691666200757027
step: 20, loss: 0.015015414915978909
step: 30, loss: 0.05668959766626358
step: 40, loss: 0.033479705452919006
step: 50, loss: 0.07528162002563477
step: 60, loss: 0.07611870020627975
step: 70, loss: 0.008914993144571781
step: 80, loss: 0.0661727637052536
step: 90, loss: 0.0530172735452652
step: 100, loss: 0.009705339558422565
step: 110, loss: 0.03422034531831741
step: 120, loss: 0.05564327910542488
step: 130, loss: 0.022846944630146027
step: 140, loss: 0.029234446585178375
step: 150, loss: 0.009053761139512062
step: 160, loss: 0.07386229932308197
step: 170, loss: 0.014571442268788815
step: 180, loss: 0.040253181010484695
step: 190, loss: 0.01567223109304905
step: 200, loss: 0.0015904931351542473
step: 210, loss: 0.004049971699714661
step: 220, loss: 0.0012206599349156022
step: 230, loss: 0.04996589943766594
step: 240, loss: 0.000463335367385298
step: 250, loss: 0.001537965377792716
step: 260, loss: 0.015257044695317745
step: 270, loss: 0.026660131290555
step: 280, loss: 0.05048184096813202
step: 290, loss: 0.020536819472908974
step: 300, loss: 0.14234256744384766
step: 310, loss: 0.018987713381648064
step: 320, loss: 0.002878258703276515
step: 330, loss: 0.04955049604177475
step: 340, loss: 0.00029655403341166675
step: 350, loss: 0.08126910775899887
step: 360, loss: 0.0235695019364357
epoch 13: dev_f1=0.7258485639686684, f1=0.7621621621621621, best_f1=0.7636363636363637
step: 0, loss: 5.107710603624582e-05
step: 10, loss: 0.049527060240507126
step: 20, loss: 0.051635582000017166
step: 30, loss: 0.018685799092054367
step: 40, loss: 0.008678819984197617
step: 50, loss: 0.004612217657268047
step: 60, loss: 0.05024028569459915
step: 70, loss: 0.03025852143764496
step: 80, loss: 0.05732100084424019
step: 90, loss: 0.0036689394619315863
step: 100, loss: 0.06732095777988434
step: 110, loss: 0.12785741686820984
step: 120, loss: 0.0021984402555972338
step: 130, loss: 0.0004948540590703487
step: 140, loss: 0.0002633296826388687
step: 150, loss: 0.10039510577917099
step: 160, loss: 0.0028171215672045946
step: 170, loss: 0.0022355499677360058
step: 180, loss: 0.07572924345731735
step: 190, loss: 0.06061413884162903
step: 200, loss: 0.02679828368127346
step: 210, loss: 0.053792934864759445
step: 220, loss: 0.01874568872153759
step: 230, loss: 0.04784078523516655
step: 240, loss: 0.06564246863126755
step: 250, loss: 0.051272813230752945
step: 260, loss: 0.0009649053099565208
step: 270, loss: 0.08553478121757507
step: 280, loss: 0.07814371585845947
step: 290, loss: 0.024575645104050636
step: 300, loss: 0.06183602660894394
step: 310, loss: 0.027942907065153122
step: 320, loss: 0.023083196952939034
step: 330, loss: 0.04823646694421768
step: 340, loss: 0.025373676791787148
step: 350, loss: 0.0012482373276725411
step: 360, loss: 0.0009524521301500499
epoch 14: dev_f1=0.7272727272727272, f1=0.7563451776649746, best_f1=0.7636363636363637
step: 0, loss: 0.009999114088714123
step: 10, loss: 0.013036210089921951
step: 20, loss: 0.056515078991651535
step: 30, loss: 0.000566408853046596
step: 40, loss: 0.012347444891929626
step: 50, loss: 0.001199896913021803
step: 60, loss: 0.0016840463504195213
step: 70, loss: 0.00036302648368291557
step: 80, loss: 8.206373604480177e-05
step: 90, loss: 0.015849411487579346
step: 100, loss: 0.04436662048101425
step: 110, loss: 0.0008182225283235312
step: 120, loss: 0.08826728910207748
step: 130, loss: 0.048196904361248016
step: 140, loss: 0.0001031726787914522
step: 150, loss: 0.04401582479476929
step: 160, loss: 0.007574792951345444
step: 170, loss: 0.025275859981775284
step: 180, loss: 0.004046098794788122
step: 190, loss: 0.02330956608057022
step: 200, loss: 0.014717288315296173
step: 210, loss: 0.003973174840211868
step: 220, loss: 0.024122117087244987
step: 230, loss: 0.016046546399593353
step: 240, loss: 0.0011343887308612466
step: 250, loss: 0.004330899100750685
step: 260, loss: 0.07793665677309036
step: 270, loss: 6.657015183009207e-05
step: 280, loss: 0.01986980251967907
step: 290, loss: 0.00020610718638636172
step: 300, loss: 0.0002643677871674299
step: 310, loss: 0.035318706184625626
step: 320, loss: 0.01589103601872921
step: 330, loss: 0.06536504626274109
step: 340, loss: 0.01915442943572998
step: 350, loss: 0.05025094002485275
step: 360, loss: 0.02867748960852623
epoch 15: dev_f1=0.7172413793103449, f1=0.7440758293838864, best_f1=0.7636363636363637
step: 0, loss: 0.0224294513463974
step: 10, loss: 0.05649340897798538
step: 20, loss: 0.0009081608150154352
step: 30, loss: 0.025253424420952797
step: 40, loss: 0.0015208401018753648
step: 50, loss: 0.02910083904862404
step: 60, loss: 0.041887231171131134
step: 70, loss: 0.01821705512702465
step: 80, loss: 0.05840149521827698
step: 90, loss: 0.026034832000732422
step: 100, loss: 0.004302630666643381
step: 110, loss: 0.013812081888318062
step: 120, loss: 0.0006562400376424193
step: 130, loss: 0.006561451591551304
step: 140, loss: 0.0004627789603546262
step: 150, loss: 0.04030299559235573
step: 160, loss: 0.018239082768559456
step: 170, loss: 0.0023010752629488707
step: 180, loss: 0.001124136266298592
step: 190, loss: 0.007549590431153774
step: 200, loss: 0.056631896644830704
step: 210, loss: 0.023458823561668396
step: 220, loss: 0.031236302107572556
step: 230, loss: 0.011584076099097729
step: 240, loss: 0.00033493596129119396
step: 250, loss: 0.04048851504921913
step: 260, loss: 7.178087253123522e-05
step: 270, loss: 0.028706736862659454
step: 280, loss: 0.0027985796332359314
step: 290, loss: 0.04286361113190651
step: 300, loss: 0.03332260251045227
step: 310, loss: 0.005437129642814398
step: 320, loss: 0.011990579776465893
step: 330, loss: 0.07074189931154251
step: 340, loss: 0.03144541755318642
step: 350, loss: 0.0022953315638005733
step: 360, loss: 0.00012945989146828651
epoch 16: dev_f1=0.7021276595744682, f1=0.7139689578713968, best_f1=0.7636363636363637
step: 0, loss: 0.0016167652793228626
step: 10, loss: 0.01586221344769001
step: 20, loss: 0.03365376591682434
step: 30, loss: 0.022207321599125862
step: 40, loss: 0.006831646431237459
step: 50, loss: 0.03310887888073921
step: 60, loss: 0.0006002928130328655
step: 70, loss: 0.09587693959474564
step: 80, loss: 0.031911931931972504
step: 90, loss: 0.0008452906622551382
step: 100, loss: 8.502029231749475e-05
step: 110, loss: 0.0007624833378940821
step: 120, loss: 0.0030545745976269245
step: 130, loss: 0.029845306649804115
step: 140, loss: 0.0054790303111076355
step: 150, loss: 0.001656810869462788
step: 160, loss: 0.0032459665089845657
step: 170, loss: 0.0009635007008910179
step: 180, loss: 0.04067593067884445
step: 190, loss: 0.0006271061720326543
step: 200, loss: 0.02018844708800316
step: 210, loss: 0.02237987518310547
step: 220, loss: 0.017888719215989113
step: 230, loss: 0.04751531407237053
step: 240, loss: 0.03179161250591278
step: 250, loss: 0.027422398328781128
step: 260, loss: 0.014921060763299465
step: 270, loss: 0.01939900778234005
step: 280, loss: 0.0323244072496891
step: 290, loss: 0.018976956605911255
step: 300, loss: 0.03535575047135353
step: 310, loss: 0.0031653668265789747
step: 320, loss: 0.00013142537500243634
step: 330, loss: 0.03538065031170845
step: 340, loss: 0.0004308271163608879
step: 350, loss: 0.1126798465847969
step: 360, loss: 0.0006022337474860251
epoch 17: dev_f1=0.7088607594936709, f1=0.7486910994764396, best_f1=0.7636363636363637
step: 0, loss: 0.0001132667894125916
step: 10, loss: 0.03324184939265251
step: 20, loss: 0.038808126002550125
step: 30, loss: 0.0187276229262352
step: 40, loss: 0.032829493284225464
step: 50, loss: 0.025654785335063934
step: 60, loss: 0.002583480905741453
step: 70, loss: 0.003396923653781414
step: 80, loss: 0.02264934964478016
step: 90, loss: 0.008972459472715855
step: 100, loss: 0.019783001393079758
step: 110, loss: 0.037527572363615036
step: 120, loss: 0.031203618273139
step: 130, loss: 0.035409633070230484
step: 140, loss: 0.052044160664081573
step: 150, loss: 0.0015595529694110155
step: 160, loss: 0.02175983227789402
step: 170, loss: 0.04080408066511154
step: 180, loss: 0.00027579235029406846
step: 190, loss: 0.05154978856444359
step: 200, loss: 0.01689087226986885
step: 210, loss: 0.08249470591545105
step: 220, loss: 0.007537266239523888
step: 230, loss: 0.048865266144275665
step: 240, loss: 0.02403292991220951
step: 250, loss: 0.0328403078019619
step: 260, loss: 0.0005771191790699959
step: 270, loss: 3.3724292734405026e-05
step: 280, loss: 0.0011050053872168064
step: 290, loss: 0.04873872175812721
step: 300, loss: 0.10518266260623932
step: 310, loss: 0.07763932645320892
step: 320, loss: 0.08531219512224197
step: 330, loss: 0.06357652693986893
step: 340, loss: 0.04706278815865517
step: 350, loss: 0.0015775274951010942
step: 360, loss: 0.027431588619947433
epoch 18: dev_f1=0.717948717948718, f1=0.7461139896373058, best_f1=0.7636363636363637
step: 0, loss: 0.017665894702076912
step: 10, loss: 0.025613943114876747
step: 20, loss: 0.04202824831008911
step: 30, loss: 0.02318137139081955
step: 40, loss: 3.9233509596670046e-05
step: 50, loss: 0.04656879976391792
step: 60, loss: 0.00018218831974081695
step: 70, loss: 0.00010364585614297539
step: 80, loss: 0.0186702199280262
step: 90, loss: 0.0002085357264149934
step: 100, loss: 0.021010365337133408
step: 110, loss: 3.38620702677872e-05
step: 120, loss: 0.030194194987416267
step: 130, loss: 0.010670531541109085
step: 140, loss: 0.00019242992857471108
step: 150, loss: 0.00028288454632274806
step: 160, loss: 0.054925959557294846
step: 170, loss: 0.0007407788652926683
step: 180, loss: 0.004553972277790308
step: 190, loss: 0.02654496394097805
step: 200, loss: 9.426427277503535e-05
step: 210, loss: 4.239450572640635e-05
step: 220, loss: 0.000266718037892133
step: 230, loss: 0.014505283907055855
step: 240, loss: 0.050871342420578
step: 250, loss: 0.0413486510515213
step: 260, loss: 0.031644199043512344
step: 270, loss: 0.0002934128569904715
step: 280, loss: 0.035963788628578186
step: 290, loss: 0.022762050852179527
step: 300, loss: 0.05894919112324715
step: 310, loss: 0.0012802022974938154
step: 320, loss: 0.0015515106497332454
step: 330, loss: 0.0009158800239674747
step: 340, loss: 0.20738345384597778
step: 350, loss: 0.014549869112670422
step: 360, loss: 0.031678471714258194
epoch 19: dev_f1=0.7182044887780549, f1=0.7480916030534353, best_f1=0.7636363636363637
step: 0, loss: 0.044636890292167664
step: 10, loss: 0.02208827994763851
step: 20, loss: 0.11047423630952835
step: 30, loss: 0.0012389183975756168
step: 40, loss: 0.03893361613154411
step: 50, loss: 0.00010736154217738658
step: 60, loss: 0.009030794724822044
step: 70, loss: 7.597688818350434e-05
step: 80, loss: 0.015349055640399456
step: 90, loss: 3.6540033761411905e-05
step: 100, loss: 0.0003833195078186691
step: 110, loss: 0.023295436054468155
step: 120, loss: 0.01708945818245411
step: 130, loss: 0.000926672131754458
step: 140, loss: 0.05507969856262207
step: 150, loss: 2.7883383154403418e-05
step: 160, loss: 2.992446934513282e-05
step: 170, loss: 0.00033290861756540835
step: 180, loss: 0.0017544581787660718
step: 190, loss: 0.06649503111839294
step: 200, loss: 0.0009252105373889208
step: 210, loss: 0.01344823744148016
step: 220, loss: 0.0031990930438041687
step: 230, loss: 2.6579549739835784e-05
step: 240, loss: 0.03872270882129669
step: 250, loss: 2.7033966034650803e-05
step: 260, loss: 3.277061478001997e-05
step: 270, loss: 0.0003286514547653496
step: 280, loss: 0.00688908388838172
step: 290, loss: 0.0216823723167181
step: 300, loss: 0.037495292723178864
step: 310, loss: 0.02994389459490776
step: 320, loss: 0.031730182468891144
step: 330, loss: 0.0011472486658021808
step: 340, loss: 0.023875782266259193
step: 350, loss: 0.030189799144864082
step: 360, loss: 3.728903175215237e-05
epoch 20: dev_f1=0.7109974424552431, f1=0.744186046511628, best_f1=0.7636363636363637
