cuda
Device: cuda
step: 0, loss: 0.8230361342430115
step: 10, loss: 0.3212764859199524
step: 20, loss: 0.13601163029670715
step: 30, loss: 0.1380680799484253
step: 40, loss: 0.3032180368900299
step: 50, loss: 0.1308535784482956
step: 60, loss: 0.132689967751503
step: 70, loss: 0.23068523406982422
step: 80, loss: 0.43887007236480713
step: 90, loss: 0.39627334475517273
step: 100, loss: 0.22711001336574554
step: 110, loss: 0.2483469545841217
step: 120, loss: 0.034941598773002625
step: 130, loss: 0.314727246761322
step: 140, loss: 0.229282408952713
step: 150, loss: 0.13483496010303497
step: 160, loss: 0.17586849629878998
step: 170, loss: 0.12481722235679626
step: 180, loss: 0.30672115087509155
step: 190, loss: 0.15645286440849304
step: 200, loss: 0.2725083529949188
step: 210, loss: 0.24245920777320862
step: 220, loss: 0.10284628719091415
step: 230, loss: 0.0845772847533226
step: 240, loss: 0.22457879781723022
step: 250, loss: 0.0711127445101738
step: 260, loss: 0.24861612915992737
step: 270, loss: 0.06576690822839737
step: 280, loss: 0.2526290714740753
step: 290, loss: 0.019150041043758392
step: 300, loss: 0.039870940148830414
step: 310, loss: 0.19730648398399353
step: 320, loss: 0.33077579736709595
step: 330, loss: 0.18375901877880096
step: 340, loss: 0.0074223862029612064
step: 350, loss: 0.06359635293483734
step: 360, loss: 0.2097344994544983
epoch 1: dev_f1=0.6312849162011173, f1=0.6908077994428968, best_f1=0.6908077994428968
step: 0, loss: 0.10887956619262695
step: 10, loss: 0.1015583872795105
step: 20, loss: 0.12981028854846954
step: 30, loss: 0.20532047748565674
step: 40, loss: 0.19809003174304962
step: 50, loss: 0.055038221180438995
step: 60, loss: 0.13725727796554565
step: 70, loss: 0.1413031965494156
step: 80, loss: 0.22353895008563995
step: 90, loss: 0.02352198027074337
step: 100, loss: 0.0744348093867302
step: 110, loss: 0.1354283094406128
step: 120, loss: 0.4939362108707428
step: 130, loss: 0.3388561010360718
step: 140, loss: 0.016791416332125664
step: 150, loss: 0.07200518995523453
step: 160, loss: 0.008630121126770973
step: 170, loss: 0.03559247776865959
step: 180, loss: 0.22455015778541565
step: 190, loss: 0.04822564870119095
step: 200, loss: 0.04780399799346924
step: 210, loss: 0.20108859241008759
step: 220, loss: 0.048470284789800644
step: 230, loss: 0.14713133871555328
step: 240, loss: 0.005604904610663652
step: 250, loss: 0.1099572703242302
step: 260, loss: 0.19391076266765594
step: 270, loss: 0.1593092679977417
step: 280, loss: 0.09670375287532806
step: 290, loss: 0.03011447750031948
step: 300, loss: 0.06823401898145676
step: 310, loss: 0.09586002677679062
step: 320, loss: 0.04837345331907272
step: 330, loss: 0.035596590489149094
step: 340, loss: 0.04204482585191727
step: 350, loss: 0.16224363446235657
step: 360, loss: 0.17558933794498444
epoch 2: dev_f1=0.7225433526011561, f1=0.7299703264094957, best_f1=0.7299703264094957
step: 0, loss: 0.08834949880838394
step: 10, loss: 0.06719796359539032
step: 20, loss: 0.038885846734046936
step: 30, loss: 0.08440250158309937
step: 40, loss: 0.041758373379707336
step: 50, loss: 0.05273748189210892
step: 60, loss: 0.11359599977731705
step: 70, loss: 0.08544378727674484
step: 80, loss: 0.07561939209699631
step: 90, loss: 0.038971271365880966
step: 100, loss: 0.028876375406980515
step: 110, loss: 0.06097050756216049
step: 120, loss: 0.0247368011623621
step: 130, loss: 0.1681518256664276
step: 140, loss: 0.016148259863257408
step: 150, loss: 0.18130674958229065
step: 160, loss: 0.049557290971279144
step: 170, loss: 0.0033497423864901066
step: 180, loss: 0.10335932672023773
step: 190, loss: 0.14563345909118652
step: 200, loss: 0.0608658641576767
step: 210, loss: 0.07960111647844315
step: 220, loss: 0.03842908889055252
step: 230, loss: 0.11349665373563766
step: 240, loss: 0.18440833687782288
step: 250, loss: 0.0526590570807457
step: 260, loss: 0.09213481843471527
step: 270, loss: 0.1461033821105957
step: 280, loss: 0.0512697696685791
step: 290, loss: 0.07232457399368286
step: 300, loss: 0.229305237531662
step: 310, loss: 0.0392359234392643
step: 320, loss: 0.07852457463741302
step: 330, loss: 0.12185024470090866
step: 340, loss: 0.03647999092936516
step: 350, loss: 0.03339088708162308
step: 360, loss: 0.13892394304275513
epoch 3: dev_f1=0.7214854111405835, f1=0.7365439093484418, best_f1=0.7299703264094957
step: 0, loss: 0.04005924612283707
step: 10, loss: 0.08119581639766693
step: 20, loss: 0.23360753059387207
step: 30, loss: 0.09344781935214996
step: 40, loss: 0.1597442775964737
step: 50, loss: 0.07606010138988495
step: 60, loss: 0.08373414725065231
step: 70, loss: 0.0771663635969162
step: 80, loss: 0.07514932006597519
step: 90, loss: 0.3016248643398285
step: 100, loss: 0.03513726592063904
step: 110, loss: 0.12284426391124725
step: 120, loss: 0.018464231863617897
step: 130, loss: 0.07593157887458801
step: 140, loss: 0.03547259792685509
step: 150, loss: 0.11372608691453934
step: 160, loss: 0.08158396184444427
step: 170, loss: 0.021041424944996834
step: 180, loss: 0.10266627371311188
step: 190, loss: 0.07702932506799698
step: 200, loss: 0.09456530213356018
step: 210, loss: 0.1293204426765442
step: 220, loss: 0.01676783338189125
step: 230, loss: 0.11493615061044693
step: 240, loss: 0.08088695257902145
step: 250, loss: 0.1343548595905304
step: 260, loss: 0.08531835675239563
step: 270, loss: 0.02196255512535572
step: 280, loss: 0.07413162291049957
step: 290, loss: 0.01282620057463646
step: 300, loss: 0.015459073707461357
step: 310, loss: 0.12969444692134857
step: 320, loss: 0.04030192270874977
step: 330, loss: 0.15683802962303162
step: 340, loss: 0.018518919125199318
step: 350, loss: 0.044971588999032974
step: 360, loss: 0.17456015944480896
epoch 4: dev_f1=0.6702702702702703, f1=0.7142857142857142, best_f1=0.7299703264094957
step: 0, loss: 0.1039736196398735
step: 10, loss: 0.028986260294914246
step: 20, loss: 0.05830174311995506
step: 30, loss: 0.0544586218893528
step: 40, loss: 0.061619359999895096
step: 50, loss: 0.1230715662240982
step: 60, loss: 0.12444408982992172
step: 70, loss: 0.010709856636822224
step: 80, loss: 0.05791587382555008
step: 90, loss: 0.13490448892116547
step: 100, loss: 0.12935088574886322
step: 110, loss: 0.013939282856881618
step: 120, loss: 0.036754362285137177
step: 130, loss: 0.14396391808986664
step: 140, loss: 0.016498025506734848
step: 150, loss: 0.0029588406905531883
step: 160, loss: 0.014063962735235691
step: 170, loss: 0.055396754294633865
step: 180, loss: 0.08414851874113083
step: 190, loss: 0.04967226833105087
step: 200, loss: 0.0033558199647814035
step: 210, loss: 0.18275684118270874
step: 220, loss: 0.07769544422626495
step: 230, loss: 0.013736193999648094
step: 240, loss: 0.01923513598740101
step: 250, loss: 0.05615846440196037
step: 260, loss: 0.10251530259847641
step: 270, loss: 0.06845252960920334
step: 280, loss: 0.00648722006008029
step: 290, loss: 0.014043275266885757
step: 300, loss: 0.08016505092382431
step: 310, loss: 0.026341868564486504
step: 320, loss: 0.062111347913742065
step: 330, loss: 0.08978290110826492
step: 340, loss: 0.015333810821175575
step: 350, loss: 0.062116287648677826
step: 360, loss: 0.07900766283273697
epoch 5: dev_f1=0.7374005305039788, f1=0.7554347826086956, best_f1=0.7554347826086956
step: 0, loss: 0.02384701371192932
step: 10, loss: 0.07631514221429825
step: 20, loss: 0.05107870325446129
step: 30, loss: 0.058511003851890564
step: 40, loss: 0.01830059476196766
step: 50, loss: 0.026704981923103333
step: 60, loss: 0.012537307105958462
step: 70, loss: 0.005739414133131504
step: 80, loss: 0.041590359061956406
step: 90, loss: 0.02087216079235077
step: 100, loss: 0.12116003036499023
step: 110, loss: 0.016539113596081734
step: 120, loss: 0.024225926026701927
step: 130, loss: 0.005942661780864
step: 140, loss: 0.07795737683773041
step: 150, loss: 0.036270443350076675
step: 160, loss: 0.1264650523662567
step: 170, loss: 0.028753697872161865
step: 180, loss: 0.07010155916213989
step: 190, loss: 0.08760058134794235
step: 200, loss: 0.06271737813949585
step: 210, loss: 0.02128387987613678
step: 220, loss: 0.05753043293952942
step: 230, loss: 0.018720123916864395
step: 240, loss: 0.04947929084300995
step: 250, loss: 0.0832420140504837
step: 260, loss: 0.045422352850437164
step: 270, loss: 0.04684283956885338
step: 280, loss: 0.06815336644649506
step: 290, loss: 0.04804924502968788
step: 300, loss: 0.17866012454032898
step: 310, loss: 0.195336252450943
step: 320, loss: 0.11562423408031464
step: 330, loss: 0.06321831792593002
step: 340, loss: 0.017550257965922356
step: 350, loss: 0.11408083885908127
step: 360, loss: 0.02860369347035885
epoch 6: dev_f1=0.6876640419947506, f1=0.7130919220055711, best_f1=0.7554347826086956
step: 0, loss: 0.017640093341469765
step: 10, loss: 0.03769677132368088
step: 20, loss: 0.11381471902132034
step: 30, loss: 0.0877048671245575
step: 40, loss: 0.18462255597114563
step: 50, loss: 0.013441668823361397
step: 60, loss: 0.04430646821856499
step: 70, loss: 0.0750025063753128
step: 80, loss: 0.016891252249479294
step: 90, loss: 0.1095011755824089
step: 100, loss: 0.00016317643166985363
step: 110, loss: 0.025690147653222084
step: 120, loss: 0.035916585475206375
step: 130, loss: 0.023733973503112793
step: 140, loss: 0.028352858498692513
step: 150, loss: 0.0006677070050500333
step: 160, loss: 0.0022798290010541677
step: 170, loss: 0.00018662303045857698
step: 180, loss: 0.14521567523479462
step: 190, loss: 0.06687185913324356
step: 200, loss: 0.03841950744390488
step: 210, loss: 0.1756971925497055
step: 220, loss: 0.011928996071219444
step: 230, loss: 0.0728086307644844
step: 240, loss: 0.06719358265399933
step: 250, loss: 0.038363710045814514
step: 260, loss: 9.28970257518813e-05
step: 270, loss: 0.16431981325149536
step: 280, loss: 0.022673849016427994
step: 290, loss: 0.01630793884396553
step: 300, loss: 0.05208846554160118
step: 310, loss: 0.16152414679527283
step: 320, loss: 0.03392302989959717
step: 330, loss: 0.07470591366291046
step: 340, loss: 0.0681174024939537
step: 350, loss: 0.03082817979156971
step: 360, loss: 0.12849166989326477
epoch 7: dev_f1=0.7146401985111663, f1=0.7254408060453401, best_f1=0.7554347826086956
step: 0, loss: 0.021407734602689743
step: 10, loss: 0.0438842847943306
step: 20, loss: 0.016978899016976357
step: 30, loss: 0.05820411443710327
step: 40, loss: 0.07381611317396164
step: 50, loss: 0.16896386444568634
step: 60, loss: 0.04249230772256851
step: 70, loss: 0.04244322329759598
step: 80, loss: 0.10741442441940308
step: 90, loss: 0.03017403744161129
step: 100, loss: 0.02037876844406128
step: 110, loss: 0.0033807815052568913
step: 120, loss: 0.07276086509227753
step: 130, loss: 0.05163845047354698
step: 140, loss: 0.015682796016335487
step: 150, loss: 0.018568789586424828
step: 160, loss: 0.08341078460216522
step: 170, loss: 0.0013562680687755346
step: 180, loss: 0.08516193926334381
step: 190, loss: 0.029726900160312653
step: 200, loss: 0.06443095952272415
step: 210, loss: 0.06203998997807503
step: 220, loss: 0.1159018948674202
step: 230, loss: 0.026838352903723717
step: 240, loss: 0.04814637452363968
step: 250, loss: 0.18037812411785126
step: 260, loss: 0.04029737040400505
step: 270, loss: 0.08981192857027054
step: 280, loss: 0.011197283864021301
step: 290, loss: 0.02618935890495777
step: 300, loss: 0.2041807472705841
step: 310, loss: 0.103851817548275
step: 320, loss: 0.037111274898052216
step: 330, loss: 0.033350422978401184
step: 340, loss: 0.1858552247285843
step: 350, loss: 0.019467560574412346
step: 360, loss: 0.08943303674459457
epoch 8: dev_f1=0.7506297229219143, f1=0.736, best_f1=0.736
step: 0, loss: 0.00940849632024765
step: 10, loss: 0.019188648089766502
step: 20, loss: 0.16170352697372437
step: 30, loss: 0.024598797783255577
step: 40, loss: 0.14044801890850067
step: 50, loss: 0.01095975749194622
step: 60, loss: 0.034851595759391785
step: 70, loss: 0.017455371096730232
step: 80, loss: 0.07123430073261261
step: 90, loss: 0.011123677715659142
step: 100, loss: 0.06040392071008682
step: 110, loss: 0.03127063810825348
step: 120, loss: 0.016300279647111893
step: 130, loss: 0.0002599639992695302
step: 140, loss: 0.001902779215015471
step: 150, loss: 0.10730709880590439
step: 160, loss: 0.05750970542430878
step: 170, loss: 0.027257872745394707
step: 180, loss: 0.030900675803422928
step: 190, loss: 0.03504372388124466
step: 200, loss: 0.01442500576376915
step: 210, loss: 0.3011971116065979
step: 220, loss: 0.004833677783608437
step: 230, loss: 0.04974934831261635
step: 240, loss: 0.05132359638810158
step: 250, loss: 0.013905664905905724
step: 260, loss: 0.03088412433862686
step: 270, loss: 0.0630979984998703
step: 280, loss: 0.03953159227967262
step: 290, loss: 0.0006146361702121794
step: 300, loss: 0.017232850193977356
step: 310, loss: 0.016152620315551758
step: 320, loss: 0.07136047631502151
step: 330, loss: 0.00533524714410305
step: 340, loss: 0.028907276690006256
step: 350, loss: 0.1335630863904953
step: 360, loss: 0.006126998923718929
epoch 9: dev_f1=0.7106598984771575, f1=0.7371273712737126, best_f1=0.736
step: 0, loss: 0.05929430201649666
step: 10, loss: 0.018191827461123466
step: 20, loss: 0.011725029908120632
step: 30, loss: 0.028197703883051872
step: 40, loss: 0.009899214841425419
step: 50, loss: 0.04213650897145271
step: 60, loss: 0.06323657929897308
step: 70, loss: 0.016732340678572655
step: 80, loss: 0.020772483199834824
step: 90, loss: 0.03439165651798248
step: 100, loss: 0.028231265023350716
step: 110, loss: 0.00023591550416313112
step: 120, loss: 0.05827711895108223
step: 130, loss: 0.00032592687057331204
step: 140, loss: 0.038747482001781464
step: 150, loss: 0.07451868057250977
step: 160, loss: 0.010534174740314484
step: 170, loss: 0.06316136568784714
step: 180, loss: 0.039471760392189026
step: 190, loss: 0.0012739186640828848
step: 200, loss: 0.05825382471084595
step: 210, loss: 0.0034890316892415285
step: 220, loss: 0.023248225450515747
step: 230, loss: 0.045691847801208496
step: 240, loss: 0.006225949618965387
step: 250, loss: 0.05282590165734291
step: 260, loss: 0.020108964294195175
step: 270, loss: 0.03265557065606117
step: 280, loss: 0.008903156034648418
step: 290, loss: 0.016304127871990204
step: 300, loss: 0.004840224981307983
step: 310, loss: 0.07826286554336548
step: 320, loss: 0.07462403178215027
step: 330, loss: 0.06518104672431946
step: 340, loss: 0.009525790810585022
step: 350, loss: 0.033284224569797516
step: 360, loss: 0.008183501660823822
epoch 10: dev_f1=0.7355163727959698, f1=0.7204301075268816, best_f1=0.736
step: 0, loss: 0.00010579408990452066
step: 10, loss: 0.02721143700182438
step: 20, loss: 0.0563807412981987
step: 30, loss: 0.041135333478450775
step: 40, loss: 0.0022276309318840504
step: 50, loss: 0.06093043088912964
step: 60, loss: 0.03733114153146744
step: 70, loss: 0.015028961934149265
step: 80, loss: 0.005347433965653181
step: 90, loss: 0.020811306312680244
step: 100, loss: 0.032973818480968475
step: 110, loss: 0.01950060948729515
step: 120, loss: 0.07386576384305954
step: 130, loss: 0.006100102793425322
step: 140, loss: 0.008438140153884888
step: 150, loss: 0.010594102554023266
step: 160, loss: 0.07733877748250961
step: 170, loss: 0.03206077590584755
step: 180, loss: 0.0037271548062562943
step: 190, loss: 0.0006114413263276219
step: 200, loss: 0.03591003268957138
step: 210, loss: 0.008235109969973564
step: 220, loss: 0.030751030892133713
step: 230, loss: 0.0014668268850073218
step: 240, loss: 0.008593128062784672
step: 250, loss: 0.050425935536623
step: 260, loss: 0.05531046539545059
step: 270, loss: 0.06605610251426697
step: 280, loss: 0.00035132199991494417
step: 290, loss: 0.060265764594078064
step: 300, loss: 0.04204221069812775
step: 310, loss: 0.033837057650089264
step: 320, loss: 0.026580994948744774
step: 330, loss: 0.09717004001140594
step: 340, loss: 0.06421484798192978
step: 350, loss: 0.008100731298327446
step: 360, loss: 0.06719718128442764
epoch 11: dev_f1=0.7420147420147422, f1=0.7455012853470436, best_f1=0.736
step: 0, loss: 0.04088874161243439
step: 10, loss: 0.0029378440231084824
step: 20, loss: 0.020999222993850708
step: 30, loss: 0.007334205787628889
step: 40, loss: 0.05673782527446747
step: 50, loss: 0.0008772019064053893
step: 60, loss: 0.005029461346566677
step: 70, loss: 0.054209548979997635
step: 80, loss: 0.03255032002925873
step: 90, loss: 0.04947463795542717
step: 100, loss: 0.10974705964326859
step: 110, loss: 0.005101674236357212
step: 120, loss: 0.008457413874566555
step: 130, loss: 0.1088249459862709
step: 140, loss: 0.018185947090387344
step: 150, loss: 0.057043593376874924
step: 160, loss: 0.0015833278885111213
step: 170, loss: 0.0024086367338895798
step: 180, loss: 0.002060175407677889
step: 190, loss: 0.04275795444846153
step: 200, loss: 0.0032810880802571774
step: 210, loss: 0.014718713238835335
step: 220, loss: 0.030467119067907333
step: 230, loss: 0.08639353513717651
step: 240, loss: 0.0786108672618866
step: 250, loss: 0.01788520999252796
step: 260, loss: 0.00010623485286487266
step: 270, loss: 0.025813287124037743
step: 280, loss: 0.013059180229902267
step: 290, loss: 0.004339102655649185
step: 300, loss: 0.0722314715385437
step: 310, loss: 0.05885394290089607
step: 320, loss: 0.005590745713561773
step: 330, loss: 0.043100014328956604
step: 340, loss: 0.0013728212798014283
step: 350, loss: 0.021626612171530724
step: 360, loss: 0.016528651118278503
epoch 12: dev_f1=0.7146529562982005, f1=0.7169811320754718, best_f1=0.736
step: 0, loss: 0.00023752862762194127
step: 10, loss: 0.05847262591123581
step: 20, loss: 0.05378660559654236
step: 30, loss: 4.556501880870201e-05
step: 40, loss: 0.013113810680806637
step: 50, loss: 0.0002116904652211815
step: 60, loss: 5.040072574047372e-05
step: 70, loss: 0.0007889382541179657
step: 80, loss: 5.0955317419720814e-05
step: 90, loss: 0.012126748450100422
step: 100, loss: 0.0008651501848362386
step: 110, loss: 0.006206799764186144
step: 120, loss: 0.028453342616558075
step: 130, loss: 0.15387800335884094
step: 140, loss: 0.01604139246046543
step: 150, loss: 0.0011689288076013327
step: 160, loss: 0.005389406345784664
step: 170, loss: 0.03092629462480545
step: 180, loss: 0.017756255343556404
step: 190, loss: 0.013109780848026276
step: 200, loss: 0.06037482991814613
step: 210, loss: 0.007583427242934704
step: 220, loss: 0.0007675805245526135
step: 230, loss: 0.02916797623038292
step: 240, loss: 0.00045707941171713173
step: 250, loss: 0.002054763026535511
step: 260, loss: 0.019348474219441414
step: 270, loss: 0.008036782033741474
step: 280, loss: 0.021677227690815926
step: 290, loss: 0.0078094955533742905
step: 300, loss: 0.005481720436364412
step: 310, loss: 0.006789991166442633
step: 320, loss: 0.034744806587696075
step: 330, loss: 0.009285791777074337
step: 340, loss: 0.0593269057571888
step: 350, loss: 0.06541914492845535
step: 360, loss: 0.050514958798885345
epoch 13: dev_f1=0.695, f1=0.6940874035989718, best_f1=0.736
step: 0, loss: 0.004221404902637005
step: 10, loss: 0.034514132887125015
step: 20, loss: 0.01674630120396614
step: 30, loss: 0.1322479546070099
step: 40, loss: 0.0112465750426054
step: 50, loss: 0.05990898236632347
step: 60, loss: 0.007683113683015108
step: 70, loss: 2.7465957828098908e-05
step: 80, loss: 0.003028449136763811
step: 90, loss: 0.00785127468407154
step: 100, loss: 0.007261086720973253
step: 110, loss: 0.008702860213816166
step: 120, loss: 0.01322658360004425
step: 130, loss: 0.03580759838223457
step: 140, loss: 0.014494440518319607
step: 150, loss: 0.017870841547846794
step: 160, loss: 0.03685331344604492
step: 170, loss: 0.001578005962073803
step: 180, loss: 0.010347404517233372
step: 190, loss: 0.0001538807264296338
step: 200, loss: 0.05781647562980652
step: 210, loss: 0.023435477167367935
step: 220, loss: 0.013904604129493237
step: 230, loss: 0.07012516260147095
step: 240, loss: 0.08036451786756516
step: 250, loss: 0.002418316202238202
step: 260, loss: 0.03491092845797539
step: 270, loss: 0.010370293632149696
step: 280, loss: 0.00849751103669405
step: 290, loss: 0.03604048117995262
step: 300, loss: 0.01612243987619877
step: 310, loss: 0.09400277584791183
step: 320, loss: 0.011060493066906929
step: 330, loss: 0.05039503052830696
step: 340, loss: 0.0744941458106041
step: 350, loss: 0.01752893067896366
step: 360, loss: 0.14529837667942047
epoch 14: dev_f1=0.7201946472019465, f1=0.7028423772609819, best_f1=0.736
step: 0, loss: 0.07534211874008179
step: 10, loss: 0.034403275698423386
step: 20, loss: 0.03379443660378456
step: 30, loss: 0.021668529137969017
step: 40, loss: 0.030083367601037025
step: 50, loss: 0.0062040649354457855
step: 60, loss: 0.00014468673907686025
step: 70, loss: 0.0209227092564106
step: 80, loss: 0.0022112615406513214
step: 90, loss: 0.0011607012711465359
step: 100, loss: 0.00566402543336153
step: 110, loss: 0.02734193205833435
step: 120, loss: 0.04726243019104004
step: 130, loss: 0.0810813158750534
step: 140, loss: 0.007970958016812801
step: 150, loss: 0.025770360603928566
step: 160, loss: 0.012464123778045177
step: 170, loss: 0.012698061764240265
step: 180, loss: 0.0015483193565160036
step: 190, loss: 0.00027075569960288703
step: 200, loss: 0.03875300660729408
step: 210, loss: 0.026280440390110016
step: 220, loss: 3.38304162141867e-05
step: 230, loss: 0.001726339221931994
step: 240, loss: 0.09520485997200012
step: 250, loss: 0.0007668992038816214
step: 260, loss: 0.03086025081574917
step: 270, loss: 0.04814908280968666
step: 280, loss: 0.03616563603281975
step: 290, loss: 0.0008845534175634384
step: 300, loss: 0.01984870433807373
step: 310, loss: 0.04538044333457947
step: 320, loss: 0.03609767556190491
step: 330, loss: 0.023256346583366394
step: 340, loss: 0.1074722409248352
step: 350, loss: 0.026326026767492294
step: 360, loss: 0.06459634751081467
epoch 15: dev_f1=0.7240506329113923, f1=0.7037037037037037, best_f1=0.736
step: 0, loss: 7.47196827433072e-05
step: 10, loss: 0.027633458375930786
step: 20, loss: 0.0011336287716403604
step: 30, loss: 0.03708282858133316
step: 40, loss: 8.643298497190699e-05
step: 50, loss: 0.008807076141238213
step: 60, loss: 4.431207344168797e-05
step: 70, loss: 0.0014747492969036102
step: 80, loss: 0.028061917051672935
step: 90, loss: 0.004031937103718519
step: 100, loss: 0.00012030826474074274
step: 110, loss: 0.09730003029108047
step: 120, loss: 0.001011100597679615
step: 130, loss: 0.014655407518148422
step: 140, loss: 0.010514084249734879
step: 150, loss: 0.003371627302840352
step: 160, loss: 0.0015892748488113284
step: 170, loss: 0.0409683957695961
step: 180, loss: 0.028558000922203064
step: 190, loss: 0.003208345966413617
step: 200, loss: 0.000674245529808104
step: 210, loss: 0.026089603081345558
step: 220, loss: 0.010150287300348282
step: 230, loss: 0.0003721653192769736
step: 240, loss: 0.02827855572104454
step: 250, loss: 0.02350987307727337
step: 260, loss: 0.0020756979938596487
step: 270, loss: 0.0017855510814115405
step: 280, loss: 0.003224499523639679
step: 290, loss: 0.019404692575335503
step: 300, loss: 0.005457728635519743
step: 310, loss: 0.08512867987155914
step: 320, loss: 0.007801271975040436
step: 330, loss: 0.007760495878756046
step: 340, loss: 0.0015776959480717778
step: 350, loss: 0.045723188668489456
step: 360, loss: 0.0009956634603440762
epoch 16: dev_f1=0.7070217917675545, f1=0.6803069053708439, best_f1=0.736
step: 0, loss: 0.031791724264621735
step: 10, loss: 0.0006384761654771864
step: 20, loss: 0.08821146935224533
step: 30, loss: 6.181914068292826e-05
step: 40, loss: 0.013170363381505013
step: 50, loss: 0.004466191865503788
step: 60, loss: 0.0015606529777869582
step: 70, loss: 5.9925925597781315e-05
step: 80, loss: 0.04784665256738663
step: 90, loss: 0.02558133937418461
step: 100, loss: 0.00037085454096086323
step: 110, loss: 0.033858250826597214
step: 120, loss: 0.003376137465238571
step: 130, loss: 0.000544222304597497
step: 140, loss: 0.021113701164722443
step: 150, loss: 7.553347677458078e-05
step: 160, loss: 0.0013547766720876098
step: 170, loss: 0.020378123968839645
step: 180, loss: 0.00021377395023591816
step: 190, loss: 0.024054717272520065
step: 200, loss: 0.028244446963071823
step: 210, loss: 0.013435283675789833
step: 220, loss: 0.0014674876583740115
step: 230, loss: 0.0029785989318042994
step: 240, loss: 0.10061600804328918
step: 250, loss: 0.014543881639838219
step: 260, loss: 0.00861809030175209
step: 270, loss: 0.009160619229078293
step: 280, loss: 0.0005020993412472308
step: 290, loss: 6.228725396795198e-05
step: 300, loss: 0.10340310633182526
step: 310, loss: 0.05548308044672012
step: 320, loss: 0.01864817552268505
step: 330, loss: 0.00018375235958956182
step: 340, loss: 4.9199126806342974e-05
step: 350, loss: 0.00018193124560639262
step: 360, loss: 0.0449838750064373
epoch 17: dev_f1=0.7277353689567431, f1=0.7124010554089709, best_f1=0.736
step: 0, loss: 0.009483889676630497
step: 10, loss: 0.013140644878149033
step: 20, loss: 0.03540199622511864
step: 30, loss: 0.011611959896981716
step: 40, loss: 0.03809491544961929
step: 50, loss: 0.026599254459142685
step: 60, loss: 0.029070420190691948
step: 70, loss: 0.036703355610370636
step: 80, loss: 0.00263466639444232
step: 90, loss: 0.007259188685566187
step: 100, loss: 0.05570809543132782
step: 110, loss: 0.0337657555937767
step: 120, loss: 0.04211866110563278
step: 130, loss: 0.009417924098670483
step: 140, loss: 0.07645563036203384
step: 150, loss: 0.005902093835175037
step: 160, loss: 0.017969384789466858
step: 170, loss: 0.02920457161962986
step: 180, loss: 0.003812551498413086
step: 190, loss: 0.0005200610030442476
step: 200, loss: 0.005377819761633873
step: 210, loss: 0.007540982216596603
step: 220, loss: 0.009973656386137009
step: 230, loss: 0.00459714001044631
step: 240, loss: 0.03912617266178131
step: 250, loss: 0.010036558844149113
step: 260, loss: 0.04672135040163994
step: 270, loss: 2.1773848857264966e-05
step: 280, loss: 0.0034237047657370567
step: 290, loss: 0.009166180156171322
step: 300, loss: 0.0009639158961363137
step: 310, loss: 0.01816721446812153
step: 320, loss: 0.0018051067600026727
step: 330, loss: 0.0229925736784935
step: 340, loss: 0.007853906601667404
step: 350, loss: 0.10485184192657471
step: 360, loss: 0.015698933973908424
epoch 18: dev_f1=0.7292225201072386, f1=0.7075208913649025, best_f1=0.736
step: 0, loss: 0.012222024612128735
step: 10, loss: 0.0001540051744086668
step: 20, loss: 0.00023605921887792647
step: 30, loss: 0.0006454680697061121
step: 40, loss: 0.04554613679647446
step: 50, loss: 0.0001526624255347997
step: 60, loss: 0.0014987618196755648
step: 70, loss: 0.00041725748451426625
step: 80, loss: 0.0011474377242848277
step: 90, loss: 0.026018301025032997
step: 100, loss: 0.0007771753589622676
step: 110, loss: 0.0020372236613184214
step: 120, loss: 0.0038871129509061575
step: 130, loss: 0.0014041089452803135
step: 140, loss: 0.00887753814458847
step: 150, loss: 0.06434792280197144
step: 160, loss: 0.011216460727155209
step: 170, loss: 0.0010817826259881258
step: 180, loss: 0.0003868280036840588
step: 190, loss: 0.008578981272876263
step: 200, loss: 0.0004483747179619968
step: 210, loss: 0.05299030616879463
step: 220, loss: 0.01964784786105156
step: 230, loss: 2.9793725843774155e-05
step: 240, loss: 0.02432071603834629
step: 250, loss: 2.4321454475284554e-05
step: 260, loss: 0.03251579776406288
step: 270, loss: 0.009620633907616138
step: 280, loss: 0.03936537727713585
step: 290, loss: 0.006644066423177719
step: 300, loss: 0.0005791101139038801
step: 310, loss: 0.03951837122440338
step: 320, loss: 2.6668565624277107e-05
step: 330, loss: 0.05489558354020119
step: 340, loss: 0.06819479167461395
step: 350, loss: 0.021732551977038383
step: 360, loss: 0.04380946606397629
epoch 19: dev_f1=0.734375, f1=0.7150537634408601, best_f1=0.736
step: 0, loss: 0.05204828828573227
step: 10, loss: 0.0038161734119057655
step: 20, loss: 0.025258412584662437
step: 30, loss: 0.010137084871530533
step: 40, loss: 0.001828172942623496
step: 50, loss: 0.04579706862568855
step: 60, loss: 0.009889297187328339
step: 70, loss: 0.016956806182861328
step: 80, loss: 0.019136838614940643
step: 90, loss: 0.030710820108652115
step: 100, loss: 0.033834513276815414
step: 110, loss: 0.0027647758834064007
step: 120, loss: 0.011469353921711445
step: 130, loss: 2.552145451772958e-05
step: 140, loss: 0.00284441071562469
step: 150, loss: 0.10008227080106735
step: 160, loss: 0.05066459998488426
step: 170, loss: 0.03734873607754707
step: 180, loss: 0.014173534698784351
step: 190, loss: 0.023333586752414703
step: 200, loss: 0.02483069896697998
step: 210, loss: 0.01897158846259117
step: 220, loss: 0.002159584080800414
step: 230, loss: 0.0002573421807028353
step: 240, loss: 0.04613243788480759
step: 250, loss: 0.07246176153421402
step: 260, loss: 0.0007634339854121208
step: 270, loss: 0.048068102449178696
step: 280, loss: 0.00022705181618221104
step: 290, loss: 0.11676398664712906
step: 300, loss: 0.0006506913923658431
step: 310, loss: 0.03030666522681713
step: 320, loss: 0.0002009207964874804
step: 330, loss: 0.004247001372277737
step: 340, loss: 0.00014985623420216143
step: 350, loss: 0.0020831094589084387
step: 360, loss: 0.01184798777103424
epoch 20: dev_f1=0.7241379310344828, f1=0.720626631853786, best_f1=0.736
