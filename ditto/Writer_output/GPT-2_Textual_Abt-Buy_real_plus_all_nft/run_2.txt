cuda
Device: cuda
step: 0, loss: 0.6030363440513611
step: 10, loss: 0.3958945870399475
step: 20, loss: 0.45910710096359253
step: 30, loss: 0.25573235750198364
step: 40, loss: 0.3301096558570862
step: 50, loss: 0.5244852304458618
step: 60, loss: 0.377002090215683
step: 70, loss: 0.33846116065979004
step: 80, loss: 0.06465854495763779
step: 90, loss: 0.29651814699172974
step: 100, loss: 0.4230242967605591
step: 110, loss: 0.23924104869365692
step: 120, loss: 0.45560869574546814
step: 130, loss: 0.15818478167057037
step: 140, loss: 0.2865807116031647
step: 150, loss: 0.29081976413726807
step: 160, loss: 0.4018465280532837
step: 170, loss: 0.471263587474823
step: 180, loss: 0.33654990792274475
step: 190, loss: 0.3270082175731659
step: 200, loss: 0.2709812819957733
step: 210, loss: 0.23940108716487885
step: 220, loss: 0.22951050102710724
step: 230, loss: 0.4824772775173187
step: 240, loss: 0.20195864140987396
step: 250, loss: 0.3730528950691223
step: 260, loss: 0.29005885124206543
step: 270, loss: 0.29993438720703125
step: 280, loss: 0.35067427158355713
step: 290, loss: 0.2106255739927292
step: 300, loss: 0.5134117007255554
step: 310, loss: 0.37767189741134644
step: 320, loss: 0.16850066184997559
step: 330, loss: 0.15914608538150787
step: 340, loss: 0.24009042978286743
step: 350, loss: 0.16354450583457947
epoch 1: dev_f1=0.5371702637889689, f1=0.4, best_f1=0.4
step: 0, loss: 0.09127232432365417
step: 10, loss: 0.2201189547777176
step: 20, loss: 0.16381774842739105
step: 30, loss: 0.05226216837763786
step: 40, loss: 0.20879021286964417
step: 50, loss: 0.28498604893684387
step: 60, loss: 0.22732068598270416
step: 70, loss: 0.16944871842861176
step: 80, loss: 0.30284175276756287
step: 90, loss: 0.2708827257156372
step: 100, loss: 0.1731220930814743
step: 110, loss: 0.12818768620491028
step: 120, loss: 0.33085355162620544
step: 130, loss: 0.2673882842063904
step: 140, loss: 0.29075995087623596
step: 150, loss: 0.15751390159130096
step: 160, loss: 0.3281790316104889
step: 170, loss: 0.3283354938030243
step: 180, loss: 0.1753523051738739
step: 190, loss: 0.2744554877281189
step: 200, loss: 0.2510049343109131
step: 210, loss: 0.44304776191711426
step: 220, loss: 0.28245580196380615
step: 230, loss: 0.09319745749235153
step: 240, loss: 0.306732177734375
step: 250, loss: 0.2338070571422577
step: 260, loss: 0.22512973845005035
step: 270, loss: 0.1320081651210785
step: 280, loss: 0.32951125502586365
step: 290, loss: 0.1324259489774704
step: 300, loss: 0.22173704206943512
step: 310, loss: 0.27540338039398193
step: 320, loss: 0.2571268677711487
step: 330, loss: 0.21628010272979736
step: 340, loss: 0.2919243574142456
step: 350, loss: 0.21752075850963593
epoch 2: dev_f1=0.7625570776255708, f1=0.6851851851851851, best_f1=0.6851851851851851
step: 0, loss: 0.19135309755802155
step: 10, loss: 0.38341572880744934
step: 20, loss: 0.13707512617111206
step: 30, loss: 0.498954713344574
step: 40, loss: 0.11424803733825684
step: 50, loss: 0.151936873793602
step: 60, loss: 0.18388354778289795
step: 70, loss: 0.2554779350757599
step: 80, loss: 0.5988237261772156
step: 90, loss: 0.07444087415933609
step: 100, loss: 0.039740562438964844
step: 110, loss: 0.22193273901939392
step: 120, loss: 0.20053483545780182
step: 130, loss: 0.1783696711063385
step: 140, loss: 0.2803848385810852
step: 150, loss: 0.04878370091319084
step: 160, loss: 0.16627073287963867
step: 170, loss: 0.04134809970855713
step: 180, loss: 0.21079221367835999
step: 190, loss: 0.14444303512573242
step: 200, loss: 0.3393573760986328
step: 210, loss: 0.20047958195209503
step: 220, loss: 0.2212517112493515
step: 230, loss: 0.1253085732460022
step: 240, loss: 0.35397979617118835
step: 250, loss: 0.062400639057159424
step: 260, loss: 0.07556888461112976
step: 270, loss: 0.16650502383708954
step: 280, loss: 0.2598509192466736
step: 290, loss: 0.1375284641981125
step: 300, loss: 0.08215305209159851
step: 310, loss: 0.2620807886123657
step: 320, loss: 0.17283913493156433
step: 330, loss: 0.1827460527420044
step: 340, loss: 0.24869340658187866
step: 350, loss: 0.3813990354537964
epoch 3: dev_f1=0.7922077922077922, f1=0.7396061269146608, best_f1=0.7396061269146608
step: 0, loss: 0.08399254083633423
step: 10, loss: 0.11097513139247894
step: 20, loss: 0.03174552693963051
step: 30, loss: 0.06692513078451157
step: 40, loss: 0.1240273267030716
step: 50, loss: 0.16078828275203705
step: 60, loss: 0.24480724334716797
step: 70, loss: 0.12087515741586685
step: 80, loss: 0.10875262320041656
step: 90, loss: 0.2000758945941925
step: 100, loss: 0.04488905519247055
step: 110, loss: 0.4155395030975342
step: 120, loss: 0.44946974515914917
step: 130, loss: 0.09399016201496124
step: 140, loss: 0.18130876123905182
step: 150, loss: 0.04419953003525734
step: 160, loss: 0.25712841749191284
step: 170, loss: 0.12225621193647385
step: 180, loss: 0.21043933928012848
step: 190, loss: 0.1603175401687622
step: 200, loss: 0.2532420754432678
step: 210, loss: 0.1904398798942566
step: 220, loss: 0.042156923562288284
step: 230, loss: 0.08591819554567337
step: 240, loss: 0.24055956304073334
step: 250, loss: 0.025019658729434013
step: 260, loss: 0.13047820329666138
step: 270, loss: 0.15679539740085602
step: 280, loss: 0.17437870800495148
step: 290, loss: 0.17155836522579193
step: 300, loss: 0.035062264651060104
step: 310, loss: 0.1385788470506668
step: 320, loss: 0.23103070259094238
step: 330, loss: 0.0755864605307579
step: 340, loss: 0.2034764438867569
step: 350, loss: 0.1512787938117981
epoch 4: dev_f1=0.7972350230414745, f1=0.7310344827586207, best_f1=0.7310344827586207
step: 0, loss: 0.07996389269828796
step: 10, loss: 0.02674875035881996
step: 20, loss: 0.07428412139415741
step: 30, loss: 0.018441298976540565
step: 40, loss: 0.09829101711511612
step: 50, loss: 0.01909073069691658
step: 60, loss: 0.02823192998766899
step: 70, loss: 0.162434384226799
step: 80, loss: 0.24950090050697327
step: 90, loss: 0.3027247190475464
step: 100, loss: 0.19855061173439026
step: 110, loss: 0.14133457839488983
step: 120, loss: 0.1586899608373642
step: 130, loss: 0.06093474105000496
step: 140, loss: 0.10816382616758347
step: 150, loss: 0.09013860672712326
step: 160, loss: 0.07239396870136261
step: 170, loss: 0.07032331079244614
step: 180, loss: 0.033147841691970825
step: 190, loss: 0.05295296013355255
step: 200, loss: 0.0219271183013916
step: 210, loss: 0.08033780753612518
step: 220, loss: 0.05571728199720383
step: 230, loss: 0.22469699382781982
step: 240, loss: 0.05654577538371086
step: 250, loss: 0.03332812711596489
step: 260, loss: 0.17707933485507965
step: 270, loss: 0.0763598382472992
step: 280, loss: 0.19761520624160767
step: 290, loss: 0.047866567969322205
step: 300, loss: 0.24597005546092987
step: 310, loss: 0.12499568611383438
step: 320, loss: 0.14944230020046234
step: 330, loss: 0.09889491647481918
step: 340, loss: 0.07700890302658081
step: 350, loss: 0.06327716261148453
epoch 5: dev_f1=0.813953488372093, f1=0.765661252900232, best_f1=0.765661252900232
step: 0, loss: 0.12871381640434265
step: 10, loss: 0.10007566958665848
step: 20, loss: 0.06014173477888107
step: 30, loss: 0.14725267887115479
step: 40, loss: 0.006486370228230953
step: 50, loss: 0.10508628934621811
step: 60, loss: 0.053626686334609985
step: 70, loss: 0.08168742060661316
step: 80, loss: 0.12240937352180481
step: 90, loss: 0.04673650488257408
step: 100, loss: 0.0868094265460968
step: 110, loss: 0.0870998352766037
step: 120, loss: 0.21488718688488007
step: 130, loss: 0.025026077404618263
step: 140, loss: 0.02380903623998165
step: 150, loss: 0.020761677995324135
step: 160, loss: 0.03768767789006233
step: 170, loss: 0.09264900535345078
step: 180, loss: 0.03884212300181389
step: 190, loss: 0.16962984204292297
step: 200, loss: 0.08751202374696732
step: 210, loss: 0.03610210493206978
step: 220, loss: 0.02978483773767948
step: 230, loss: 0.004325887653976679
step: 240, loss: 0.06339181959629059
step: 250, loss: 0.17525386810302734
step: 260, loss: 0.04456697404384613
step: 270, loss: 0.02485535852611065
step: 280, loss: 0.04535134881734848
step: 290, loss: 0.12981726229190826
step: 300, loss: 0.028534073382616043
step: 310, loss: 0.0970088467001915
step: 320, loss: 0.18683846294879913
step: 330, loss: 0.04138047993183136
step: 340, loss: 0.0841241404414177
step: 350, loss: 0.111595518887043
epoch 6: dev_f1=0.8144796380090498, f1=0.7528344671201814, best_f1=0.7528344671201814
step: 0, loss: 0.028229137882590294
step: 10, loss: 0.04596315324306488
step: 20, loss: 0.10112946480512619
step: 30, loss: 0.004081846680492163
step: 40, loss: 0.05922183021903038
step: 50, loss: 0.013682331889867783
step: 60, loss: 0.03884907439351082
step: 70, loss: 0.13919804990291595
step: 80, loss: 0.102869912981987
step: 90, loss: 0.08307874947786331
step: 100, loss: 0.030569834634661674
step: 110, loss: 0.06446857005357742
step: 120, loss: 0.08997605741024017
step: 130, loss: 0.027225174009799957
step: 140, loss: 0.043887507170438766
step: 150, loss: 0.15189075469970703
step: 160, loss: 0.02772824652493
step: 170, loss: 0.05269156023859978
step: 180, loss: 0.08663994073867798
step: 190, loss: 0.04617859423160553
step: 200, loss: 0.04695189371705055
step: 210, loss: 0.05540899932384491
step: 220, loss: 0.05284540727734566
step: 230, loss: 0.0460677370429039
step: 240, loss: 0.0947885513305664
step: 250, loss: 0.06291590631008148
step: 260, loss: 0.07563235610723495
step: 270, loss: 0.040705788880586624
step: 280, loss: 0.13882888853549957
step: 290, loss: 0.048794642090797424
step: 300, loss: 0.028907593339681625
step: 310, loss: 0.0022828883957117796
step: 320, loss: 0.1130969375371933
step: 330, loss: 0.017060404643416405
step: 340, loss: 0.035570692270994186
step: 350, loss: 0.050391633063554764
epoch 7: dev_f1=0.8104265402843601, f1=0.7107843137254901, best_f1=0.7528344671201814
step: 0, loss: 0.008375640027225018
step: 10, loss: 0.00873149186372757
step: 20, loss: 0.09568627923727036
step: 30, loss: 0.004386389162391424
step: 40, loss: 0.2542998790740967
step: 50, loss: 0.04932970181107521
step: 60, loss: 0.034749165177345276
step: 70, loss: 0.1039254441857338
step: 80, loss: 0.009116215631365776
step: 90, loss: 0.009299327619373798
step: 100, loss: 0.022215913981199265
step: 110, loss: 0.03018251620233059
step: 120, loss: 0.0791674330830574
step: 130, loss: 0.004802873823791742
step: 140, loss: 0.025673883035779
step: 150, loss: 0.014200635254383087
step: 160, loss: 0.007332833018153906
step: 170, loss: 0.045555680990219116
step: 180, loss: 0.0946318507194519
step: 190, loss: 0.04473695158958435
step: 200, loss: 0.15764911472797394
step: 210, loss: 0.18892942368984222
step: 220, loss: 0.19228270649909973
step: 230, loss: 0.013982626609504223
step: 240, loss: 0.06658203154802322
step: 250, loss: 0.05347355082631111
step: 260, loss: 0.02419164776802063
step: 270, loss: 0.056827470660209656
step: 280, loss: 0.07743125408887863
step: 290, loss: 0.04256433621048927
step: 300, loss: 0.015953972935676575
step: 310, loss: 0.12438134849071503
step: 320, loss: 0.002523400355130434
step: 330, loss: 0.03876476362347603
step: 340, loss: 0.015712006017565727
step: 350, loss: 0.2140815705060959
epoch 8: dev_f1=0.786096256684492, f1=0.6793478260869565, best_f1=0.7528344671201814
step: 0, loss: 0.016883987933397293
step: 10, loss: 0.05902852118015289
step: 20, loss: 0.13812029361724854
step: 30, loss: 0.04889986664056778
step: 40, loss: 0.11161050200462341
step: 50, loss: 0.0048065814189612865
step: 60, loss: 0.002167728263884783
step: 70, loss: 0.007828417234122753
step: 80, loss: 0.025953063741326332
step: 90, loss: 0.021287916228175163
step: 100, loss: 0.05438327044248581
step: 110, loss: 0.08650048077106476
step: 120, loss: 0.03609418496489525
step: 130, loss: 0.03690612316131592
step: 140, loss: 0.046819038689136505
step: 150, loss: 0.011018896475434303
step: 160, loss: 0.03310403227806091
step: 170, loss: 0.021437102928757668
step: 180, loss: 0.06147871911525726
step: 190, loss: 0.10460729897022247
step: 200, loss: 0.05106644704937935
step: 210, loss: 0.07434152811765671
step: 220, loss: 0.05506996065378189
step: 230, loss: 0.0456751249730587
step: 240, loss: 0.11607033759355545
step: 250, loss: 0.01896371878683567
step: 260, loss: 0.010651006363332272
step: 270, loss: 0.029939817264676094
step: 280, loss: 0.0011948071187362075
step: 290, loss: 0.0041055213660001755
step: 300, loss: 0.028484495356678963
step: 310, loss: 0.09109210968017578
step: 320, loss: 0.020163921639323235
step: 330, loss: 0.10465998202562332
step: 340, loss: 0.07949917763471603
step: 350, loss: 0.01162514928728342
epoch 9: dev_f1=0.8073878627968338, f1=0.7214854111405835, best_f1=0.7528344671201814
step: 0, loss: 0.03332044929265976
step: 10, loss: 0.009771501645445824
step: 20, loss: 0.0011039185337722301
step: 30, loss: 0.07271891087293625
step: 40, loss: 0.006540905684232712
step: 50, loss: 0.005369179416447878
step: 60, loss: 0.02753513865172863
step: 70, loss: 0.017007578164339066
step: 80, loss: 0.16650885343551636
step: 90, loss: 0.0639149472117424
step: 100, loss: 0.011194605380296707
step: 110, loss: 0.010793354362249374
step: 120, loss: 0.05045383796095848
step: 130, loss: 0.06416521221399307
step: 140, loss: 0.02751086838543415
step: 150, loss: 0.04524629935622215
step: 160, loss: 0.004894412588328123
step: 170, loss: 0.14724017679691315
step: 180, loss: 0.0065529728308320045
step: 190, loss: 0.081259585916996
step: 200, loss: 0.24150127172470093
step: 210, loss: 0.006994504947215319
step: 220, loss: 0.024852462112903595
step: 230, loss: 0.04064309597015381
step: 240, loss: 0.004663413856178522
step: 250, loss: 0.057009488344192505
step: 260, loss: 0.0020400313660502434
step: 270, loss: 0.006784216500818729
step: 280, loss: 0.01650135964155197
step: 290, loss: 0.07467242330312729
step: 300, loss: 0.07653552293777466
step: 310, loss: 0.07515843957662582
step: 320, loss: 0.0007809093804098666
step: 330, loss: 0.13442033529281616
step: 340, loss: 0.08624113351106644
step: 350, loss: 0.01392456330358982
epoch 10: dev_f1=0.8341708542713568, f1=0.7481296758104738, best_f1=0.7481296758104738
step: 0, loss: 0.006214594002813101
step: 10, loss: 0.037214118987321854
step: 20, loss: 0.003938002977520227
step: 30, loss: 0.0021242666989564896
step: 40, loss: 0.0075154174119234085
step: 50, loss: 0.004597494378685951
step: 60, loss: 0.1149711161851883
step: 70, loss: 0.0008584390743635595
step: 80, loss: 0.004429842811077833
step: 90, loss: 0.007133402395993471
step: 100, loss: 0.0007136315107345581
step: 110, loss: 0.022601759061217308
step: 120, loss: 0.004049316979944706
step: 130, loss: 0.059360574930906296
step: 140, loss: 0.009066669270396233
step: 150, loss: 0.09010305255651474
step: 160, loss: 0.04591931775212288
step: 170, loss: 0.004740574862807989
step: 180, loss: 0.004598238971084356
step: 190, loss: 0.0005234649870544672
step: 200, loss: 0.007859831675887108
step: 210, loss: 0.00602766266092658
step: 220, loss: 0.025415975600481033
step: 230, loss: 0.018341759219765663
step: 240, loss: 0.0024385610595345497
step: 250, loss: 0.000832811463624239
step: 260, loss: 0.008714506402611732
step: 270, loss: 0.01107753999531269
step: 280, loss: 0.0057730479165911674
step: 290, loss: 0.035020746290683746
step: 300, loss: 0.052568625658750534
step: 310, loss: 0.0016834721900522709
step: 320, loss: 0.01020218338817358
step: 330, loss: 0.0010060803033411503
step: 340, loss: 0.0016849437961354852
step: 350, loss: 0.005704775918275118
epoch 11: dev_f1=0.8179551122194514, f1=0.759493670886076, best_f1=0.7481296758104738
step: 0, loss: 0.0027294685132801533
step: 10, loss: 0.019225874915719032
step: 20, loss: 0.015018800273537636
step: 30, loss: 0.001916808309033513
step: 40, loss: 0.0002824764815159142
step: 50, loss: 0.023359103128314018
step: 60, loss: 0.012481797486543655
step: 70, loss: 0.0037843903992325068
step: 80, loss: 0.0010863218922168016
step: 90, loss: 0.0036173034459352493
step: 100, loss: 0.2505923807621002
step: 110, loss: 0.016446653753519058
step: 120, loss: 0.01277089025825262
step: 130, loss: 0.0008987768669612706
step: 140, loss: 0.0026572428178042173
step: 150, loss: 0.013634574599564075
step: 160, loss: 0.006492260377854109
step: 170, loss: 0.006043111905455589
step: 180, loss: 0.017931342124938965
step: 190, loss: 0.00034376708208583295
step: 200, loss: 0.00018981781613547355
step: 210, loss: 0.01208852045238018
step: 220, loss: 0.004380349535495043
step: 230, loss: 0.029501229524612427
step: 240, loss: 0.004105804953724146
step: 250, loss: 0.00464505422860384
step: 260, loss: 0.0022336908150464296
step: 270, loss: 0.022734858095645905
step: 280, loss: 0.0090573625639081
step: 290, loss: 0.010001374408602715
step: 300, loss: 0.11129461973905563
step: 310, loss: 0.0008290120749734342
step: 320, loss: 0.0034492816776037216
step: 330, loss: 0.006354840938001871
step: 340, loss: 0.18719319999217987
step: 350, loss: 0.0008585019968450069
epoch 12: dev_f1=0.8382352941176471, f1=0.7563451776649747, best_f1=0.7563451776649747
step: 0, loss: 0.021563714370131493
step: 10, loss: 0.01165001466870308
step: 20, loss: 0.003676059190183878
step: 30, loss: 0.0027933872770518064
step: 40, loss: 0.0014855252811685205
step: 50, loss: 0.05243048444390297
step: 60, loss: 0.003660020884126425
step: 70, loss: 0.03272676467895508
step: 80, loss: 0.0009288143482990563
step: 90, loss: 0.06558649986982346
step: 100, loss: 0.03865590691566467
step: 110, loss: 0.001073043909855187
step: 120, loss: 0.04223218932747841
step: 130, loss: 0.0007770458469167352
step: 140, loss: 0.0008462942787446082
step: 150, loss: 0.0008326004608534276
step: 160, loss: 0.02632739022374153
step: 170, loss: 0.0011425374541431665
step: 180, loss: 0.0010045417584478855
step: 190, loss: 0.004675354342907667
step: 200, loss: 0.028696171939373016
step: 210, loss: 0.04533477500081062
step: 220, loss: 0.0016832869732752442
step: 230, loss: 0.0011800052598118782
step: 240, loss: 0.002010944066569209
step: 250, loss: 0.001451557269319892
step: 260, loss: 0.08350785076618195
step: 270, loss: 0.0008120309212245047
step: 280, loss: 0.04467826709151268
step: 290, loss: 0.00427305418998003
step: 300, loss: 0.002791157690808177
step: 310, loss: 0.001120907487347722
step: 320, loss: 0.001493730815127492
step: 330, loss: 0.000743193319067359
step: 340, loss: 0.02143438532948494
step: 350, loss: 0.011117617599666119
epoch 13: dev_f1=0.8066825775656326, f1=0.7475728155339806, best_f1=0.7563451776649747
step: 0, loss: 0.010309147648513317
step: 10, loss: 0.005813357885926962
step: 20, loss: 0.004814662504941225
step: 30, loss: 0.00206356355920434
step: 40, loss: 0.07271889597177505
step: 50, loss: 0.002914338605478406
step: 60, loss: 0.004613137803971767
step: 70, loss: 0.005293846596032381
step: 80, loss: 0.0043979547917842865
step: 90, loss: 0.0006982263294048607
step: 100, loss: 0.01048444863408804
step: 110, loss: 0.003939855378121138
step: 120, loss: 0.0008592745871283114
step: 130, loss: 0.053609542548656464
step: 140, loss: 0.013756747357547283
step: 150, loss: 0.0010411959374323487
step: 160, loss: 0.05393455922603607
step: 170, loss: 0.0005622853059321642
step: 180, loss: 0.06058404594659805
step: 190, loss: 0.00854470208287239
step: 200, loss: 0.0045061977580189705
step: 210, loss: 0.13796277344226837
step: 220, loss: 0.004321180749684572
step: 230, loss: 0.0005794583703391254
step: 240, loss: 0.0002752321888692677
step: 250, loss: 0.003609619801864028
step: 260, loss: 0.00025101302890107036
step: 270, loss: 0.0005384832038544118
step: 280, loss: 0.003855685004964471
step: 290, loss: 0.025690684095025063
step: 300, loss: 0.008459046483039856
step: 310, loss: 0.019638875499367714
step: 320, loss: 0.0011886931024491787
step: 330, loss: 0.02731850929558277
step: 340, loss: 0.0005225339555181563
step: 350, loss: 0.0016128959832713008
epoch 14: dev_f1=0.8144578313253013, f1=0.7246376811594203, best_f1=0.7563451776649747
step: 0, loss: 0.0002616643032524735
step: 10, loss: 0.008019120432436466
step: 20, loss: 0.002502158284187317
step: 30, loss: 0.10670316964387894
step: 40, loss: 0.00026900373632088304
step: 50, loss: 0.19822928309440613
step: 60, loss: 0.23501372337341309
step: 70, loss: 0.0010591292520985007
step: 80, loss: 0.0036855817306786776
step: 90, loss: 0.026595989242196083
step: 100, loss: 0.025308046489953995
step: 110, loss: 0.0024605540093034506
step: 120, loss: 0.006564153358340263
step: 130, loss: 0.002967794658616185
step: 140, loss: 0.0005866537103429437
step: 150, loss: 0.0011771984864026308
step: 160, loss: 0.042386364191770554
step: 170, loss: 0.0075525324791669846
step: 180, loss: 0.0059705157764256
step: 190, loss: 0.003216092474758625
step: 200, loss: 0.00037338980473577976
step: 210, loss: 0.002791555365547538
step: 220, loss: 0.19408658146858215
step: 230, loss: 0.00026975543005391955
step: 240, loss: 0.02806815877556801
step: 250, loss: 0.0002801694499794394
step: 260, loss: 0.0016273065702989697
step: 270, loss: 0.12690895795822144
step: 280, loss: 0.0002465401485096663
step: 290, loss: 0.0003438585845287889
step: 300, loss: 0.0015259674983099103
step: 310, loss: 0.004747084341943264
step: 320, loss: 0.008109857328236103
step: 330, loss: 0.015215746127068996
step: 340, loss: 0.0006411552894860506
step: 350, loss: 0.0003228556306567043
epoch 15: dev_f1=0.8195121951219512, f1=0.7326732673267327, best_f1=0.7563451776649747
step: 0, loss: 0.00027311628218740225
step: 10, loss: 0.07908713072538376
step: 20, loss: 0.000324408698361367
step: 30, loss: 0.09568408131599426
step: 40, loss: 0.00037560900091193616
step: 50, loss: 0.017211629077792168
step: 60, loss: 0.0026430741418153048
step: 70, loss: 0.0003774792712647468
step: 80, loss: 0.011806764639914036
step: 90, loss: 0.002937225392088294
step: 100, loss: 0.00794543419033289
step: 110, loss: 0.0012667978880926967
step: 120, loss: 0.0012617942411452532
step: 130, loss: 0.0011912763584405184
step: 140, loss: 0.004844028502702713
step: 150, loss: 0.0005864606937393546
step: 160, loss: 0.002175890142098069
step: 170, loss: 0.000321466795867309
step: 180, loss: 0.0009781703120097518
step: 190, loss: 0.008234863169491291
step: 200, loss: 0.11412980407476425
step: 210, loss: 0.0010244740406051278
step: 220, loss: 0.00015573506243526936
step: 230, loss: 0.006840108428150415
step: 240, loss: 0.00018029386410489678
step: 250, loss: 0.0012806544546037912
step: 260, loss: 0.0007468600524589419
step: 270, loss: 0.0016331697115674615
step: 280, loss: 0.0001291272637899965
step: 290, loss: 0.0006289532175287604
step: 300, loss: 0.0022363222669810057
step: 310, loss: 0.00030366305145435035
step: 320, loss: 0.0013816602295264602
step: 330, loss: 0.0012456055264919996
step: 340, loss: 0.0018605974037200212
step: 350, loss: 0.0011009675217792392
epoch 16: dev_f1=0.8308457711442786, f1=0.7397959183673468, best_f1=0.7563451776649747
step: 0, loss: 0.0010774743277579546
step: 10, loss: 0.00011857301433337852
step: 20, loss: 0.0002305896778125316
step: 30, loss: 0.02020050771534443
step: 40, loss: 0.001979437191039324
step: 50, loss: 0.00016768307250458747
step: 60, loss: 0.004511991050094366
step: 70, loss: 0.011178215965628624
step: 80, loss: 0.08058983087539673
step: 90, loss: 0.002991714049130678
step: 100, loss: 0.0013836094876751304
step: 110, loss: 0.00019835526472888887
step: 120, loss: 0.0031369300559163094
step: 130, loss: 0.02007993869483471
step: 140, loss: 0.03454352542757988
step: 150, loss: 0.004015488084405661
step: 160, loss: 0.03497226536273956
step: 170, loss: 0.0001257654366781935
step: 180, loss: 0.00025028668460436165
step: 190, loss: 0.0009203618974424899
step: 200, loss: 0.0007486391114071012
step: 210, loss: 0.0008286737138405442
step: 220, loss: 0.0020335097797214985
step: 230, loss: 0.0017049972666427493
step: 240, loss: 0.0059560444205999374
step: 250, loss: 0.0006928343791514635
step: 260, loss: 0.019115058705210686
step: 270, loss: 0.0029735383577644825
step: 280, loss: 0.0003169034607708454
step: 290, loss: 0.0005625833291560411
step: 300, loss: 0.0013839112361893058
step: 310, loss: 0.00019968497508671135
step: 320, loss: 0.00011641532182693481
step: 330, loss: 0.00039603107143193483
step: 340, loss: 0.013196034356951714
step: 350, loss: 0.000529991986695677
epoch 17: dev_f1=0.8308457711442786, f1=0.7384615384615385, best_f1=0.7563451776649747
step: 0, loss: 0.2351309359073639
step: 10, loss: 0.10430946201086044
step: 20, loss: 0.00017946417210623622
step: 30, loss: 0.00024351742467842996
step: 40, loss: 0.00019405505736358464
step: 50, loss: 0.0006112596020102501
step: 60, loss: 0.006599116139113903
step: 70, loss: 0.0004919095081277192
step: 80, loss: 0.01054568774998188
step: 90, loss: 0.0016741396393626928
step: 100, loss: 0.0003901899326592684
step: 110, loss: 0.0928732305765152
step: 120, loss: 0.02738437056541443
step: 130, loss: 0.0003553236892912537
step: 140, loss: 0.0025423613842576742
step: 150, loss: 0.00048251182306557894
step: 160, loss: 0.0074632135219872
step: 170, loss: 0.004192895721644163
step: 180, loss: 0.0007653774810023606
step: 190, loss: 0.00044520656228996813
step: 200, loss: 0.0030213017016649246
step: 210, loss: 0.0002979314886033535
step: 220, loss: 0.0021882744040340185
step: 230, loss: 0.0010760135482996702
step: 240, loss: 0.00023619754938408732
step: 250, loss: 0.101942278444767
step: 260, loss: 0.00021179355098865926
step: 270, loss: 0.006091234739869833
step: 280, loss: 0.0004355464188847691
step: 290, loss: 0.1248299777507782
step: 300, loss: 0.017815491184592247
step: 310, loss: 0.01508405152708292
step: 320, loss: 0.02380438894033432
step: 330, loss: 0.0008551655919291079
step: 340, loss: 0.039053939282894135
step: 350, loss: 0.007786068599671125
epoch 18: dev_f1=0.825, f1=0.734375, best_f1=0.7563451776649747
step: 0, loss: 0.003409602213650942
step: 10, loss: 0.00011305675434414297
step: 20, loss: 0.000549245742149651
step: 30, loss: 0.00041121503454633057
step: 40, loss: 0.0062830182723701
step: 50, loss: 0.0005169822252355516
step: 60, loss: 0.002041048603132367
step: 70, loss: 0.006842078175395727
step: 80, loss: 0.0004466404498089105
step: 90, loss: 0.0006754444329999387
step: 100, loss: 0.001079894369468093
step: 110, loss: 0.0002737044997047633
step: 120, loss: 0.00036728850682266057
step: 130, loss: 0.0001272820372832939
step: 140, loss: 0.0012152664130553603
step: 150, loss: 0.0005305518279783428
step: 160, loss: 0.06162991747260094
step: 170, loss: 0.00036013202043250203
step: 180, loss: 0.0009442142327316105
step: 190, loss: 0.00025856602587737143
step: 200, loss: 0.003970276098698378
step: 210, loss: 0.0012407220201566815
step: 220, loss: 0.0009465133189223707
step: 230, loss: 0.000382030731998384
step: 240, loss: 0.0003485546912997961
step: 250, loss: 0.0007275816169567406
step: 260, loss: 0.0003566516097635031
step: 270, loss: 0.0003689329023472965
step: 280, loss: 0.0024045491591095924
step: 290, loss: 0.03673176094889641
step: 300, loss: 0.0005678138695657253
step: 310, loss: 0.001559228403493762
step: 320, loss: 0.020998070016503334
step: 330, loss: 0.0008696840377524495
step: 340, loss: 0.000127525650896132
step: 350, loss: 0.0024521653540432453
epoch 19: dev_f1=0.8268733850129198, f1=0.7326203208556149, best_f1=0.7563451776649747
step: 0, loss: 0.01978365145623684
step: 10, loss: 0.00016714820230845362
step: 20, loss: 0.0034281047992408276
step: 30, loss: 0.0021589959505945444
step: 40, loss: 0.0002033600612776354
step: 50, loss: 0.0008337312610819936
step: 60, loss: 0.00025983413797803223
step: 70, loss: 0.00016431348922196776
step: 80, loss: 0.00045447645243257284
step: 90, loss: 0.0004460254276636988
step: 100, loss: 0.00028282954008318484
step: 110, loss: 0.00029629209893755615
step: 120, loss: 0.0006666692206636071
step: 130, loss: 0.00021605980873573571
step: 140, loss: 0.00012788108142558485
step: 150, loss: 0.0007227344322018325
step: 160, loss: 0.00013215054059401155
step: 170, loss: 0.00014377562911249697
step: 180, loss: 0.000999700860120356
step: 190, loss: 0.03485698625445366
step: 200, loss: 0.0005297053721733391
step: 210, loss: 0.0025765723548829556
step: 220, loss: 0.006381277460604906
step: 230, loss: 0.009202921763062477
step: 240, loss: 0.0013452291022986174
step: 250, loss: 0.04895668849349022
step: 260, loss: 0.0005059053655713797
step: 270, loss: 0.005868725944310427
step: 280, loss: 0.026834163814783096
step: 290, loss: 0.0006770557374693453
step: 300, loss: 0.00013583542022388428
step: 310, loss: 0.0002323462686035782
step: 320, loss: 0.0014064748538658023
step: 330, loss: 0.0004388687666505575
step: 340, loss: 0.015183503739535809
step: 350, loss: 0.00045727044926024973
epoch 20: dev_f1=0.8329048843187661, f1=0.7340425531914895, best_f1=0.7563451776649747
