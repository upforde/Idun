cuda
Device: cuda
step: 0, loss: 0.6660508513450623
step: 10, loss: 0.35215142369270325
step: 20, loss: 0.3170066475868225
step: 30, loss: 0.23644070327281952
step: 40, loss: 0.49759742617607117
step: 50, loss: 0.3142578601837158
step: 60, loss: 0.618095874786377
step: 70, loss: 0.3113280236721039
step: 80, loss: 0.35417330265045166
step: 90, loss: 0.2168540209531784
step: 100, loss: 0.3820513188838959
step: 110, loss: 0.1270117610692978
step: 120, loss: 0.22402282059192657
step: 130, loss: 0.3801961839199066
step: 140, loss: 0.22287316620349884
step: 150, loss: 0.2525312006473541
step: 160, loss: 0.1305302232503891
step: 170, loss: 0.2872852385044098
step: 180, loss: 0.2889758050441742
step: 190, loss: 0.16019955277442932
step: 200, loss: 0.18755042552947998
step: 210, loss: 0.36751094460487366
step: 220, loss: 0.47456514835357666
step: 230, loss: 0.3345949351787567
step: 240, loss: 0.13494819402694702
step: 250, loss: 0.3125457763671875
step: 260, loss: 0.36988699436187744
step: 270, loss: 0.24978138506412506
step: 280, loss: 0.19290952384471893
step: 290, loss: 0.27896714210510254
step: 300, loss: 0.8372986912727356
step: 310, loss: 0.2152356654405594
step: 320, loss: 0.408272922039032
step: 330, loss: 0.18851390480995178
step: 340, loss: 0.1733211725950241
step: 350, loss: 0.12391240894794464
epoch 1: dev_f1=0.4627249357326478, f1=0.32460732984293195, best_f1=0.32460732984293195
step: 0, loss: 0.19265738129615784
step: 10, loss: 0.3345251679420471
step: 20, loss: 0.10978174209594727
step: 30, loss: 0.4396441578865051
step: 40, loss: 0.3448825478553772
step: 50, loss: 0.16430765390396118
step: 60, loss: 0.2719912827014923
step: 70, loss: 0.26293379068374634
step: 80, loss: 0.13638819754123688
step: 90, loss: 0.2966710329055786
step: 100, loss: 0.23449397087097168
step: 110, loss: 0.22589966654777527
step: 120, loss: 0.2517249286174774
step: 130, loss: 0.2983104884624481
step: 140, loss: 0.361604779958725
step: 150, loss: 0.2584553062915802
step: 160, loss: 0.4139750599861145
step: 170, loss: 0.3103690445423126
step: 180, loss: 0.326183557510376
step: 190, loss: 0.3084457218647003
step: 200, loss: 0.277711421251297
step: 210, loss: 0.13332800567150116
step: 220, loss: 0.1273164004087448
step: 230, loss: 0.32765573263168335
step: 240, loss: 0.18529555201530457
step: 250, loss: 0.12484259903430939
step: 260, loss: 0.3761613368988037
step: 270, loss: 0.21872876584529877
step: 280, loss: 0.23297220468521118
step: 290, loss: 0.15195991098880768
step: 300, loss: 0.18844491243362427
step: 310, loss: 0.191910520195961
step: 320, loss: 0.2410905808210373
step: 330, loss: 0.30849331617355347
step: 340, loss: 0.07182701677083969
step: 350, loss: 0.37850677967071533
epoch 2: dev_f1=0.7688442211055277, f1=0.6307692307692306, best_f1=0.6307692307692306
step: 0, loss: 0.13120271265506744
step: 10, loss: 0.18079115450382233
step: 20, loss: 0.41670727729797363
step: 30, loss: 0.20284532010555267
step: 40, loss: 0.047929245978593826
step: 50, loss: 0.199212446808815
step: 60, loss: 0.27202582359313965
step: 70, loss: 0.14739567041397095
step: 80, loss: 0.2433326095342636
step: 90, loss: 0.04977450147271156
step: 100, loss: 0.37055280804634094
step: 110, loss: 0.23097608983516693
step: 120, loss: 0.0682767704129219
step: 130, loss: 0.1379815936088562
step: 140, loss: 0.11576587706804276
step: 150, loss: 0.13316375017166138
step: 160, loss: 0.17214612662792206
step: 170, loss: 0.18231070041656494
step: 180, loss: 0.17618973553180695
step: 190, loss: 0.12109094858169556
step: 200, loss: 0.2652616798877716
step: 210, loss: 0.062020689249038696
step: 220, loss: 0.2004694789648056
step: 230, loss: 0.08936242759227753
step: 240, loss: 0.21992641687393188
step: 250, loss: 0.03921283408999443
step: 260, loss: 0.14535562694072723
step: 270, loss: 0.1899438053369522
step: 280, loss: 0.1308608204126358
step: 290, loss: 0.2528420388698578
step: 300, loss: 0.42493337392807007
step: 310, loss: 0.1965358555316925
step: 320, loss: 0.4254165589809418
step: 330, loss: 0.15307794511318207
step: 340, loss: 0.3117346167564392
step: 350, loss: 0.24224747717380524
epoch 3: dev_f1=0.8045454545454546, f1=0.7235023041474654, best_f1=0.7235023041474654
step: 0, loss: 0.17285028100013733
step: 10, loss: 0.2677733898162842
step: 20, loss: 0.0716027021408081
step: 30, loss: 0.09186995774507523
step: 40, loss: 0.11282532662153244
step: 50, loss: 0.13488270342350006
step: 60, loss: 0.27636855840682983
step: 70, loss: 0.17650939524173737
step: 80, loss: 0.23268873989582062
step: 90, loss: 0.3156985342502594
step: 100, loss: 0.24063795804977417
step: 110, loss: 0.3499319851398468
step: 120, loss: 0.11963941156864166
step: 130, loss: 0.04082679748535156
step: 140, loss: 0.13974322378635406
step: 150, loss: 0.20173819363117218
step: 160, loss: 0.10150691866874695
step: 170, loss: 0.1404208391904831
step: 180, loss: 0.09087307751178741
step: 190, loss: 0.0904696136713028
step: 200, loss: 0.08883591741323471
step: 210, loss: 0.13872212171554565
step: 220, loss: 0.3851224184036255
step: 230, loss: 0.12002682685852051
step: 240, loss: 0.06382795423269272
step: 250, loss: 0.0864219143986702
step: 260, loss: 0.06696557253599167
step: 270, loss: 0.07859455794095993
step: 280, loss: 0.1989409625530243
step: 290, loss: 0.12829236686229706
step: 300, loss: 0.17423290014266968
step: 310, loss: 0.11891606450080872
step: 320, loss: 0.2491416335105896
step: 330, loss: 0.04900723695755005
step: 340, loss: 0.12559789419174194
step: 350, loss: 0.07290440797805786
epoch 4: dev_f1=0.7882882882882883, f1=0.6916299559471365, best_f1=0.7235023041474654
step: 0, loss: 0.05357084423303604
step: 10, loss: 0.04253358021378517
step: 20, loss: 0.08421646058559418
step: 30, loss: 0.24525989592075348
step: 40, loss: 0.10645544528961182
step: 50, loss: 0.1448352038860321
step: 60, loss: 0.13379772007465363
step: 70, loss: 0.1373400241136551
step: 80, loss: 0.1452435404062271
step: 90, loss: 0.1039295569062233
step: 100, loss: 0.23456025123596191
step: 110, loss: 0.11718562990427017
step: 120, loss: 0.07796813547611237
step: 130, loss: 0.10945504158735275
step: 140, loss: 0.19127535820007324
step: 150, loss: 0.06009053438901901
step: 160, loss: 0.09416715055704117
step: 170, loss: 0.12566648423671722
step: 180, loss: 0.268656849861145
step: 190, loss: 0.19656585156917572
step: 200, loss: 0.1878737509250641
step: 210, loss: 0.06676319241523743
step: 220, loss: 0.09544805437326431
step: 230, loss: 0.13160796463489532
step: 240, loss: 0.12024708092212677
step: 250, loss: 0.03613673523068428
step: 260, loss: 0.049640633165836334
step: 270, loss: 0.2678971588611603
step: 280, loss: 0.06546161323785782
step: 290, loss: 0.04886464402079582
step: 300, loss: 0.0723046362400055
step: 310, loss: 0.047733090817928314
step: 320, loss: 0.18042722344398499
step: 330, loss: 0.14260299503803253
step: 340, loss: 0.13509994745254517
step: 350, loss: 0.06472498178482056
epoch 5: dev_f1=0.8194444444444444, f1=0.7341176470588235, best_f1=0.7341176470588235
step: 0, loss: 0.07301287353038788
step: 10, loss: 0.01778055727481842
step: 20, loss: 0.16961966454982758
step: 30, loss: 0.11404692381620407
step: 40, loss: 0.18784351646900177
step: 50, loss: 0.05881846696138382
step: 60, loss: 0.046678029000759125
step: 70, loss: 0.06898128986358643
step: 80, loss: 0.038789764046669006
step: 90, loss: 0.17029686272144318
step: 100, loss: 0.03498251363635063
step: 110, loss: 0.052198559045791626
step: 120, loss: 0.19301345944404602
step: 130, loss: 0.02072921022772789
step: 140, loss: 0.09972482174634933
step: 150, loss: 0.1747690588235855
step: 160, loss: 0.026006294414401054
step: 170, loss: 0.24403777718544006
step: 180, loss: 0.07448366284370422
step: 190, loss: 0.1651790589094162
step: 200, loss: 0.06822092086076736
step: 210, loss: 0.15338753163814545
step: 220, loss: 0.021539906039834023
step: 230, loss: 0.047579292207956314
step: 240, loss: 0.07000894844532013
step: 250, loss: 0.2002466768026352
step: 260, loss: 0.08485566824674606
step: 270, loss: 0.012579928152263165
step: 280, loss: 0.059430383145809174
step: 290, loss: 0.07581205666065216
step: 300, loss: 0.05737101659178734
step: 310, loss: 0.06740424036979675
step: 320, loss: 0.24092134833335876
step: 330, loss: 0.10848366469144821
step: 340, loss: 0.11517000198364258
step: 350, loss: 0.0547824427485466
epoch 6: dev_f1=0.8126520681265207, f1=0.7227722772277227, best_f1=0.7341176470588235
step: 0, loss: 0.07114525139331818
step: 10, loss: 0.2299739122390747
step: 20, loss: 0.21586576104164124
step: 30, loss: 0.02083146758377552
step: 40, loss: 0.008645557798445225
step: 50, loss: 0.15175065398216248
step: 60, loss: 0.08051613718271255
step: 70, loss: 0.0030038561671972275
step: 80, loss: 0.008717772550880909
step: 90, loss: 0.064200259745121
step: 100, loss: 0.10535356402397156
step: 110, loss: 0.025653457269072533
step: 120, loss: 0.06287136673927307
step: 130, loss: 0.23424775898456573
step: 140, loss: 0.09289754182100296
step: 150, loss: 0.007237555459141731
step: 160, loss: 0.18296048045158386
step: 170, loss: 0.031865641474723816
step: 180, loss: 0.11721375584602356
step: 190, loss: 0.06487882137298584
step: 200, loss: 0.015301701612770557
step: 210, loss: 0.057397354394197464
step: 220, loss: 0.1324043571949005
step: 230, loss: 0.039267584681510925
step: 240, loss: 0.03451014310121536
step: 250, loss: 0.030146120116114616
step: 260, loss: 0.03888578712940216
step: 270, loss: 0.11267820745706558
step: 280, loss: 0.09929274767637253
step: 290, loss: 0.13997690379619598
step: 300, loss: 0.1327820122241974
step: 310, loss: 0.010620242916047573
step: 320, loss: 0.08053428679704666
step: 330, loss: 0.06758469343185425
step: 340, loss: 0.0548630952835083
step: 350, loss: 0.07569749653339386
epoch 7: dev_f1=0.8388746803069055, f1=0.7301587301587301, best_f1=0.7301587301587301
step: 0, loss: 0.022241849452257156
step: 10, loss: 0.06701380759477615
step: 20, loss: 0.008094608783721924
step: 30, loss: 0.015433277003467083
step: 40, loss: 0.07928141951560974
step: 50, loss: 0.008040059357881546
step: 60, loss: 0.02266968972980976
step: 70, loss: 0.08818664401769638
step: 80, loss: 0.09974098205566406
step: 90, loss: 0.014580979943275452
step: 100, loss: 0.004282289184629917
step: 110, loss: 0.01848382130265236
step: 120, loss: 0.01938457414507866
step: 130, loss: 0.006501365453004837
step: 140, loss: 0.15058085322380066
step: 150, loss: 0.04859538748860359
step: 160, loss: 0.03486169874668121
step: 170, loss: 0.130283921957016
step: 180, loss: 0.21159881353378296
step: 190, loss: 0.0054285889491438866
step: 200, loss: 0.3099268078804016
step: 210, loss: 0.13331247866153717
step: 220, loss: 0.1814737319946289
step: 230, loss: 0.08964794874191284
step: 240, loss: 0.06945422291755676
step: 250, loss: 0.028922157362103462
step: 260, loss: 0.11473135650157928
step: 270, loss: 0.1612662672996521
step: 280, loss: 0.011883653700351715
step: 290, loss: 0.1112685352563858
step: 300, loss: 0.23551110923290253
step: 310, loss: 0.037508103996515274
step: 320, loss: 0.0545680858194828
step: 330, loss: 0.09615455567836761
step: 340, loss: 0.03198223561048508
step: 350, loss: 0.06679724156856537
epoch 8: dev_f1=0.813953488372093, f1=0.7307692307692307, best_f1=0.7301587301587301
step: 0, loss: 0.04669664427638054
step: 10, loss: 0.003391026984900236
step: 20, loss: 0.009525535628199577
step: 30, loss: 0.00345783238299191
step: 40, loss: 0.014021629467606544
step: 50, loss: 0.05259126424789429
step: 60, loss: 0.041454434394836426
step: 70, loss: 0.02126559242606163
step: 80, loss: 0.04921926185488701
step: 90, loss: 0.056057970970869064
step: 100, loss: 0.1849254071712494
step: 110, loss: 0.10671617835760117
step: 120, loss: 0.0053116255439817905
step: 130, loss: 0.03609662503004074
step: 140, loss: 0.10627422481775284
step: 150, loss: 0.10940692573785782
step: 160, loss: 0.0935516282916069
step: 170, loss: 0.1479845941066742
step: 180, loss: 0.06997326016426086
step: 190, loss: 0.09985765814781189
step: 200, loss: 0.014117891900241375
step: 210, loss: 0.03441779315471649
step: 220, loss: 0.03662243112921715
step: 230, loss: 0.009010287001729012
step: 240, loss: 0.05204097554087639
step: 250, loss: 0.08480328321456909
step: 260, loss: 0.0983550027012825
step: 270, loss: 0.028813665732741356
step: 280, loss: 0.030880481004714966
step: 290, loss: 0.013231784105300903
step: 300, loss: 0.011034063063561916
step: 310, loss: 0.04034337401390076
step: 320, loss: 0.02056940831243992
step: 330, loss: 0.16538776457309723
step: 340, loss: 0.05164222791790962
step: 350, loss: 0.073367640376091
epoch 9: dev_f1=0.8229426433915212, f1=0.7173913043478262, best_f1=0.7301587301587301
step: 0, loss: 0.00548879848793149
step: 10, loss: 0.07015150785446167
step: 20, loss: 0.09320694953203201
step: 30, loss: 0.03683755174279213
step: 40, loss: 0.051646120846271515
step: 50, loss: 0.19039562344551086
step: 60, loss: 0.03915019333362579
step: 70, loss: 0.10430315881967545
step: 80, loss: 0.004885589238256216
step: 90, loss: 0.006785282865166664
step: 100, loss: 0.019711731001734734
step: 110, loss: 0.0024475131649523973
step: 120, loss: 0.010387050919234753
step: 130, loss: 0.005233448464423418
step: 140, loss: 0.006431765854358673
step: 150, loss: 0.0695517286658287
step: 160, loss: 0.07745447009801865
step: 170, loss: 0.08847526460886002
step: 180, loss: 0.004912886302918196
step: 190, loss: 0.10195394605398178
step: 200, loss: 0.02659229002892971
step: 210, loss: 0.018958626314997673
step: 220, loss: 0.10622153431177139
step: 230, loss: 0.039894748479127884
step: 240, loss: 0.004785466473549604
step: 250, loss: 0.0014807109255343676
step: 260, loss: 0.028523264452815056
step: 270, loss: 0.0012438871199265122
step: 280, loss: 0.01348926778882742
step: 290, loss: 0.05884816125035286
step: 300, loss: 0.012640123255550861
step: 310, loss: 0.03213641792535782
step: 320, loss: 0.07180976867675781
step: 330, loss: 0.048529237508773804
step: 340, loss: 0.08728071302175522
step: 350, loss: 0.004825399722903967
epoch 10: dev_f1=0.831353919239905, f1=0.7231920199501248, best_f1=0.7301587301587301
step: 0, loss: 0.027279004454612732
step: 10, loss: 0.10641707479953766
step: 20, loss: 0.08410661667585373
step: 30, loss: 0.009752633981406689
step: 40, loss: 0.002850446617230773
step: 50, loss: 0.010663341730833054
step: 60, loss: 0.0032215823885053396
step: 70, loss: 0.048668812960386276
step: 80, loss: 0.0027225548401474953
step: 90, loss: 0.007715302053838968
step: 100, loss: 0.10633619129657745
step: 110, loss: 0.012445949949324131
step: 120, loss: 0.011798426508903503
step: 130, loss: 0.05061226338148117
step: 140, loss: 0.023088542744517326
step: 150, loss: 0.025737682357430458
step: 160, loss: 0.0032839891500771046
step: 170, loss: 0.09254177659749985
step: 180, loss: 0.012419469654560089
step: 190, loss: 0.01726216822862625
step: 200, loss: 0.009024297818541527
step: 210, loss: 0.014853616245090961
step: 220, loss: 0.07963500916957855
step: 230, loss: 0.018350796774029732
step: 240, loss: 0.008477223105728626
step: 250, loss: 0.005596482660621405
step: 260, loss: 0.010133633390069008
step: 270, loss: 0.029794447124004364
step: 280, loss: 0.054344579577445984
step: 290, loss: 0.05329484865069389
step: 300, loss: 0.006337159778922796
step: 310, loss: 0.010017918422818184
step: 320, loss: 0.006670697126537561
step: 330, loss: 0.0034853213001042604
step: 340, loss: 0.0323285274207592
step: 350, loss: 0.13217654824256897
epoch 11: dev_f1=0.8246913580246913, f1=0.7365728900255755, best_f1=0.7301587301587301
step: 0, loss: 0.002410170855000615
step: 10, loss: 0.0631382018327713
step: 20, loss: 0.024534408003091812
step: 30, loss: 0.21223053336143494
step: 40, loss: 0.11490771174430847
step: 50, loss: 0.005721480585634708
step: 60, loss: 0.017589734867215157
step: 70, loss: 0.008196445181965828
step: 80, loss: 0.016535945236682892
step: 90, loss: 0.012660104781389236
step: 100, loss: 0.03284366428852081
step: 110, loss: 0.00034028227673843503
step: 120, loss: 0.004027039743959904
step: 130, loss: 0.10090231895446777
step: 140, loss: 0.01474052481353283
step: 150, loss: 0.0035435580648481846
step: 160, loss: 0.0031478579621762037
step: 170, loss: 0.0180438794195652
step: 180, loss: 0.011329195462167263
step: 190, loss: 0.0010084349196404219
step: 200, loss: 0.05859280377626419
step: 210, loss: 0.001702172914519906
step: 220, loss: 0.00024459685664623976
step: 230, loss: 0.025872157886624336
step: 240, loss: 0.014774755574762821
step: 250, loss: 0.002866641618311405
step: 260, loss: 0.0002508806937839836
step: 270, loss: 0.04862425848841667
step: 280, loss: 0.06337394565343857
step: 290, loss: 0.01410715188831091
step: 300, loss: 0.002476045163348317
step: 310, loss: 0.007303423248231411
step: 320, loss: 0.05578853189945221
step: 330, loss: 0.013386101461946964
step: 340, loss: 0.01726358011364937
step: 350, loss: 0.024708177894353867
epoch 12: dev_f1=0.8450704225352113, f1=0.7263922518159807, best_f1=0.7263922518159807
step: 0, loss: 0.0134537098929286
step: 10, loss: 0.0437726229429245
step: 20, loss: 0.0023193571250885725
step: 30, loss: 0.028013214468955994
step: 40, loss: 0.001578995492309332
step: 50, loss: 0.02154156193137169
step: 60, loss: 0.002792201703414321
step: 70, loss: 0.015557599253952503
step: 80, loss: 0.010949896648526192
step: 90, loss: 0.0016874631401151419
step: 100, loss: 0.04601549729704857
step: 110, loss: 0.005567630287259817
step: 120, loss: 0.019880598410964012
step: 130, loss: 0.09554236382246017
step: 140, loss: 0.031689416617155075
step: 150, loss: 0.058943841606378555
step: 160, loss: 0.06840343028306961
step: 170, loss: 0.00818187277764082
step: 180, loss: 0.08597257733345032
step: 190, loss: 0.02028711512684822
step: 200, loss: 0.006828214507550001
step: 210, loss: 0.0007410519174300134
step: 220, loss: 0.00954513344913721
step: 230, loss: 0.011548157781362534
step: 240, loss: 0.0437641479074955
step: 250, loss: 0.0015496015548706055
step: 260, loss: 0.05301738902926445
step: 270, loss: 0.036032143980264664
step: 280, loss: 0.03963091969490051
step: 290, loss: 0.017376448959112167
step: 300, loss: 0.002820659428834915
step: 310, loss: 0.0006058731232769787
step: 320, loss: 0.019755396991968155
step: 330, loss: 0.0015162995550781488
step: 340, loss: 0.0009639272466301918
step: 350, loss: 0.0033957723062485456
epoch 13: dev_f1=0.8267326732673267, f1=0.7240506329113925, best_f1=0.7263922518159807
step: 0, loss: 0.040115322917699814
step: 10, loss: 0.002517466666176915
step: 20, loss: 0.0004847687960136682
step: 30, loss: 0.018544478341937065
step: 40, loss: 0.005442689172923565
step: 50, loss: 0.033644165843725204
step: 60, loss: 0.0009309382294304669
step: 70, loss: 0.010728368535637856
step: 80, loss: 0.004459530580788851
step: 90, loss: 0.010834316723048687
step: 100, loss: 0.0064553688280284405
step: 110, loss: 0.01274507399648428
step: 120, loss: 0.0009076084825210273
step: 130, loss: 0.0039071496576070786
step: 140, loss: 0.12601864337921143
step: 150, loss: 0.017402134835720062
step: 160, loss: 0.0004254288214724511
step: 170, loss: 0.03778384253382683
step: 180, loss: 0.0005849406006745994
step: 190, loss: 0.010093779303133488
step: 200, loss: 0.2070600837469101
step: 210, loss: 0.0004333364777266979
step: 220, loss: 0.0036167113576084375
step: 230, loss: 0.027633661404252052
step: 240, loss: 0.003294743364676833
step: 250, loss: 0.04409172013401985
step: 260, loss: 0.0008443674887530506
step: 270, loss: 0.0003141437191516161
step: 280, loss: 0.07563614845275879
step: 290, loss: 0.02111908048391342
step: 300, loss: 0.005266537889838219
step: 310, loss: 0.06134604290127754
step: 320, loss: 0.0010519667994230986
step: 330, loss: 0.0011614308459684253
step: 340, loss: 0.00043833101517520845
step: 350, loss: 0.0001918510824907571
epoch 14: dev_f1=0.8250652741514359, f1=0.7413333333333334, best_f1=0.7263922518159807
step: 0, loss: 0.10352777689695358
step: 10, loss: 0.01629784144461155
step: 20, loss: 0.0007743582245893776
step: 30, loss: 0.0004171841137576848
step: 40, loss: 9.746540308697149e-05
step: 50, loss: 0.005565276835113764
step: 60, loss: 0.000410448235925287
step: 70, loss: 0.07305213063955307
step: 80, loss: 0.16563113033771515
step: 90, loss: 0.007093528751283884
step: 100, loss: 0.06767542660236359
step: 110, loss: 0.006194718647748232
step: 120, loss: 0.0454324446618557
step: 130, loss: 0.0005285220686346292
step: 140, loss: 0.04014454409480095
step: 150, loss: 0.006854135077446699
step: 160, loss: 0.0004925306420773268
step: 170, loss: 0.19719892740249634
step: 180, loss: 0.0011985235614702106
step: 190, loss: 0.0043739196844398975
step: 200, loss: 0.0030185673385858536
step: 210, loss: 0.017049789428710938
step: 220, loss: 0.0032780286855995655
step: 230, loss: 0.00024013436632230878
step: 240, loss: 0.0005738275358453393
step: 250, loss: 0.09995442628860474
step: 260, loss: 0.00031938584288582206
step: 270, loss: 0.0010624730493873358
step: 280, loss: 0.03442763537168503
step: 290, loss: 0.005183060187846422
step: 300, loss: 0.009341780096292496
step: 310, loss: 0.0047270553186535835
step: 320, loss: 0.08398007601499557
step: 330, loss: 0.02578308992087841
step: 340, loss: 0.00044068670831620693
step: 350, loss: 0.0005854851333424449
epoch 15: dev_f1=0.8372093023255814, f1=0.7301587301587301, best_f1=0.7263922518159807
step: 0, loss: 0.0002327877446077764
step: 10, loss: 0.0476074181497097
step: 20, loss: 0.00019039137987419963
step: 30, loss: 0.0002941427519544959
step: 40, loss: 0.00036005457513965666
step: 50, loss: 0.0008853364270180464
step: 60, loss: 0.0005270104738883674
step: 70, loss: 0.001380580011755228
step: 80, loss: 0.0011911611072719097
step: 90, loss: 0.0015921001322567463
step: 100, loss: 0.20477299392223358
step: 110, loss: 0.0029924814589321613
step: 120, loss: 0.0008614278631284833
step: 130, loss: 0.017830749973654747
step: 140, loss: 0.001410256139934063
step: 150, loss: 0.0006068484508432448
step: 160, loss: 0.0012944729533046484
step: 170, loss: 0.00152035360224545
step: 180, loss: 0.002603863598778844
step: 190, loss: 0.0010397344594821334
step: 200, loss: 0.00027015365776605904
step: 210, loss: 0.00286903977394104
step: 220, loss: 0.0005492522032000124
step: 230, loss: 0.0022986792027950287
step: 240, loss: 0.0023767156526446342
step: 250, loss: 0.002288799500092864
step: 260, loss: 0.03473860025405884
step: 270, loss: 0.0012689088471233845
step: 280, loss: 0.0061290208250284195
step: 290, loss: 0.0018865024903789163
step: 300, loss: 0.00022879520838614553
step: 310, loss: 0.0016628766898065805
step: 320, loss: 0.00040631109732203186
step: 330, loss: 0.0003668864374049008
step: 340, loss: 0.0087994784116745
step: 350, loss: 0.0011628473876044154
epoch 16: dev_f1=0.8277511961722487, f1=0.7342995169082126, best_f1=0.7263922518159807
step: 0, loss: 0.051459334790706635
step: 10, loss: 0.0015640276251360774
step: 20, loss: 0.0035667731426656246
step: 30, loss: 0.002094789408147335
step: 40, loss: 0.02998463809490204
step: 50, loss: 0.00010592295438982546
step: 60, loss: 0.006690359208732843
step: 70, loss: 0.0006136349402368069
step: 80, loss: 0.00021168714738450944
step: 90, loss: 0.0008692776318639517
step: 100, loss: 0.009855013340711594
step: 110, loss: 0.011675679124891758
step: 120, loss: 0.0026378557085990906
step: 130, loss: 0.02375832572579384
step: 140, loss: 0.0012403433211147785
step: 150, loss: 0.024476250633597374
step: 160, loss: 0.02456020563840866
step: 170, loss: 0.0030887979082763195
step: 180, loss: 0.00014553716755472124
step: 190, loss: 0.04846099764108658
step: 200, loss: 0.00045263726497069
step: 210, loss: 0.0017311801202595234
step: 220, loss: 0.0038212656509131193
step: 230, loss: 0.00015478007844649255
step: 240, loss: 0.0018289542058482766
step: 250, loss: 0.0020037514623254538
step: 260, loss: 0.1454862505197525
step: 270, loss: 0.018965937197208405
step: 280, loss: 0.0029675972182303667
step: 290, loss: 0.04168013855814934
step: 300, loss: 0.08541180938482285
step: 310, loss: 0.0377303771674633
step: 320, loss: 0.00010069194104289636
step: 330, loss: 0.0005804536631330848
step: 340, loss: 0.009596048854291439
step: 350, loss: 0.0007690426427870989
epoch 17: dev_f1=0.8258706467661691, f1=0.7204030226700252, best_f1=0.7263922518159807
step: 0, loss: 0.00048155832337215543
step: 10, loss: 0.054471250623464584
step: 20, loss: 0.0005157055566087365
step: 30, loss: 0.0032696479465812445
step: 40, loss: 0.001206889864988625
step: 50, loss: 0.0031666220165789127
step: 60, loss: 0.0002338549675187096
step: 70, loss: 0.026563705876469612
step: 80, loss: 0.004268236458301544
step: 90, loss: 0.026034658774733543
step: 100, loss: 0.0008033411577343941
step: 110, loss: 0.0004066601977683604
step: 120, loss: 0.0005998308770358562
step: 130, loss: 0.004384451545774937
step: 140, loss: 0.015001155436038971
step: 150, loss: 0.0007943891687318683
step: 160, loss: 0.012794319540262222
step: 170, loss: 0.09900859743356705
step: 180, loss: 0.0005186641355976462
step: 190, loss: 0.00012436376709956676
step: 200, loss: 0.01369666401296854
step: 210, loss: 0.0018940309528261423
step: 220, loss: 0.00119387439917773
step: 230, loss: 0.00014394414029084146
step: 240, loss: 0.0002972338115796447
step: 250, loss: 0.0009850034257397056
step: 260, loss: 0.12660859525203705
step: 270, loss: 0.03205346316099167
step: 280, loss: 0.005522285588085651
step: 290, loss: 7.442139030899853e-05
step: 300, loss: 0.000789904035627842
step: 310, loss: 0.0017969111213460565
step: 320, loss: 9.390027116751298e-05
step: 330, loss: 0.005412592086941004
step: 340, loss: 0.00011692336556734517
step: 350, loss: 0.003358331276103854
epoch 18: dev_f1=0.8229166666666666, f1=0.7049180327868854, best_f1=0.7263922518159807
step: 0, loss: 0.0021440181881189346
step: 10, loss: 0.00013856388977728784
step: 20, loss: 0.0007671171333640814
step: 30, loss: 0.0004112217284273356
step: 40, loss: 0.0037525370717048645
step: 50, loss: 0.0003575905575416982
step: 60, loss: 0.0007274096133187413
step: 70, loss: 8.660358435008675e-05
step: 80, loss: 0.0004676172975450754
step: 90, loss: 0.00356121314689517
step: 100, loss: 0.003117160638794303
step: 110, loss: 0.0004855906590819359
step: 120, loss: 0.002279348438605666
step: 130, loss: 0.016595033928751945
step: 140, loss: 0.004337283316999674
step: 150, loss: 0.0066167209297418594
step: 160, loss: 0.004039764869958162
step: 170, loss: 0.0205399002879858
step: 180, loss: 0.00014542137796524912
step: 190, loss: 0.028469610959291458
step: 200, loss: 0.007605092599987984
step: 210, loss: 0.00019136504852212965
step: 220, loss: 0.0024503683671355247
step: 230, loss: 0.00023692881222814322
step: 240, loss: 0.000579114886932075
step: 250, loss: 0.00027006148593500257
step: 260, loss: 0.02498433366417885
step: 270, loss: 0.00038012670120224357
step: 280, loss: 0.00035007880069315434
step: 290, loss: 0.0007885615923441947
step: 300, loss: 0.0003422033623792231
step: 310, loss: 0.012403356842696667
step: 320, loss: 0.0001776206772774458
step: 330, loss: 0.0001038083282765001
step: 340, loss: 0.002467835322022438
step: 350, loss: 0.00014337907487060875
epoch 19: dev_f1=0.8232445520581113, f1=0.7281553398058251, best_f1=0.7263922518159807
step: 0, loss: 0.00019151248852722347
step: 10, loss: 0.00039434153586626053
step: 20, loss: 0.000340752478223294
step: 30, loss: 0.0010145814158022404
step: 40, loss: 0.00014853566244710237
step: 50, loss: 0.0009304177947342396
step: 60, loss: 0.0009332274203188717
step: 70, loss: 0.00012790442269761115
step: 80, loss: 0.0008581540896557271
step: 90, loss: 0.010162916034460068
step: 100, loss: 0.01850893907248974
step: 110, loss: 0.00031672639306634665
step: 120, loss: 0.0005051178741268814
step: 130, loss: 0.00012175135634606704
step: 140, loss: 0.00012617446191143245
step: 150, loss: 0.0005115207750350237
step: 160, loss: 0.015753645449876785
step: 170, loss: 0.0030313991010189056
step: 180, loss: 0.00031296961242333055
step: 190, loss: 0.0001511456648586318
step: 200, loss: 9.661017975304276e-05
step: 210, loss: 0.06699658185243607
step: 220, loss: 0.00029183694277890027
step: 230, loss: 0.00013696560927201062
step: 240, loss: 0.00026726064970716834
step: 250, loss: 0.007515507750213146
step: 260, loss: 0.0011031703324988484
step: 270, loss: 0.0022875836584717035
step: 280, loss: 0.015615775249898434
step: 290, loss: 0.01398093905299902
step: 300, loss: 0.0010639519896358252
step: 310, loss: 0.00031463263439945877
step: 320, loss: 0.00010151583410333842
step: 330, loss: 0.000166614176123403
step: 340, loss: 0.0020799937192350626
step: 350, loss: 0.0047593265771865845
epoch 20: dev_f1=0.8337468982630273, f1=0.730478589420655, best_f1=0.7263922518159807
