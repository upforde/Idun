cuda
Device: cuda
step: 0, loss: 0.6833669543266296
step: 10, loss: 0.6992599368095398
step: 20, loss: 0.4671861231327057
step: 30, loss: 0.3816453814506531
step: 40, loss: 0.26774710416793823
step: 50, loss: 0.5141284465789795
step: 60, loss: 0.4171505868434906
step: 70, loss: 0.37707820534706116
step: 80, loss: 0.31033092737197876
step: 90, loss: 0.6291153430938721
step: 100, loss: 0.43181467056274414
step: 110, loss: 0.43520498275756836
step: 120, loss: 0.30211329460144043
step: 130, loss: 0.1578456461429596
step: 140, loss: 0.17762377858161926
step: 150, loss: 0.23321187496185303
step: 160, loss: 0.4475235342979431
step: 170, loss: 0.2538861632347107
step: 180, loss: 0.486263245344162
step: 190, loss: 0.37377527356147766
step: 200, loss: 0.43625253438949585
step: 210, loss: 0.35354411602020264
step: 220, loss: 0.4131304621696472
step: 230, loss: 0.30139264464378357
step: 240, loss: 0.16240210831165314
step: 250, loss: 0.2959095537662506
step: 260, loss: 0.3511708676815033
step: 270, loss: 0.3346819579601288
step: 280, loss: 0.3307337760925293
step: 290, loss: 0.29657191038131714
step: 300, loss: 0.30474916100502014
step: 310, loss: 0.22110861539840698
step: 320, loss: 0.1708998829126358
step: 330, loss: 0.2397620528936386
step: 340, loss: 0.29617565870285034
step: 350, loss: 0.3197880983352661
epoch 1: dev_f1=0.42105263157894735, f1=0.3461538461538461, best_f1=0.3461538461538461
step: 0, loss: 0.3384242057800293
step: 10, loss: 0.25507524609565735
step: 20, loss: 0.16188769042491913
step: 30, loss: 0.3250178098678589
step: 40, loss: 0.24366961419582367
step: 50, loss: 0.2466551810503006
step: 60, loss: 0.35210108757019043
step: 70, loss: 0.4408603310585022
step: 80, loss: 0.5092973113059998
step: 90, loss: 0.22231993079185486
step: 100, loss: 0.4074242115020752
step: 110, loss: 0.19351357221603394
step: 120, loss: 0.4164755940437317
step: 130, loss: 0.1395573914051056
step: 140, loss: 0.43154993653297424
step: 150, loss: 0.21432387828826904
step: 160, loss: 0.17115071415901184
step: 170, loss: 0.06046591326594353
step: 180, loss: 0.25835004448890686
step: 190, loss: 0.09862245619297028
step: 200, loss: 0.2510387897491455
step: 210, loss: 0.28193068504333496
step: 220, loss: 0.3654596507549286
step: 230, loss: 0.5550282001495361
step: 240, loss: 0.3778800964355469
step: 250, loss: 0.15205559134483337
step: 260, loss: 0.2714742124080658
step: 270, loss: 0.23802459239959717
step: 280, loss: 0.30922606587409973
step: 290, loss: 0.2635703384876251
step: 300, loss: 0.22347107529640198
step: 310, loss: 0.19207964837551117
step: 320, loss: 0.23139525949954987
step: 330, loss: 0.31422868371009827
step: 340, loss: 0.13960571587085724
step: 350, loss: 0.30881357192993164
epoch 2: dev_f1=0.7352941176470589, f1=0.6734177215189874, best_f1=0.6734177215189874
step: 0, loss: 0.17251931130886078
step: 10, loss: 0.1891818642616272
step: 20, loss: 0.23679356276988983
step: 30, loss: 0.1881280541419983
step: 40, loss: 0.1222267597913742
step: 50, loss: 0.14744345843791962
step: 60, loss: 0.18495233356952667
step: 70, loss: 0.3041478395462036
step: 80, loss: 0.1375051587820053
step: 90, loss: 0.21156218647956848
step: 100, loss: 0.1520014852285385
step: 110, loss: 0.29410520195961
step: 120, loss: 0.15518169105052948
step: 130, loss: 0.27753597497940063
step: 140, loss: 0.2605253756046295
step: 150, loss: 0.14734365046024323
step: 160, loss: 0.2632085382938385
step: 170, loss: 0.19733156263828278
step: 180, loss: 0.2033732831478119
step: 190, loss: 0.3125120997428894
step: 200, loss: 0.18265153467655182
step: 210, loss: 0.17802810668945312
step: 220, loss: 0.3255062699317932
step: 230, loss: 0.2792627513408661
step: 240, loss: 0.22933827340602875
step: 250, loss: 0.16250872611999512
step: 260, loss: 0.08422389626502991
step: 270, loss: 0.08102636784315109
step: 280, loss: 0.043512485921382904
step: 290, loss: 0.22652286291122437
step: 300, loss: 0.3869473934173584
step: 310, loss: 0.08397074043750763
step: 320, loss: 0.13198262453079224
step: 330, loss: 0.08466391265392303
step: 340, loss: 0.07692950963973999
step: 350, loss: 0.18623359501361847
epoch 3: dev_f1=0.810304449648712, f1=0.7523809523809523, best_f1=0.7523809523809523
step: 0, loss: 0.12505407631397247
step: 10, loss: 0.32286742329597473
step: 20, loss: 0.13631238043308258
step: 30, loss: 0.044370461255311966
step: 40, loss: 0.28463634848594666
step: 50, loss: 0.26252561807632446
step: 60, loss: 0.23409050703048706
step: 70, loss: 0.26991671323776245
step: 80, loss: 0.08489073067903519
step: 90, loss: 0.05581013113260269
step: 100, loss: 0.3403444290161133
step: 110, loss: 0.0938231572508812
step: 120, loss: 0.023913919925689697
step: 130, loss: 0.13563939929008484
step: 140, loss: 0.0776415765285492
step: 150, loss: 0.46177375316619873
step: 160, loss: 0.07988044619560242
step: 170, loss: 0.04967517778277397
step: 180, loss: 0.10901516675949097
step: 190, loss: 0.13510039448738098
step: 200, loss: 0.26874664425849915
step: 210, loss: 0.32239556312561035
step: 220, loss: 0.16359254717826843
step: 230, loss: 0.05784082040190697
step: 240, loss: 0.07762131839990616
step: 250, loss: 0.5006921887397766
step: 260, loss: 0.2987973093986511
step: 270, loss: 0.22957755625247955
step: 280, loss: 0.13076238334178925
step: 290, loss: 0.0447511300444603
step: 300, loss: 0.13408350944519043
step: 310, loss: 0.07855762541294098
step: 320, loss: 0.17203816771507263
step: 330, loss: 0.1672506034374237
step: 340, loss: 0.06341447681188583
step: 350, loss: 0.4837190508842468
epoch 4: dev_f1=0.8068459657701711, f1=0.7330097087378641, best_f1=0.7523809523809523
step: 0, loss: 0.21569915115833282
step: 10, loss: 0.06612367182970047
step: 20, loss: 0.10598374158143997
step: 30, loss: 0.04554663226008415
step: 40, loss: 0.19784481823444366
step: 50, loss: 0.036472517997026443
step: 60, loss: 0.030959052965044975
step: 70, loss: 0.14888450503349304
step: 80, loss: 0.06301360577344894
step: 90, loss: 0.04206547141075134
step: 100, loss: 0.16570471227169037
step: 110, loss: 0.09611152112483978
step: 120, loss: 0.248539000749588
step: 130, loss: 0.05956777185201645
step: 140, loss: 0.13191582262516022
step: 150, loss: 0.01322441641241312
step: 160, loss: 0.05326024815440178
step: 170, loss: 0.09838183969259262
step: 180, loss: 0.04372894763946533
step: 190, loss: 0.103911392390728
step: 200, loss: 0.04158131033182144
step: 210, loss: 0.021500559523701668
step: 220, loss: 0.21182499825954437
step: 230, loss: 0.22551648318767548
step: 240, loss: 0.12797516584396362
step: 250, loss: 0.11944840848445892
step: 260, loss: 0.14839835464954376
step: 270, loss: 0.18801483511924744
step: 280, loss: 0.07595643401145935
step: 290, loss: 0.04697384685277939
step: 300, loss: 0.03256480023264885
step: 310, loss: 0.10304059833288193
step: 320, loss: 0.05219350755214691
step: 330, loss: 0.10589787364006042
step: 340, loss: 0.35287269949913025
step: 350, loss: 0.15656256675720215
epoch 5: dev_f1=0.7945823927765238, f1=0.7247706422018347, best_f1=0.7523809523809523
step: 0, loss: 0.22646792232990265
step: 10, loss: 0.06108948215842247
step: 20, loss: 0.076789990067482
step: 30, loss: 0.24908152222633362
step: 40, loss: 0.033571697771549225
step: 50, loss: 0.08546801656484604
step: 60, loss: 0.039688173681497574
step: 70, loss: 0.00785254780203104
step: 80, loss: 0.23638373613357544
step: 90, loss: 0.08997257053852081
step: 100, loss: 0.03299062326550484
step: 110, loss: 0.14689628779888153
step: 120, loss: 0.12635935842990875
step: 130, loss: 0.031105762347579002
step: 140, loss: 0.042240604758262634
step: 150, loss: 0.04098622128367424
step: 160, loss: 0.09969954192638397
step: 170, loss: 0.06914947926998138
step: 180, loss: 0.030499614775180817
step: 190, loss: 0.11658160388469696
step: 200, loss: 0.02555396780371666
step: 210, loss: 0.03491566702723503
step: 220, loss: 0.11401154100894928
step: 230, loss: 0.015899181365966797
step: 240, loss: 0.05308300256729126
step: 250, loss: 0.021371079608798027
step: 260, loss: 0.15722672641277313
step: 270, loss: 0.2449621856212616
step: 280, loss: 0.11069219559431076
step: 290, loss: 0.11730899661779404
step: 300, loss: 0.09000080823898315
step: 310, loss: 0.05489494279026985
step: 320, loss: 0.017910564318299294
step: 330, loss: 0.013536546379327774
step: 340, loss: 0.19977842271327972
step: 350, loss: 0.17428483068943024
epoch 6: dev_f1=0.8, f1=0.7186147186147185, best_f1=0.7523809523809523
step: 0, loss: 0.018438179045915604
step: 10, loss: 0.0512487068772316
step: 20, loss: 0.03269313648343086
step: 30, loss: 0.1542806178331375
step: 40, loss: 0.057236820459365845
step: 50, loss: 0.13895893096923828
step: 60, loss: 0.04664924740791321
step: 70, loss: 0.3563847541809082
step: 80, loss: 0.07854766398668289
step: 90, loss: 0.03085630014538765
step: 100, loss: 0.03286602348089218
step: 110, loss: 0.014065759256482124
step: 120, loss: 0.010302480310201645
step: 130, loss: 0.0024654679000377655
step: 140, loss: 0.17404548823833466
step: 150, loss: 0.09985984116792679
step: 160, loss: 0.051592860370874405
step: 170, loss: 0.21356002986431122
step: 180, loss: 0.024934012442827225
step: 190, loss: 0.016445422545075417
step: 200, loss: 0.06041855365037918
step: 210, loss: 0.1161501333117485
step: 220, loss: 0.03918306902050972
step: 230, loss: 0.024763789027929306
step: 240, loss: 0.03624552860856056
step: 250, loss: 0.018711471930146217
step: 260, loss: 0.03545530140399933
step: 270, loss: 0.04832964017987251
step: 280, loss: 0.016873836517333984
step: 290, loss: 0.06530662626028061
step: 300, loss: 0.170106440782547
step: 310, loss: 0.036188069730997086
step: 320, loss: 0.07585459202528
step: 330, loss: 0.22244761884212494
step: 340, loss: 0.010450635105371475
step: 350, loss: 0.0671539157629013
epoch 7: dev_f1=0.8177339901477833, f1=0.7518796992481204, best_f1=0.7518796992481204
step: 0, loss: 0.010520989075303078
step: 10, loss: 0.007815993390977383
step: 20, loss: 0.0397484265267849
step: 30, loss: 0.011547534726560116
step: 40, loss: 0.006683314684778452
step: 50, loss: 0.09644567221403122
step: 60, loss: 0.051405396312475204
step: 70, loss: 0.06228256970643997
step: 80, loss: 0.026451002806425095
step: 90, loss: 0.0521685928106308
step: 100, loss: 0.03599768504500389
step: 110, loss: 0.02035609446465969
step: 120, loss: 0.01364236418157816
step: 130, loss: 0.01093829981982708
step: 140, loss: 0.07857067883014679
step: 150, loss: 0.018546681851148605
step: 160, loss: 0.0633956640958786
step: 170, loss: 0.014027065597474575
step: 180, loss: 0.025931596755981445
step: 190, loss: 0.003830190747976303
step: 200, loss: 0.015051273629069328
step: 210, loss: 0.007917304523289204
step: 220, loss: 0.05899167060852051
step: 230, loss: 0.003188669914379716
step: 240, loss: 0.02306271158158779
step: 250, loss: 0.014354894869029522
step: 260, loss: 0.08845465630292892
step: 270, loss: 0.05730684846639633
step: 280, loss: 0.07074595242738724
step: 290, loss: 0.16914288699626923
step: 300, loss: 0.015176374465227127
step: 310, loss: 0.08361217379570007
step: 320, loss: 0.027311991900205612
step: 330, loss: 0.007117840461432934
step: 340, loss: 0.008418506942689419
step: 350, loss: 0.005635866895318031
epoch 8: dev_f1=0.8144578313253013, f1=0.7209876543209877, best_f1=0.7518796992481204
step: 0, loss: 0.01115911453962326
step: 10, loss: 0.0075868768617510796
step: 20, loss: 0.14734455943107605
step: 30, loss: 0.09111165255308151
step: 40, loss: 0.006090753246098757
step: 50, loss: 0.002563794841989875
step: 60, loss: 0.22323189675807953
step: 70, loss: 0.09478277713060379
step: 80, loss: 0.09207996726036072
step: 90, loss: 0.012003741227090359
step: 100, loss: 0.003400689922273159
step: 110, loss: 0.002923171268776059
step: 120, loss: 0.005747981369495392
step: 130, loss: 0.016144108027219772
step: 140, loss: 0.025678087025880814
step: 150, loss: 0.1575116217136383
step: 160, loss: 0.019760696217417717
step: 170, loss: 0.1353358030319214
step: 180, loss: 0.06141103059053421
step: 190, loss: 0.00633232481777668
step: 200, loss: 0.00775882275775075
step: 210, loss: 0.00968929659575224
step: 220, loss: 0.1264801174402237
step: 230, loss: 0.0264467503875494
step: 240, loss: 0.003919128328561783
step: 250, loss: 0.05621740594506264
step: 260, loss: 0.02155054733157158
step: 270, loss: 0.01495983637869358
step: 280, loss: 0.06473350524902344
step: 290, loss: 0.003626743331551552
step: 300, loss: 0.09457903355360031
step: 310, loss: 0.1106996238231659
step: 320, loss: 0.007878202013671398
step: 330, loss: 0.04012160748243332
step: 340, loss: 0.16190403699874878
step: 350, loss: 0.04428455978631973
epoch 9: dev_f1=0.8167053364269141, f1=0.7597254004576659, best_f1=0.7518796992481204
step: 0, loss: 0.023459210991859436
step: 10, loss: 0.0006873348029330373
step: 20, loss: 0.010785996913909912
step: 30, loss: 0.06898079812526703
step: 40, loss: 0.03769070282578468
step: 50, loss: 0.0032265528570860624
step: 60, loss: 0.02976219914853573
step: 70, loss: 0.015230582095682621
step: 80, loss: 0.005081138573586941
step: 90, loss: 0.1509982943534851
step: 100, loss: 0.012276163324713707
step: 110, loss: 0.01925245299935341
step: 120, loss: 0.006110416725277901
step: 130, loss: 0.015873484313488007
step: 140, loss: 0.04176316037774086
step: 150, loss: 0.0017868091817945242
step: 160, loss: 0.0468539297580719
step: 170, loss: 0.05370475351810455
step: 180, loss: 0.009241299703717232
step: 190, loss: 0.02165267989039421
step: 200, loss: 0.0051927100867033005
step: 210, loss: 0.10557074844837189
step: 220, loss: 0.11991460621356964
step: 230, loss: 0.016868794336915016
step: 240, loss: 0.03190537169575691
step: 250, loss: 0.009642266668379307
step: 260, loss: 0.012312850914895535
step: 270, loss: 0.005856666713953018
step: 280, loss: 0.23451648652553558
step: 290, loss: 0.06005588918924332
step: 300, loss: 0.012817870825529099
step: 310, loss: 0.10768595337867737
step: 320, loss: 0.0007389257661998272
step: 330, loss: 0.10288367420434952
step: 340, loss: 0.051836561411619186
step: 350, loss: 0.0030813466291874647
epoch 10: dev_f1=0.7943262411347518, f1=0.6900000000000001, best_f1=0.7518796992481204
step: 0, loss: 0.01751859113574028
step: 10, loss: 0.00035305097117088735
step: 20, loss: 0.03282175958156586
step: 30, loss: 0.004336007405072451
step: 40, loss: 0.02486264333128929
step: 50, loss: 0.018939759582281113
step: 60, loss: 0.020267995074391365
step: 70, loss: 0.014989000745117664
step: 80, loss: 0.07777082175016403
step: 90, loss: 0.08480779826641083
step: 100, loss: 0.11194662749767303
step: 110, loss: 0.0037539831828325987
step: 120, loss: 0.030106402933597565
step: 130, loss: 0.011747349984943867
step: 140, loss: 0.010001400485634804
step: 150, loss: 0.008303589187562466
step: 160, loss: 0.009103953838348389
step: 170, loss: 0.0051714652217924595
step: 180, loss: 0.007874845527112484
step: 190, loss: 0.006150715984404087
step: 200, loss: 0.004154492635279894
step: 210, loss: 0.002066393382847309
step: 220, loss: 0.0018201712518930435
step: 230, loss: 0.10826310515403748
step: 240, loss: 0.037183064967393875
step: 250, loss: 0.006977972108870745
step: 260, loss: 0.009131229482591152
step: 270, loss: 0.00154089683201164
step: 280, loss: 0.009696397930383682
step: 290, loss: 0.0038156118243932724
step: 300, loss: 0.017705567181110382
step: 310, loss: 0.0070004588924348354
step: 320, loss: 0.04997028410434723
step: 330, loss: 0.0023965525906533003
step: 340, loss: 0.000694107438903302
step: 350, loss: 0.02802230603992939
epoch 11: dev_f1=0.8047058823529412, f1=0.7156862745098039, best_f1=0.7518796992481204
step: 0, loss: 0.0005517132813110948
step: 10, loss: 0.006568999495357275
step: 20, loss: 0.05897284671664238
step: 30, loss: 0.028034919872879982
step: 40, loss: 0.0010547293350100517
step: 50, loss: 0.0037977665197104216
step: 60, loss: 0.004000658635050058
step: 70, loss: 0.0011676701251417398
step: 80, loss: 0.0032628101762384176
step: 90, loss: 0.0378902330994606
step: 100, loss: 0.0004896938917227089
step: 110, loss: 0.005034084897488356
step: 120, loss: 0.0007695039384998381
step: 130, loss: 0.006916113197803497
step: 140, loss: 0.002482015872374177
step: 150, loss: 0.1306503266096115
step: 160, loss: 0.0013007271336391568
step: 170, loss: 0.00910855084657669
step: 180, loss: 0.01492819283157587
step: 190, loss: 0.06056941673159599
step: 200, loss: 0.015970861539244652
step: 210, loss: 0.0012934855185449123
step: 220, loss: 0.0006380925769917667
step: 230, loss: 0.021568207070231438
step: 240, loss: 0.00502494303509593
step: 250, loss: 0.02394995093345642
step: 260, loss: 0.06682544201612473
step: 270, loss: 0.004275089129805565
step: 280, loss: 0.0028279360849410295
step: 290, loss: 0.004565752577036619
step: 300, loss: 0.004189825151115656
step: 310, loss: 0.02247929573059082
step: 320, loss: 0.0037219575606286526
step: 330, loss: 0.017409956082701683
step: 340, loss: 0.005138936452567577
step: 350, loss: 0.0075295851565897465
epoch 12: dev_f1=0.7951807228915662, f1=0.7427184466019418, best_f1=0.7518796992481204
step: 0, loss: 0.12308710068464279
step: 10, loss: 0.0013039535842835903
step: 20, loss: 0.0585922971367836
step: 30, loss: 0.006737923715263605
step: 40, loss: 0.0008279525209218264
step: 50, loss: 0.0018461821600794792
step: 60, loss: 0.004803727380931377
step: 70, loss: 0.0024479904677718878
step: 80, loss: 0.0004024071095045656
step: 90, loss: 0.003237698459997773
step: 100, loss: 0.21256716549396515
step: 110, loss: 0.007987919263541698
step: 120, loss: 0.0069422549568116665
step: 130, loss: 0.0010256441310048103
step: 140, loss: 0.011911844834685326
step: 150, loss: 0.0004820148169528693
step: 160, loss: 0.00024458442931063473
step: 170, loss: 0.0015118296723812819
step: 180, loss: 0.04100712761282921
step: 190, loss: 0.0006588177056983113
step: 200, loss: 0.0006964667118154466
step: 210, loss: 0.0023964063730090857
step: 220, loss: 0.0018979841843247414
step: 230, loss: 0.02987472154200077
step: 240, loss: 0.01508180983364582
step: 250, loss: 0.002591290045529604
step: 260, loss: 0.00028432279941625893
step: 270, loss: 0.017566030845046043
step: 280, loss: 0.07106595486402512
step: 290, loss: 0.0006461609154939651
step: 300, loss: 0.005645231809467077
step: 310, loss: 0.05236918106675148
step: 320, loss: 0.03260578587651253
step: 330, loss: 0.0004987969296053052
step: 340, loss: 0.046518828719854355
step: 350, loss: 0.004708587657660246
epoch 13: dev_f1=0.7971698113207547, f1=0.7259615384615385, best_f1=0.7518796992481204
step: 0, loss: 0.009883429855108261
step: 10, loss: 0.009843467734754086
step: 20, loss: 0.00869175884872675
step: 30, loss: 0.033805642277002335
step: 40, loss: 0.0035249940119683743
step: 50, loss: 0.0005396336782723665
step: 60, loss: 0.0018544517224654555
step: 70, loss: 0.015070537105202675
step: 80, loss: 0.005846526939421892
step: 90, loss: 0.006792232394218445
step: 100, loss: 0.004640024155378342
step: 110, loss: 0.06459756195545197
step: 120, loss: 0.0048196702264249325
step: 130, loss: 0.0028057952877134085
step: 140, loss: 0.0009906127816066146
step: 150, loss: 0.006120647769421339
step: 160, loss: 0.007475330028682947
step: 170, loss: 0.0008784793899394572
step: 180, loss: 0.03009098395705223
step: 190, loss: 0.0064426143653690815
step: 200, loss: 0.041014835238456726
step: 210, loss: 0.0003779687685891986
step: 220, loss: 0.0625745877623558
step: 230, loss: 0.001470040064305067
step: 240, loss: 0.005391442216932774
step: 250, loss: 0.02407699078321457
step: 260, loss: 0.0003731194883584976
step: 270, loss: 0.0010558402864262462
step: 280, loss: 0.0002585551410447806
step: 290, loss: 0.032919250428676605
step: 300, loss: 0.0001940306683536619
step: 310, loss: 0.007989396341145039
step: 320, loss: 0.006074330769479275
step: 330, loss: 0.0014374649617820978
step: 340, loss: 0.060566533356904984
step: 350, loss: 0.26061421632766724
epoch 14: dev_f1=0.8020833333333334, f1=0.6904109589041095, best_f1=0.7518796992481204
step: 0, loss: 0.02648397721350193
step: 10, loss: 0.005007259082049131
step: 20, loss: 0.007713875733315945
step: 30, loss: 0.00764517392963171
step: 40, loss: 0.0016464674845337868
step: 50, loss: 0.17169514298439026
step: 60, loss: 0.0013254357036203146
step: 70, loss: 0.0007551368908025324
step: 80, loss: 0.0002781578223221004
step: 90, loss: 0.0007911575376056135
step: 100, loss: 0.0005888057639822364
step: 110, loss: 0.004104532767087221
step: 120, loss: 0.0019855210557579994
step: 130, loss: 0.03813859447836876
step: 140, loss: 0.03249203786253929
step: 150, loss: 0.0009725505369715393
step: 160, loss: 0.0024592222180217505
step: 170, loss: 0.0003349832841195166
step: 180, loss: 0.0019441009499132633
step: 190, loss: 0.0003054136468563229
step: 200, loss: 0.0005414095940068364
step: 210, loss: 0.036413855850696564
step: 220, loss: 0.0005313173751346767
step: 230, loss: 0.0006863056332804263
step: 240, loss: 0.007955063134431839
step: 250, loss: 0.005234390031546354
step: 260, loss: 0.0004354393167886883
step: 270, loss: 0.0014647747157141566
step: 280, loss: 0.005648239050060511
step: 290, loss: 0.00039576514973305166
step: 300, loss: 0.012708266265690327
step: 310, loss: 0.0054466319270431995
step: 320, loss: 0.08437822014093399
step: 330, loss: 0.0004076759214513004
step: 340, loss: 0.0026067178696393967
step: 350, loss: 0.00034696373040787876
epoch 15: dev_f1=0.8093994778067886, f1=0.7340425531914895, best_f1=0.7518796992481204
step: 0, loss: 0.05858490616083145
step: 10, loss: 0.002186623401939869
step: 20, loss: 0.027710502967238426
step: 30, loss: 0.0005512050120159984
step: 40, loss: 0.027150655165314674
step: 50, loss: 0.047453682869672775
step: 60, loss: 0.0004067871777806431
step: 70, loss: 0.004440756049007177
step: 80, loss: 0.0006568359094671905
step: 90, loss: 0.02708061970770359
step: 100, loss: 0.000441005831817165
step: 110, loss: 0.029224392026662827
step: 120, loss: 0.0019476722227409482
step: 130, loss: 0.024920497089624405
step: 140, loss: 0.0010312717640772462
step: 150, loss: 0.034488845616579056
step: 160, loss: 0.002356486627832055
step: 170, loss: 0.08645717799663544
step: 180, loss: 0.0012895251857116818
step: 190, loss: 0.0005258008022792637
step: 200, loss: 0.0004768095968756825
step: 210, loss: 0.003667372278869152
step: 220, loss: 0.001123664085753262
step: 230, loss: 0.0081108333542943
step: 240, loss: 0.02829878218472004
step: 250, loss: 0.00018067809287458658
step: 260, loss: 0.004230951424688101
step: 270, loss: 0.000861068838275969
step: 280, loss: 0.02400638535618782
step: 290, loss: 0.0003849947825074196
step: 300, loss: 0.10533968359231949
step: 310, loss: 0.00030013814102858305
step: 320, loss: 0.005208791699260473
step: 330, loss: 0.0008406265405938029
step: 340, loss: 0.0008006434654816985
step: 350, loss: 0.010679667815566063
epoch 16: dev_f1=0.8054054054054054, f1=0.6837606837606837, best_f1=0.7518796992481204
step: 0, loss: 0.036770690232515335
step: 10, loss: 0.03034367971122265
step: 20, loss: 0.0005326657555997372
step: 30, loss: 0.0016140162479132414
step: 40, loss: 0.011006269603967667
step: 50, loss: 0.0013021756894886494
step: 60, loss: 0.0003202029620297253
step: 70, loss: 0.0008628912619315088
step: 80, loss: 0.001358287874609232
step: 90, loss: 0.0004595576610881835
step: 100, loss: 0.06518924236297607
step: 110, loss: 0.0007960230577737093
step: 120, loss: 0.019658468663692474
step: 130, loss: 0.017214402556419373
step: 140, loss: 0.0003626359102781862
step: 150, loss: 0.0009072340908460319
step: 160, loss: 0.0009742534020915627
step: 170, loss: 0.005837869364768267
step: 180, loss: 0.0005858548684045672
step: 190, loss: 0.04230405017733574
step: 200, loss: 0.0004880558990407735
step: 210, loss: 0.006290188059210777
step: 220, loss: 0.0018455538665875793
step: 230, loss: 0.0004915709141641855
step: 240, loss: 0.005822598468512297
step: 250, loss: 0.0007134738843888044
step: 260, loss: 0.0007049568230286241
step: 270, loss: 0.00018138022278435528
step: 280, loss: 0.0017904378473758698
step: 290, loss: 0.00014558267139364034
step: 300, loss: 0.0009278859361074865
step: 310, loss: 0.0001413982972735539
step: 320, loss: 0.00018759512749966234
step: 330, loss: 0.00029984107823111117
step: 340, loss: 0.005898760166019201
step: 350, loss: 0.0005059611285105348
epoch 17: dev_f1=0.8064516129032259, f1=0.6985915492957746, best_f1=0.7518796992481204
step: 0, loss: 0.023041406646370888
step: 10, loss: 0.01688339188694954
step: 20, loss: 0.03835924342274666
step: 30, loss: 0.0011566674802452326
step: 40, loss: 0.0011448789155110717
step: 50, loss: 0.005558806471526623
step: 60, loss: 0.0010599971283227205
step: 70, loss: 0.0015260929940268397
step: 80, loss: 0.0009011725196614861
step: 90, loss: 0.006461287382990122
step: 100, loss: 0.00021575693972408772
step: 110, loss: 0.005918353330343962
step: 120, loss: 0.017643915489315987
step: 130, loss: 0.0009126077638939023
step: 140, loss: 0.001114357146434486
step: 150, loss: 0.0007726854528300464
step: 160, loss: 0.00021874855156056583
step: 170, loss: 0.00010785698395920917
step: 180, loss: 0.004072947893291712
step: 190, loss: 0.0005681885522790253
step: 200, loss: 0.00010446876694913954
step: 210, loss: 0.0007076578331179917
step: 220, loss: 0.025708163157105446
step: 230, loss: 0.0010313245002180338
step: 240, loss: 0.0012092383112758398
step: 250, loss: 0.0003801490820478648
step: 260, loss: 0.0001550965680507943
step: 270, loss: 0.0003817625402007252
step: 280, loss: 0.0024228335823863745
step: 290, loss: 0.009136534295976162
step: 300, loss: 0.0015045286854729056
step: 310, loss: 0.0002863522677216679
step: 320, loss: 0.00014066830044612288
step: 330, loss: 0.000525694980751723
step: 340, loss: 0.0031417179852724075
step: 350, loss: 0.00022785551846027374
epoch 18: dev_f1=0.8031914893617021, f1=0.6942148760330578, best_f1=0.7518796992481204
step: 0, loss: 0.028701776638627052
step: 10, loss: 0.007743710186332464
step: 20, loss: 0.0004962142556905746
step: 30, loss: 0.00021280728105921298
step: 40, loss: 0.0002599367580842227
step: 50, loss: 0.0025688305031508207
step: 60, loss: 0.00036120094591751695
step: 70, loss: 0.003083344316110015
step: 80, loss: 0.00023881238303147256
step: 90, loss: 0.0056335944682359695
step: 100, loss: 0.00039533324888907373
step: 110, loss: 0.002820539055392146
step: 120, loss: 0.2575967609882355
step: 130, loss: 0.0002985925239045173
step: 140, loss: 0.00010363643377786502
step: 150, loss: 0.00019266399613115937
step: 160, loss: 0.00561172841116786
step: 170, loss: 0.00011787538096541539
step: 180, loss: 0.007128097582608461
step: 190, loss: 0.00020482332911342382
step: 200, loss: 0.09006336331367493
step: 210, loss: 0.00036593026015907526
step: 220, loss: 0.0036815397907048464
step: 230, loss: 0.0004626662703230977
step: 240, loss: 0.04884941503405571
step: 250, loss: 0.0002858057268895209
step: 260, loss: 0.00033442737185396254
step: 270, loss: 0.00045028841122984886
step: 280, loss: 0.006944684311747551
step: 290, loss: 0.00047726923367008567
step: 300, loss: 0.0006871447549201548
step: 310, loss: 0.037084128707647324
step: 320, loss: 0.0014890743186697364
step: 330, loss: 0.0003583329380489886
step: 340, loss: 0.022596942260861397
step: 350, loss: 0.0020705205388367176
epoch 19: dev_f1=0.8, f1=0.6885245901639344, best_f1=0.7518796992481204
step: 0, loss: 0.00029113725759088993
step: 10, loss: 0.00011239112791372463
step: 20, loss: 0.006930368021130562
step: 30, loss: 0.00012047566997352988
step: 40, loss: 0.00010642581037245691
step: 50, loss: 0.06647145748138428
step: 60, loss: 0.00021184732031542808
step: 70, loss: 0.0020472430624067783
step: 80, loss: 0.0001276348193641752
step: 90, loss: 0.00045796119957230985
step: 100, loss: 0.0022468341048806906
step: 110, loss: 0.005258108023554087
step: 120, loss: 0.00014919544628355652
step: 130, loss: 0.0005472733173519373
step: 140, loss: 0.0014297663001343608
step: 150, loss: 7.241533603519201e-05
step: 160, loss: 0.0002887355221901089
step: 170, loss: 0.01796467788517475
step: 180, loss: 0.055312518030405045
step: 190, loss: 0.000116422597784549
step: 200, loss: 0.00021476818074006587
step: 210, loss: 0.00015798884851392359
step: 220, loss: 0.008901440538465977
step: 230, loss: 0.00018695516337174922
step: 240, loss: 0.00030254709417931736
step: 250, loss: 0.000181123788934201
step: 260, loss: 0.0005097365356050432
step: 270, loss: 9.779613901628181e-05
step: 280, loss: 0.016997719183564186
step: 290, loss: 0.0214840117841959
step: 300, loss: 0.0039707450196146965
step: 310, loss: 0.0001982107205549255
step: 320, loss: 0.0002666544169187546
step: 330, loss: 0.0002532869693823159
step: 340, loss: 0.18594293296337128
step: 350, loss: 0.002555375685915351
epoch 20: dev_f1=0.8052631578947368, f1=0.7100271002710027, best_f1=0.7518796992481204
