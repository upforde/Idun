cuda
Device: cuda
step: 0, loss: 0.7709959149360657
step: 10, loss: 0.44772520661354065
step: 20, loss: 0.24115300178527832
step: 30, loss: 0.3677467107772827
step: 40, loss: 0.21421144902706146
step: 50, loss: 0.2999306619167328
step: 60, loss: 0.43027931451797485
step: 70, loss: 0.05556177347898483
step: 80, loss: 0.15187834203243256
step: 90, loss: 0.1355813443660736
step: 100, loss: 0.0931391790509224
step: 110, loss: 0.23463115096092224
step: 120, loss: 0.2573886811733246
step: 130, loss: 0.132315531373024
step: 140, loss: 0.14980703592300415
step: 150, loss: 0.2048337161540985
step: 160, loss: 0.12130237370729446
step: 170, loss: 0.11520132422447205
step: 180, loss: 0.04237300902605057
step: 190, loss: 0.3216536343097687
step: 200, loss: 0.1581648886203766
step: 210, loss: 0.16386516392230988
step: 220, loss: 0.08005071431398392
step: 230, loss: 0.0634603500366211
step: 240, loss: 0.15748059749603271
step: 250, loss: 0.1523931324481964
step: 260, loss: 0.15507087111473083
step: 270, loss: 0.1912515014410019
step: 280, loss: 0.08731983602046967
step: 290, loss: 0.28755849599838257
step: 300, loss: 0.11706091463565826
step: 310, loss: 0.03371825069189072
step: 320, loss: 0.22043099999427795
step: 330, loss: 0.05760708823800087
step: 340, loss: 0.15504102408885956
step: 350, loss: 0.11759038269519806
step: 360, loss: 0.2962178587913513
step: 370, loss: 0.0803082212805748
step: 380, loss: 0.1439722329378128
step: 390, loss: 0.13663411140441895
step: 400, loss: 0.12593913078308105
step: 410, loss: 0.04832348972558975
step: 420, loss: 0.07118073850870132
step: 430, loss: 0.02152767963707447
step: 440, loss: 0.19230207800865173
step: 450, loss: 0.08256031572818756
step: 460, loss: 0.06428886950016022
step: 470, loss: 0.06300228089094162
step: 480, loss: 0.08010760694742203
step: 490, loss: 0.06220334768295288
step: 500, loss: 0.11945484578609467
step: 510, loss: 0.09244275838136673
step: 520, loss: 0.09384911507368088
step: 530, loss: 0.08117686212062836
step: 540, loss: 0.18468642234802246
step: 550, loss: 0.008041828870773315
step: 560, loss: 0.04984099045395851
step: 570, loss: 0.1240217313170433
step: 580, loss: 0.14143122732639313
step: 590, loss: 0.21414604783058167
step: 600, loss: 0.09038087725639343
step: 610, loss: 0.12217174470424652
step: 620, loss: 0.0756777673959732
step: 630, loss: 0.06074045971035957
step: 640, loss: 0.05253615230321884
step: 650, loss: 0.11043716967105865
step: 660, loss: 0.1428225338459015
step: 670, loss: 0.12165378034114838
step: 680, loss: 0.018120460212230682
step: 690, loss: 0.22668865323066711
step: 700, loss: 0.11583419889211655
step: 710, loss: 0.12575401365756989
step: 720, loss: 0.15725617110729218
step: 730, loss: 0.10432659834623337
step: 740, loss: 0.19806386530399323
step: 750, loss: 0.06635374575853348
step: 760, loss: 0.21593721210956573
step: 770, loss: 0.08766717463731766
step: 780, loss: 0.03532075509428978
step: 790, loss: 0.1591678112745285
step: 800, loss: 0.13628502190113068
step: 810, loss: 0.0990312397480011
step: 820, loss: 0.07416953146457672
step: 830, loss: 0.06830886006355286
step: 840, loss: 0.06887633353471756
step: 850, loss: 0.03275778517127037
step: 860, loss: 0.1591411978006363
step: 870, loss: 0.1307610273361206
step: 880, loss: 0.1031113713979721
step: 890, loss: 0.2574593424797058
step: 900, loss: 0.13269029557704926
step: 910, loss: 0.09114890545606613
step: 920, loss: 0.12039295583963394
step: 930, loss: 0.06814507395029068
step: 940, loss: 0.16160422563552856
step: 950, loss: 0.11373722553253174
step: 960, loss: 0.07901892066001892
step: 970, loss: 0.08708392083644867
epoch 1: dev_f1=0.9257246376811593, f1=0.9329131022062134, best_f1=0.9329131022062134
step: 0, loss: 0.17383147776126862
step: 10, loss: 0.11542483419179916
step: 20, loss: 0.12941111624240875
step: 30, loss: 0.03654047101736069
step: 40, loss: 0.10980311036109924
step: 50, loss: 0.21970976889133453
step: 60, loss: 0.0702669620513916
step: 70, loss: 0.0931057333946228
step: 80, loss: 0.04846216365695
step: 90, loss: 0.09586300700902939
step: 100, loss: 0.13653841614723206
step: 110, loss: 0.10361559689044952
step: 120, loss: 0.12204737216234207
step: 130, loss: 0.01813771016895771
step: 140, loss: 0.08974851667881012
step: 150, loss: 0.03233056887984276
step: 160, loss: 0.008861863054335117
step: 170, loss: 0.0699036568403244
step: 180, loss: 0.14187495410442352
step: 190, loss: 0.14093874394893646
step: 200, loss: 0.1580379456281662
step: 210, loss: 0.10050228238105774
step: 220, loss: 0.08579418808221817
step: 230, loss: 0.03684627264738083
step: 240, loss: 0.05733606219291687
step: 250, loss: 0.1826906055212021
step: 260, loss: 0.16271908581256866
step: 270, loss: 0.06361167132854462
step: 280, loss: 0.11913110315799713
step: 290, loss: 0.13489557802677155
step: 300, loss: 0.11689205467700958
step: 310, loss: 0.09480631351470947
step: 320, loss: 0.05556514859199524
step: 330, loss: 0.04466799274086952
step: 340, loss: 0.021917790174484253
step: 350, loss: 0.052181676030159
step: 360, loss: 0.19912821054458618
step: 370, loss: 0.12747925519943237
step: 380, loss: 0.1089712381362915
step: 390, loss: 0.07390932738780975
step: 400, loss: 0.06931393593549728
step: 410, loss: 0.10379715263843536
step: 420, loss: 0.13814717531204224
step: 430, loss: 0.02942044287919998
step: 440, loss: 0.3629900813102722
step: 450, loss: 0.1356162279844284
step: 460, loss: 0.0959630087018013
step: 470, loss: 0.09937380254268646
step: 480, loss: 0.06754808127880096
step: 490, loss: 0.06998886913061142
step: 500, loss: 0.04647462069988251
step: 510, loss: 0.13274268805980682
step: 520, loss: 0.2384481281042099
step: 530, loss: 0.20329232513904572
step: 540, loss: 0.08369306474924088
step: 550, loss: 0.07887166738510132
step: 560, loss: 0.02370007336139679
step: 570, loss: 0.18279288709163666
step: 580, loss: 0.051328934729099274
step: 590, loss: 0.18263286352157593
step: 600, loss: 0.049238696694374084
step: 610, loss: 0.1594146192073822
step: 620, loss: 0.07367891073226929
step: 630, loss: 0.02394747920334339
step: 640, loss: 0.12437150627374649
step: 650, loss: 0.11041867733001709
step: 660, loss: 0.1391672044992447
step: 670, loss: 0.2665693461894989
step: 680, loss: 0.06790360808372498
step: 690, loss: 0.119059719145298
step: 700, loss: 0.04428194463253021
step: 710, loss: 0.11231862753629684
step: 720, loss: 0.11343266069889069
step: 730, loss: 0.03677695244550705
step: 740, loss: 0.06022439897060394
step: 750, loss: 0.20702198147773743
step: 760, loss: 0.1445542871952057
step: 770, loss: 0.05811762437224388
step: 780, loss: 0.14744718372821808
step: 790, loss: 0.07328100502490997
step: 800, loss: 0.11727805435657501
step: 810, loss: 0.08089904487133026
step: 820, loss: 0.04774443805217743
step: 830, loss: 0.04349317401647568
step: 840, loss: 0.018992768600583076
step: 850, loss: 0.05692821368575096
step: 860, loss: 0.06945542991161346
step: 870, loss: 0.16329072415828705
step: 880, loss: 0.07191168516874313
step: 890, loss: 0.09562919288873672
step: 900, loss: 0.1262531876564026
step: 910, loss: 0.013977937400341034
step: 920, loss: 0.30160173773765564
step: 930, loss: 0.10384536534547806
step: 940, loss: 0.07891296595335007
step: 950, loss: 0.06817834824323654
step: 960, loss: 0.11500810831785202
step: 970, loss: 0.14117836952209473
epoch 2: dev_f1=0.9180790960451978, f1=0.9225746268656716, best_f1=0.9329131022062134
step: 0, loss: 0.1316250115633011
step: 10, loss: 0.11894334852695465
step: 20, loss: 0.05382402613759041
step: 30, loss: 0.0913519412279129
step: 40, loss: 0.010922815650701523
step: 50, loss: 0.08783314377069473
step: 60, loss: 0.025477197021245956
step: 70, loss: 0.09230860322713852
step: 80, loss: 0.043025270104408264
step: 90, loss: 0.2080739587545395
step: 100, loss: 0.11147259920835495
step: 110, loss: 0.10589834302663803
step: 120, loss: 0.12232771515846252
step: 130, loss: 0.11095590144395828
step: 140, loss: 0.0972164124250412
step: 150, loss: 0.04679218679666519
step: 160, loss: 0.11337299644947052
step: 170, loss: 0.08533482253551483
step: 180, loss: 0.11338604241609573
step: 190, loss: 0.023875854909420013
step: 200, loss: 0.17771640419960022
step: 210, loss: 0.06735655665397644
step: 220, loss: 0.15560577809810638
step: 230, loss: 0.1021733283996582
step: 240, loss: 0.11903984099626541
step: 250, loss: 0.09393912553787231
step: 260, loss: 0.04349356144666672
step: 270, loss: 0.07393699139356613
step: 280, loss: 0.06506158411502838
step: 290, loss: 0.07192517071962357
step: 300, loss: 0.10648594051599503
step: 310, loss: 0.12921613454818726
step: 320, loss: 0.05530383437871933
step: 330, loss: 0.07180153578519821
step: 340, loss: 0.04038160294294357
step: 350, loss: 0.0784783884882927
step: 360, loss: 0.11227034777402878
step: 370, loss: 0.09743422269821167
step: 380, loss: 0.06467902660369873
step: 390, loss: 0.20810937881469727
step: 400, loss: 0.08073581755161285
step: 410, loss: 0.02573935128748417
step: 420, loss: 0.0804380401968956
step: 430, loss: 0.16479796171188354
step: 440, loss: 0.023066982626914978
step: 450, loss: 0.10675811022520065
step: 460, loss: 0.15875183045864105
step: 470, loss: 0.01684974506497383
step: 480, loss: 0.05208923667669296
step: 490, loss: 0.029677629470825195
step: 500, loss: 0.2806277871131897
step: 510, loss: 0.0876230001449585
step: 520, loss: 0.0773024633526802
step: 530, loss: 0.013837531208992004
step: 540, loss: 0.10306445509195328
step: 550, loss: 0.08482969552278519
step: 560, loss: 0.1395745724439621
step: 570, loss: 0.09783856570720673
step: 580, loss: 0.11694471538066864
step: 590, loss: 0.08770855516195297
step: 600, loss: 0.08378563821315765
step: 610, loss: 0.04570452496409416
step: 620, loss: 0.1212352067232132
step: 630, loss: 0.11873777955770493
step: 640, loss: 0.10177444666624069
step: 650, loss: 0.13359875977039337
step: 660, loss: 0.04664641246199608
step: 670, loss: 0.06252243369817734
step: 680, loss: 0.08978226035833359
step: 690, loss: 0.1013026237487793
step: 700, loss: 0.11277510225772858
step: 710, loss: 0.034412067383527756
step: 720, loss: 0.02981400303542614
step: 730, loss: 0.026857048273086548
step: 740, loss: 0.049350250512361526
step: 750, loss: 0.21056661009788513
step: 760, loss: 0.04228276386857033
step: 770, loss: 0.10462332516908646
step: 780, loss: 0.16384606063365936
step: 790, loss: 0.014449207112193108
step: 800, loss: 0.07036860287189484
step: 810, loss: 0.08212462067604065
step: 820, loss: 0.0296302642673254
step: 830, loss: 0.04843426123261452
step: 840, loss: 0.08829651772975922
step: 850, loss: 0.08326977491378784
step: 860, loss: 0.09436525404453278
step: 870, loss: 0.03670479357242584
step: 880, loss: 0.15017659962177277
step: 890, loss: 0.11828139424324036
step: 900, loss: 0.05572626367211342
step: 910, loss: 0.016694316640496254
step: 920, loss: 0.029010452330112457
step: 930, loss: 0.12503287196159363
step: 940, loss: 0.10665035992860794
step: 950, loss: 0.07889503985643387
step: 960, loss: 0.01053447276353836
step: 970, loss: 0.13000719249248505
epoch 3: dev_f1=0.9373549883990719, f1=0.9278445883441258, best_f1=0.9278445883441258
step: 0, loss: 0.022299757227301598
step: 10, loss: 0.13439075648784637
step: 20, loss: 0.09886770695447922
step: 30, loss: 0.0723426565527916
step: 40, loss: 0.0752008929848671
step: 50, loss: 0.09267910569906235
step: 60, loss: 0.027562284842133522
step: 70, loss: 0.0725448951125145
step: 80, loss: 0.13634063303470612
step: 90, loss: 0.039119523018598557
step: 100, loss: 0.06559738516807556
step: 110, loss: 0.13379928469657898
step: 120, loss: 0.02639041282236576
step: 130, loss: 0.08916567265987396
step: 140, loss: 0.09024042636156082
step: 150, loss: 0.06765958666801453
step: 160, loss: 0.04040156677365303
step: 170, loss: 0.0547533743083477
step: 180, loss: 0.08645787835121155
step: 190, loss: 0.0374893955886364
step: 200, loss: 0.032567549496889114
step: 210, loss: 0.08932358771562576
step: 220, loss: 0.04103625565767288
step: 230, loss: 0.022589504718780518
step: 240, loss: 0.07895992696285248
step: 250, loss: 0.07639139145612717
step: 260, loss: 0.09557409584522247
step: 270, loss: 0.0856124609708786
step: 280, loss: 0.018521206453442574
step: 290, loss: 0.0810733363032341
step: 300, loss: 0.11815635859966278
step: 310, loss: 0.08839509636163712
step: 320, loss: 0.0885879173874855
step: 330, loss: 0.06227394565939903
step: 340, loss: 0.046101197600364685
step: 350, loss: 0.0957677811384201
step: 360, loss: 0.10693176835775375
step: 370, loss: 0.04455669969320297
step: 380, loss: 0.03191595897078514
step: 390, loss: 0.030762288719415665
step: 400, loss: 0.18064077198505402
step: 410, loss: 0.02898082695901394
step: 420, loss: 0.034310635179281235
step: 430, loss: 0.07787562906742096
step: 440, loss: 0.11041933298110962
step: 450, loss: 0.13472050428390503
step: 460, loss: 0.18678000569343567
step: 470, loss: 0.022965431213378906
step: 480, loss: 0.04075589403510094
step: 490, loss: 0.037658870220184326
step: 500, loss: 0.1968182772397995
step: 510, loss: 0.052907418459653854
step: 520, loss: 0.11879177391529083
step: 530, loss: 0.06344729661941528
step: 540, loss: 0.1585891991853714
step: 550, loss: 0.03898050636053085
step: 560, loss: 0.08074341714382172
step: 570, loss: 0.16505327820777893
step: 580, loss: 0.026023002341389656
step: 590, loss: 0.11857793480157852
step: 600, loss: 0.09620814025402069
step: 610, loss: 0.29063498973846436
step: 620, loss: 0.13952083885669708
step: 630, loss: 0.06010771542787552
step: 640, loss: 0.10843679308891296
step: 650, loss: 0.1103014275431633
step: 660, loss: 0.08673706650733948
step: 670, loss: 0.07629873603582382
step: 680, loss: 0.027058543637394905
step: 690, loss: 0.12279888242483139
step: 700, loss: 0.016519945114850998
step: 710, loss: 0.02280082181096077
step: 720, loss: 0.09077586978673935
step: 730, loss: 0.13763557374477386
step: 740, loss: 0.030262107029557228
step: 750, loss: 0.11765363812446594
step: 760, loss: 0.08329298347234726
step: 770, loss: 0.12006749957799911
step: 780, loss: 0.04176001250743866
step: 790, loss: 0.05290694162249565
step: 800, loss: 0.11969403177499771
step: 810, loss: 0.05991600453853607
step: 820, loss: 0.08427272737026215
step: 830, loss: 0.054645806550979614
step: 840, loss: 0.10237856954336166
step: 850, loss: 0.0591784231364727
step: 860, loss: 0.10820871591567993
step: 870, loss: 0.18516646325588226
step: 880, loss: 0.05815829709172249
step: 890, loss: 0.08086798340082169
step: 900, loss: 0.04401726275682449
step: 910, loss: 0.12659986317157745
step: 920, loss: 0.0793747752904892
step: 930, loss: 0.03988382965326309
step: 940, loss: 0.038755156099796295
step: 950, loss: 0.09183185547590256
step: 960, loss: 0.14628852903842926
step: 970, loss: 0.1296595335006714
epoch 4: dev_f1=0.9355742296918768, f1=0.9265116279069768, best_f1=0.9278445883441258
step: 0, loss: 0.1317741721868515
step: 10, loss: 0.09118957817554474
step: 20, loss: 0.05242140218615532
step: 30, loss: 0.01741025596857071
step: 40, loss: 0.06257516145706177
step: 50, loss: 0.027759326621890068
step: 60, loss: 0.012781940400600433
step: 70, loss: 0.11935201287269592
step: 80, loss: 0.09887897223234177
step: 90, loss: 0.08155989646911621
step: 100, loss: 0.051480069756507874
step: 110, loss: 0.06738033890724182
step: 120, loss: 0.12513594329357147
step: 130, loss: 0.01839064434170723
step: 140, loss: 0.054338738322257996
step: 150, loss: 0.12483207136392593
step: 160, loss: 0.12927426397800446
step: 170, loss: 0.09277112782001495
step: 180, loss: 0.09484472870826721
step: 190, loss: 0.06389745324850082
step: 200, loss: 0.0003521931357681751
step: 210, loss: 0.06585505604743958
step: 220, loss: 0.020268455147743225
step: 230, loss: 0.06921402364969254
step: 240, loss: 0.024013325572013855
step: 250, loss: 0.07031195610761642
step: 260, loss: 0.21904033422470093
step: 270, loss: 0.06932125240564346
step: 280, loss: 0.10376176238059998
step: 290, loss: 0.019558042287826538
step: 300, loss: 0.08882410824298859
step: 310, loss: 0.037314604967832565
step: 320, loss: 0.12862390279769897
step: 330, loss: 0.023920487612485886
step: 340, loss: 0.08662621676921844
step: 350, loss: 0.036823567003011703
step: 360, loss: 0.034537721425294876
step: 370, loss: 0.06627857685089111
step: 380, loss: 0.09464526176452637
step: 390, loss: 0.011939864605665207
step: 400, loss: 0.03968074545264244
step: 410, loss: 0.07504042237997055
step: 420, loss: 0.06868939101696014
step: 430, loss: 0.015700262039899826
step: 440, loss: 0.05738038569688797
step: 450, loss: 0.1139293760061264
step: 460, loss: 0.0063148243352770805
step: 470, loss: 0.03002777136862278
step: 480, loss: 0.07009761780500412
step: 490, loss: 0.07334182411432266
step: 500, loss: 0.05486298352479935
step: 510, loss: 0.07414732128381729
step: 520, loss: 0.03869561851024628
step: 530, loss: 0.09042005985975266
step: 540, loss: 0.046230677515268326
step: 550, loss: 0.03320852294564247
step: 560, loss: 0.01138509251177311
step: 570, loss: 0.010879418812692165
step: 580, loss: 0.011038701049983501
step: 590, loss: 0.16697454452514648
step: 600, loss: 0.10051020234823227
step: 610, loss: 0.00011035404168069363
step: 620, loss: 0.03723203018307686
step: 630, loss: 0.05832650139927864
step: 640, loss: 0.09936035424470901
step: 650, loss: 0.09840118885040283
step: 660, loss: 0.020285170525312424
step: 670, loss: 0.018732912838459015
step: 680, loss: 0.11775269359350204
step: 690, loss: 0.021014269441366196
step: 700, loss: 0.07013379782438278
step: 710, loss: 0.057496894150972366
step: 720, loss: 0.04062319174408913
step: 730, loss: 0.01737590692937374
step: 740, loss: 0.0825294703245163
step: 750, loss: 0.01594308763742447
step: 760, loss: 0.026338646188378334
step: 770, loss: 0.04787275195121765
step: 780, loss: 0.10079523921012878
step: 790, loss: 0.03649339824914932
step: 800, loss: 0.08561370521783829
step: 810, loss: 0.06656687706708908
step: 820, loss: 0.06529264897108078
step: 830, loss: 0.02091163583099842
step: 840, loss: 0.003971762489527464
step: 850, loss: 0.06635498255491257
step: 860, loss: 0.007879414595663548
step: 870, loss: 0.11029377579689026
step: 880, loss: 0.04957454279065132
step: 890, loss: 0.03654250502586365
step: 900, loss: 0.0904906764626503
step: 910, loss: 0.07957202941179276
step: 920, loss: 0.14469127357006073
step: 930, loss: 0.06097991019487381
step: 940, loss: 0.12326055020093918
step: 950, loss: 0.019360270351171494
step: 960, loss: 0.020322108641266823
step: 970, loss: 0.08781427890062332
epoch 5: dev_f1=0.9341983317886932, f1=0.9338235294117647, best_f1=0.9278445883441258
step: 0, loss: 0.07975990325212479
step: 10, loss: 0.04288671538233757
step: 20, loss: 0.09581372141838074
step: 30, loss: 0.14316821098327637
step: 40, loss: 0.07674820721149445
step: 50, loss: 0.09591346234083176
step: 60, loss: 0.02246309071779251
step: 70, loss: 0.12810538709163666
step: 80, loss: 0.04870578646659851
step: 90, loss: 0.12824901938438416
step: 100, loss: 0.10188405215740204
step: 110, loss: 0.06247442588210106
step: 120, loss: 0.08698447793722153
step: 130, loss: 0.006366242654621601
step: 140, loss: 0.011689175851643085
step: 150, loss: 0.03875546529889107
step: 160, loss: 0.15002167224884033
step: 170, loss: 0.0927722230553627
step: 180, loss: 3.2416726753581315e-05
step: 190, loss: 0.12389753758907318
step: 200, loss: 0.13314706087112427
step: 210, loss: 0.12851232290267944
step: 220, loss: 0.2021445333957672
step: 230, loss: 0.13461557030677795
step: 240, loss: 0.05424663797020912
step: 250, loss: 0.07014301419258118
step: 260, loss: 0.008467039093375206
step: 270, loss: 0.0900290459394455
step: 280, loss: 0.02116994373500347
step: 290, loss: 0.00028509931871667504
step: 300, loss: 0.021766353398561478
step: 310, loss: 0.04429377615451813
step: 320, loss: 0.0994911789894104
step: 330, loss: 0.142953559756279
step: 340, loss: 0.06225249543786049
step: 350, loss: 0.014815423637628555
step: 360, loss: 0.09550748020410538
step: 370, loss: 0.02242095395922661
step: 380, loss: 0.14143773913383484
step: 390, loss: 0.03822212666273117
step: 400, loss: 0.0450311116874218
step: 410, loss: 0.022254541516304016
step: 420, loss: 0.024214282631874084
step: 430, loss: 0.1429002583026886
step: 440, loss: 0.060705333948135376
step: 450, loss: 0.09840794652700424
step: 460, loss: 0.06637603789567947
step: 470, loss: 0.10128574073314667
step: 480, loss: 0.09939559549093246
step: 490, loss: 0.02707228809595108
step: 500, loss: 0.19201460480690002
step: 510, loss: 0.046474769711494446
step: 520, loss: 0.02346375584602356
step: 530, loss: 0.014687392860651016
step: 540, loss: 0.12405920028686523
step: 550, loss: 0.08696991950273514
step: 560, loss: 0.031607575714588165
step: 570, loss: 0.09307768195867538
step: 580, loss: 0.1563294231891632
step: 590, loss: 0.0539076030254364
step: 600, loss: 0.08187787979841232
step: 610, loss: 0.051507528871297836
step: 620, loss: 0.06355828791856766
step: 630, loss: 0.06895110756158829
step: 640, loss: 0.01859380677342415
step: 650, loss: 0.023754628375172615
step: 660, loss: 0.08209718018770218
step: 670, loss: 0.06724542379379272
step: 680, loss: 0.1113886684179306
step: 690, loss: 0.08789654076099396
step: 700, loss: 0.06673470139503479
step: 710, loss: 0.005555866751819849
step: 720, loss: 0.05666081979870796
step: 730, loss: 0.06632769107818604
step: 740, loss: 0.03316890448331833
step: 750, loss: 0.09257184714078903
step: 760, loss: 0.005997461266815662
step: 770, loss: 0.12469932436943054
step: 780, loss: 0.07286091148853302
step: 790, loss: 0.04554453492164612
step: 800, loss: 0.09155438840389252
step: 810, loss: 0.06354878842830658
step: 820, loss: 0.03740846738219261
step: 830, loss: 0.007611160632222891
step: 840, loss: 0.02162015810608864
step: 850, loss: 0.1141919493675232
step: 860, loss: 0.007880470715463161
step: 870, loss: 0.08232054114341736
step: 880, loss: 0.07317151129245758
step: 890, loss: 0.06360527127981186
step: 900, loss: 0.014095300808548927
step: 910, loss: 0.023917123675346375
step: 920, loss: 0.017407895997166634
step: 930, loss: 0.03422137722373009
step: 940, loss: 0.008850529789924622
step: 950, loss: 0.01355187501758337
step: 960, loss: 0.1128789484500885
step: 970, loss: 0.06663039326667786
epoch 6: dev_f1=0.9452054794520548, f1=0.9320565435476517, best_f1=0.9320565435476517
step: 0, loss: 0.07263573259115219
step: 10, loss: 0.020107191056013107
step: 20, loss: 0.07246620953083038
step: 30, loss: 0.05896024778485298
step: 40, loss: 0.09144969284534454
step: 50, loss: 0.025140447542071342
step: 60, loss: 0.04278810694813728
step: 70, loss: 0.03236463665962219
step: 80, loss: 0.027540838345885277
step: 90, loss: 0.026802711188793182
step: 100, loss: 0.01626168005168438
step: 110, loss: 0.02773141860961914
step: 120, loss: 0.04879128560423851
step: 130, loss: 0.046916887164115906
step: 140, loss: 0.019393974915146828
step: 150, loss: 0.03752365708351135
step: 160, loss: 0.008165361359715462
step: 170, loss: 0.050462979823350906
step: 180, loss: 0.2102625072002411
step: 190, loss: 0.04819558933377266
step: 200, loss: 0.00859482865780592
step: 210, loss: 0.09114430099725723
step: 220, loss: 0.07020547986030579
step: 230, loss: 0.10810963809490204
step: 240, loss: 0.08200108259916306
step: 250, loss: 0.006774215959012508
step: 260, loss: 0.0662837103009224
step: 270, loss: 0.04682101309299469
step: 280, loss: 0.1069442480802536
step: 290, loss: 0.044355809688568115
step: 300, loss: 0.13953885436058044
step: 310, loss: 0.1460288166999817
step: 320, loss: 0.016599319875240326
step: 330, loss: 0.16941459476947784
step: 340, loss: 0.04071032628417015
step: 350, loss: 0.11605576425790787
step: 360, loss: 0.06757118552923203
step: 370, loss: 0.01799321547150612
step: 380, loss: 0.022472519427537918
step: 390, loss: 0.07586923986673355
step: 400, loss: 0.08800423890352249
step: 410, loss: 0.020990271121263504
step: 420, loss: 0.019748637452721596
step: 430, loss: 0.0992414653301239
step: 440, loss: 0.06955297291278839
step: 450, loss: 0.1664278656244278
step: 460, loss: 0.04263896122574806
step: 470, loss: 0.06451926380395889
step: 480, loss: 0.12105203419923782
step: 490, loss: 0.07817956060171127
step: 500, loss: 0.19338272511959076
step: 510, loss: 0.030800970271229744
step: 520, loss: 0.16552051901817322
step: 530, loss: 0.03510888293385506
step: 540, loss: 0.10865854471921921
step: 550, loss: 0.10388170927762985
step: 560, loss: 0.09468825906515121
step: 570, loss: 0.015752801671624184
step: 580, loss: 0.056438181549310684
step: 590, loss: 0.009269501082599163
step: 600, loss: 0.10093960165977478
step: 610, loss: 0.07611937075853348
step: 620, loss: 0.013735121116042137
step: 630, loss: 0.011819775216281414
step: 640, loss: 0.06627260893583298
step: 650, loss: 0.03336920961737633
step: 660, loss: 0.03107239119708538
step: 670, loss: 0.03625313937664032
step: 680, loss: 0.02444225549697876
step: 690, loss: 0.14059196412563324
step: 700, loss: 0.07040134817361832
step: 710, loss: 0.007898284122347832
step: 720, loss: 0.10090810060501099
step: 730, loss: 0.03768458589911461
step: 740, loss: 0.03640194237232208
step: 750, loss: 0.03146923705935478
step: 760, loss: 0.006945837754756212
step: 770, loss: 0.008468792773783207
step: 780, loss: 0.035513315349817276
step: 790, loss: 0.0652845948934555
step: 800, loss: 0.04781000688672066
step: 810, loss: 0.057629894465208054
step: 820, loss: 0.02106756530702114
step: 830, loss: 0.03433816507458687
step: 840, loss: 0.09754271805286407
step: 850, loss: 0.041591402143239975
step: 860, loss: 0.0958757996559143
step: 870, loss: 0.03531086444854736
step: 880, loss: 0.02847278118133545
step: 890, loss: 0.08089526742696762
step: 900, loss: 0.03552613407373428
step: 910, loss: 0.09490260481834412
step: 920, loss: 0.10989648103713989
step: 930, loss: 0.18087497353553772
step: 940, loss: 0.008561832830309868
step: 950, loss: 0.10863584280014038
step: 960, loss: 0.1046944335103035
step: 970, loss: 0.027501225471496582
epoch 7: dev_f1=0.936111111111111, f1=0.9300827966881325, best_f1=0.9320565435476517
step: 0, loss: 0.002549650613218546
step: 10, loss: 0.10141561925411224
step: 20, loss: 0.11234963685274124
step: 30, loss: 0.022782769054174423
step: 40, loss: 0.011660532094538212
step: 50, loss: 0.04314611852169037
step: 60, loss: 0.052636973559856415
step: 70, loss: 0.0520930290222168
step: 80, loss: 0.10638836771249771
step: 90, loss: 0.0907842144370079
step: 100, loss: 0.09545470774173737
step: 110, loss: 0.06745343655347824
step: 120, loss: 0.07718503475189209
step: 130, loss: 0.06345628947019577
step: 140, loss: 0.03604979068040848
step: 150, loss: 0.15597738325595856
step: 160, loss: 0.06489048898220062
step: 170, loss: 0.01010945439338684
step: 180, loss: 0.019581541419029236
step: 190, loss: 0.019258715212345123
step: 200, loss: 0.031926389783620834
step: 210, loss: 0.10254835337400436
step: 220, loss: 0.010194464586675167
step: 230, loss: 0.00912342220544815
step: 240, loss: 0.018461287021636963
step: 250, loss: 0.16057826578617096
step: 260, loss: 0.13021822273731232
step: 270, loss: 0.006248644553124905
step: 280, loss: 0.0019180758390575647
step: 290, loss: 0.09422993659973145
step: 300, loss: 0.02499905228614807
step: 310, loss: 0.01155168004333973
step: 320, loss: 0.09198462963104248
step: 330, loss: 0.10675334185361862
step: 340, loss: 0.021830745041370392
step: 350, loss: 0.014197561889886856
step: 360, loss: 0.030956633388996124
step: 370, loss: 0.03799928352236748
step: 380, loss: 0.09487686306238174
step: 390, loss: 0.10923192650079727
step: 400, loss: 0.04537058621644974
step: 410, loss: 0.057227447628974915
step: 420, loss: 0.08277787268161774
step: 430, loss: 0.030019596219062805
step: 440, loss: 0.04722663015127182
step: 450, loss: 0.00030515086837112904
step: 460, loss: 0.01094711571931839
step: 470, loss: 0.0905885398387909
step: 480, loss: 0.032837845385074615
step: 490, loss: 0.062283679842948914
step: 500, loss: 0.17126722633838654
step: 510, loss: 0.05552826076745987
step: 520, loss: 0.09115779399871826
step: 530, loss: 0.10749908536672592
step: 540, loss: 0.07403258234262466
step: 550, loss: 0.035127416253089905
step: 560, loss: 0.1226726621389389
step: 570, loss: 0.05305971950292587
step: 580, loss: 0.05864579975605011
step: 590, loss: 0.026448864489793777
step: 600, loss: 0.016883237287402153
step: 610, loss: 0.08877627551555634
step: 620, loss: 0.007797681726515293
step: 630, loss: 0.12747833132743835
step: 640, loss: 0.05276221036911011
step: 650, loss: 0.042251404374837875
step: 660, loss: 0.15063564479351044
step: 670, loss: 0.08734259009361267
step: 680, loss: 0.027989462018013
step: 690, loss: 0.061834800988435745
step: 700, loss: 0.008361739106476307
step: 710, loss: 0.08864177018404007
step: 720, loss: 0.043238457292318344
step: 730, loss: 0.04917771741747856
step: 740, loss: 0.20257754623889923
step: 750, loss: 0.09522322565317154
step: 760, loss: 0.053849101066589355
step: 770, loss: 0.0816953033208847
step: 780, loss: 0.002032577758654952
step: 790, loss: 0.10394500941038132
step: 800, loss: 0.07853087037801743
step: 810, loss: 0.0663638487458229
step: 820, loss: 0.010034509003162384
step: 830, loss: 0.00847906619310379
step: 840, loss: 0.07945864647626877
step: 850, loss: 0.023355204612016678
step: 860, loss: 0.1408328115940094
step: 870, loss: 0.009010955691337585
step: 880, loss: 0.07367398589849472
step: 890, loss: 0.07623057812452316
step: 900, loss: 0.10278184711933136
step: 910, loss: 0.11754077672958374
step: 920, loss: 0.03582561016082764
step: 930, loss: 0.016246376559138298
step: 940, loss: 0.0633120909333229
step: 950, loss: 0.07796279340982437
step: 960, loss: 0.02013932727277279
step: 970, loss: 0.007184532471001148
epoch 8: dev_f1=0.937528921795465, f1=0.929784304726939, best_f1=0.9320565435476517
step: 0, loss: 0.08484771847724915
step: 10, loss: 0.02618071250617504
step: 20, loss: 0.13670165836811066
step: 30, loss: 0.04288038611412048
step: 40, loss: 0.005000140983611345
step: 50, loss: 0.013799579814076424
step: 60, loss: 0.017272941768169403
step: 70, loss: 0.004966493230313063
step: 80, loss: 0.009964476339519024
step: 90, loss: 0.014471626840531826
step: 100, loss: 0.06441902369260788
step: 110, loss: 0.0008427972788922489
step: 120, loss: 0.03536253422498703
step: 130, loss: 0.003931072540581226
step: 140, loss: 0.015478241257369518
step: 150, loss: 0.100680410861969
step: 160, loss: 0.08406165987253189
step: 170, loss: 0.06376466900110245
step: 180, loss: 0.03958173841238022
step: 190, loss: 0.030962590128183365
step: 200, loss: 0.04080994427204132
step: 210, loss: 0.0029369634576141834
step: 220, loss: 0.03263617306947708
step: 230, loss: 0.05597970634698868
step: 240, loss: 0.004230571910738945
step: 250, loss: 0.07670503109693527
step: 260, loss: 0.11179795116186142
step: 270, loss: 0.06831467896699905
step: 280, loss: 0.01886100508272648
step: 290, loss: 0.03552248328924179
step: 300, loss: 0.11848081648349762
step: 310, loss: 0.08141495287418365
step: 320, loss: 0.01581919565796852
step: 330, loss: 0.0905308797955513
step: 340, loss: 0.1456001102924347
step: 350, loss: 0.14490985870361328
step: 360, loss: 0.005310492590069771
step: 370, loss: 0.005946744233369827
step: 380, loss: 0.03693149983882904
step: 390, loss: 0.08604684472084045
step: 400, loss: 0.19842690229415894
step: 410, loss: 0.04090655967593193
step: 420, loss: 0.024151835590600967
step: 430, loss: 0.007580722216516733
step: 440, loss: 0.027930043637752533
step: 450, loss: 0.034809768199920654
step: 460, loss: 0.019690439105033875
step: 470, loss: 0.05838729813694954
step: 480, loss: 0.10513533651828766
step: 490, loss: 0.13810697197914124
step: 500, loss: 0.11613862216472626
step: 510, loss: 0.04205244034528732
step: 520, loss: 0.018753815442323685
step: 530, loss: 0.11157642304897308
step: 540, loss: 0.10755258053541183
step: 550, loss: 0.04230876639485359
step: 560, loss: 0.0388847291469574
step: 570, loss: 0.034414008259773254
step: 580, loss: 0.018820136785507202
step: 590, loss: 0.033534370362758636
step: 600, loss: 0.034054916352033615
step: 610, loss: 0.11566023528575897
step: 620, loss: 0.08415073901414871
step: 630, loss: 0.105772003531456
step: 640, loss: 0.15920117497444153
step: 650, loss: 0.020596738904714584
step: 660, loss: 0.042329493910074234
step: 670, loss: 0.003365842392668128
step: 680, loss: 0.2807048261165619
step: 690, loss: 0.06553178280591965
step: 700, loss: 0.02179145999252796
step: 710, loss: 0.017558149993419647
step: 720, loss: 0.051027268171310425
step: 730, loss: 0.010693065822124481
step: 740, loss: 0.045363809913396835
step: 750, loss: 0.039890263229608536
step: 760, loss: 0.019944779574871063
step: 770, loss: 0.04228085279464722
step: 780, loss: 0.051096364855766296
step: 790, loss: 0.004222800023853779
step: 800, loss: 0.05817575380206108
step: 810, loss: 0.045957572758197784
step: 820, loss: 0.061344683170318604
step: 830, loss: 0.0572797991335392
step: 840, loss: 0.023794131353497505
step: 850, loss: 0.043910346925258636
step: 860, loss: 0.09140811115503311
step: 870, loss: 0.08913516998291016
step: 880, loss: 0.08825932443141937
step: 890, loss: 0.06173432990908623
step: 900, loss: 0.06839337944984436
step: 910, loss: 0.09662563353776932
step: 920, loss: 0.1370115429162979
step: 930, loss: 0.08313164114952087
step: 940, loss: 0.049025990068912506
step: 950, loss: 0.010890788398683071
step: 960, loss: 0.06650049239397049
step: 970, loss: 0.0448116660118103
epoch 9: dev_f1=0.9370629370629371, f1=0.9314179796107508, best_f1=0.9320565435476517
step: 0, loss: 0.01323278620839119
step: 10, loss: 0.10312255471944809
step: 20, loss: 0.04899494722485542
step: 30, loss: 0.0829346552491188
step: 40, loss: 0.03841615840792656
step: 50, loss: 0.040890246629714966
step: 60, loss: 0.08385419100522995
step: 70, loss: 0.012141002342104912
step: 80, loss: 0.07242418825626373
step: 90, loss: 0.05637545511126518
step: 100, loss: 0.038137711584568024
step: 110, loss: 0.04475753754377365
step: 120, loss: 0.11351779848337173
step: 130, loss: 0.09785164892673492
step: 140, loss: 0.010562525130808353
step: 150, loss: 0.061177272349596024
step: 160, loss: 0.015265106223523617
step: 170, loss: 0.014797259122133255
step: 180, loss: 0.03760803863406181
step: 190, loss: 0.015923550352454185
step: 200, loss: 0.01311805471777916
step: 210, loss: 0.08888138085603714
step: 220, loss: 0.033835094422101974
step: 230, loss: 0.1392858624458313
step: 240, loss: 0.04105272516608238
step: 250, loss: 0.013179526664316654
step: 260, loss: 0.07067734748125076
step: 270, loss: 0.024783460423350334
step: 280, loss: 0.15834522247314453
step: 290, loss: 0.08832847326993942
step: 300, loss: 0.13536669313907623
step: 310, loss: 0.10529479384422302
step: 320, loss: 0.048618391156196594
step: 330, loss: 0.029210679233074188
step: 340, loss: 0.0012420882703736424
step: 350, loss: 0.031555864959955215
step: 360, loss: 0.08901042491197586
step: 370, loss: 0.047222062945365906
step: 380, loss: 0.05460464954376221
step: 390, loss: 0.09493523836135864
step: 400, loss: 0.01115192100405693
step: 410, loss: 0.013698149472475052
step: 420, loss: 0.03509828448295593
step: 430, loss: 0.07109224051237106
step: 440, loss: 0.005972817540168762
step: 450, loss: 0.013055307790637016
step: 460, loss: 0.013657270930707455
step: 470, loss: 0.057782094925642014
step: 480, loss: 0.012609951198101044
step: 490, loss: 0.058269113302230835
step: 500, loss: 0.020045533776283264
step: 510, loss: 0.014263329096138477
step: 520, loss: 0.028765525668859482
step: 530, loss: 0.00858526024967432
step: 540, loss: 0.04238074645400047
step: 550, loss: 0.1308327466249466
step: 560, loss: 0.004020187072455883
step: 570, loss: 0.03659183531999588
step: 580, loss: 0.0380912683904171
step: 590, loss: 0.01049898937344551
step: 600, loss: 0.0779898464679718
step: 610, loss: 0.013301280327141285
step: 620, loss: 0.1241491287946701
step: 630, loss: 0.0053323497995734215
step: 640, loss: 0.036544907838106155
step: 650, loss: 0.042657360434532166
step: 660, loss: 0.06981007009744644
step: 670, loss: 0.07919030636548996
step: 680, loss: 0.01094837486743927
step: 690, loss: 0.015422387048602104
step: 700, loss: 0.09622025489807129
step: 710, loss: 0.005060619208961725
step: 720, loss: 0.031045611947774887
step: 730, loss: 0.09904716163873672
step: 740, loss: 0.03740133345127106
step: 750, loss: 0.041582804173231125
step: 760, loss: 0.09575855731964111
step: 770, loss: 0.011670777574181557
step: 780, loss: 0.04287977144122124
step: 790, loss: 0.018158316612243652
step: 800, loss: 0.07962074130773544
step: 810, loss: 0.04175776243209839
step: 820, loss: 0.10209816694259644
step: 830, loss: 0.04915357008576393
step: 840, loss: 0.030800839886069298
step: 850, loss: 0.0824265107512474
step: 860, loss: 0.08401814848184586
step: 870, loss: 0.0027185443323105574
step: 880, loss: 0.08404120802879333
step: 890, loss: 0.07039602100849152
step: 900, loss: 0.02681788057088852
step: 910, loss: 0.04613853245973587
step: 920, loss: 0.030744560062885284
step: 930, loss: 0.13022944331169128
step: 940, loss: 0.029747623950242996
step: 950, loss: 0.07781467586755753
step: 960, loss: 0.002522417576983571
step: 970, loss: 0.05945349484682083
epoch 10: dev_f1=0.9317453046266607, f1=0.9277879341864718, best_f1=0.9320565435476517
step: 0, loss: 0.039792031049728394
step: 10, loss: 0.08056054264307022
step: 20, loss: 0.03392047435045242
step: 30, loss: 0.007202132139354944
step: 40, loss: 0.012961454689502716
step: 50, loss: 0.0019783335737884045
step: 60, loss: 0.0003543031925801188
step: 70, loss: 0.024854760617017746
step: 80, loss: 0.027195507660508156
step: 90, loss: 0.0807986855506897
step: 100, loss: 0.03081507980823517
step: 110, loss: 0.11587999761104584
step: 120, loss: 0.08995367586612701
step: 130, loss: 0.016269968822598457
step: 140, loss: 0.041385725140571594
step: 150, loss: 0.014418423175811768
step: 160, loss: 0.11053997278213501
step: 170, loss: 0.0902978777885437
step: 180, loss: 0.026898328214883804
step: 190, loss: 0.013827549293637276
step: 200, loss: 0.02230287902057171
step: 210, loss: 0.034469835460186005
step: 220, loss: 0.024608487263321877
step: 230, loss: 0.05039622262120247
step: 240, loss: 0.019027618691325188
step: 250, loss: 0.03311006724834442
step: 260, loss: 0.025940164923667908
step: 270, loss: 0.00410247128456831
step: 280, loss: 0.1290408819913864
step: 290, loss: 0.02321578562259674
step: 300, loss: 0.007707667071372271
step: 310, loss: 0.1204419657588005
step: 320, loss: 0.040972426533699036
step: 330, loss: 0.012882284820079803
step: 340, loss: 0.004602564964443445
step: 350, loss: 0.12271276861429214
step: 360, loss: 0.024930721148848534
step: 370, loss: 0.023418352007865906
step: 380, loss: 0.002484849886968732
step: 390, loss: 0.04965531826019287
step: 400, loss: 0.02913411520421505
step: 410, loss: 0.12983688712120056
step: 420, loss: 0.14527195692062378
step: 430, loss: 0.028402168303728104
step: 440, loss: 0.24292802810668945
step: 450, loss: 0.11500710248947144
step: 460, loss: 0.07372406870126724
step: 470, loss: 0.023501260206103325
step: 480, loss: 0.23610134422779083
step: 490, loss: 0.003047170815989375
step: 500, loss: 0.05804760754108429
step: 510, loss: 0.007187511771917343
step: 520, loss: 0.0031650569289922714
step: 530, loss: 0.08734694123268127
step: 540, loss: 0.044226814061403275
step: 550, loss: 0.2249947488307953
step: 560, loss: 0.1043674647808075
step: 570, loss: 0.058970797806978226
step: 580, loss: 0.06851234287023544
step: 590, loss: 0.08760814368724823
step: 600, loss: 0.08813457190990448
step: 610, loss: 0.0036870529875159264
step: 620, loss: 0.01100350171327591
step: 630, loss: 0.04778642579913139
step: 640, loss: 0.032592207193374634
step: 650, loss: 0.13983500003814697
step: 660, loss: 0.046918950974941254
step: 670, loss: 0.07041992247104645
step: 680, loss: 0.006129656918346882
step: 690, loss: 0.0925235003232956
step: 700, loss: 0.04267700016498566
step: 710, loss: 0.03206884115934372
step: 720, loss: 0.019508810713887215
step: 730, loss: 0.23919257521629333
step: 740, loss: 0.11359492689371109
step: 750, loss: 0.05869626626372337
step: 760, loss: 0.03766495734453201
step: 770, loss: 0.015464331954717636
step: 780, loss: 0.05289531499147415
step: 790, loss: 0.004935304634273052
step: 800, loss: 0.059638265520334244
step: 810, loss: 0.01955515891313553
step: 820, loss: 0.07796609401702881
step: 830, loss: 0.06628827750682831
step: 840, loss: 0.049797963351011276
step: 850, loss: 0.11588823795318604
step: 860, loss: 0.03653852641582489
step: 870, loss: 0.0008185693877749145
step: 880, loss: 0.019772591069340706
step: 890, loss: 0.05104580149054527
step: 900, loss: 0.07794467359781265
step: 910, loss: 0.0011867342982441187
step: 920, loss: 0.09740772098302841
step: 930, loss: 0.0029784017242491245
step: 940, loss: 0.06709954887628555
step: 950, loss: 0.0534527562558651
step: 960, loss: 0.010821381583809853
step: 970, loss: 0.10290522873401642
epoch 11: dev_f1=0.933271547729379, f1=0.9295255642561031, best_f1=0.9320565435476517
step: 0, loss: 0.045628637075424194
step: 10, loss: 0.10861092805862427
step: 20, loss: 0.03041280433535576
step: 30, loss: 0.0010669655166566372
step: 40, loss: 0.08086682111024857
step: 50, loss: 0.012311676517128944
step: 60, loss: 0.042969055473804474
step: 70, loss: 0.0008724955259822309
step: 80, loss: 0.10219632089138031
step: 90, loss: 0.025972742587327957
step: 100, loss: 0.07462114095687866
step: 110, loss: 0.0020966758020222187
step: 120, loss: 0.025864984840154648
step: 130, loss: 0.1188112124800682
step: 140, loss: 0.07392124831676483
step: 150, loss: 0.056720733642578125
step: 160, loss: 0.001039460301399231
step: 170, loss: 0.00986770074814558
step: 180, loss: 0.08013170212507248
step: 190, loss: 0.07563015818595886
step: 200, loss: 0.03718218579888344
step: 210, loss: 0.06488636881113052
step: 220, loss: 0.07528167963027954
step: 230, loss: 0.005737345200031996
step: 240, loss: 0.015578788705170155
step: 250, loss: 0.020149171352386475
step: 260, loss: 0.02885192632675171
step: 270, loss: 0.07480599731206894
step: 280, loss: 0.026109343394637108
step: 290, loss: 0.023753074929118156
step: 300, loss: 0.04354320839047432
step: 310, loss: 0.02859071083366871
step: 320, loss: 0.030896969139575958
step: 330, loss: 0.08346019685268402
step: 340, loss: 0.05570755526423454
step: 350, loss: 0.06206699460744858
step: 360, loss: 0.005043835379183292
step: 370, loss: 0.06084307283163071
step: 380, loss: 0.046136558055877686
step: 390, loss: 0.07548984885215759
step: 400, loss: 0.010340387001633644
step: 410, loss: 0.04805232957005501
step: 420, loss: 0.016465943306684494
step: 430, loss: 0.01828577183187008
step: 440, loss: 0.006785603240132332
step: 450, loss: 0.012055105529725552
step: 460, loss: 0.0442095547914505
step: 470, loss: 0.017125070095062256
step: 480, loss: 2.3706435968051665e-05
step: 490, loss: 0.03615207597613335
step: 500, loss: 0.01893593557178974
step: 510, loss: 0.002983720740303397
step: 520, loss: 0.017134880647063255
step: 530, loss: 0.034344788640737534
step: 540, loss: 0.038364239037036896
step: 550, loss: 0.037559252232313156
step: 560, loss: 0.033558011054992676
step: 570, loss: 0.005324442870914936
step: 580, loss: 0.04904526472091675
step: 590, loss: 0.03551945462822914
step: 600, loss: 0.056511037051677704
step: 610, loss: 0.018929235637187958
step: 620, loss: 0.03517790138721466
step: 630, loss: 0.025741107761859894
step: 640, loss: 0.01849750056862831
step: 650, loss: 0.07727299630641937
step: 660, loss: 0.03619798272848129
step: 670, loss: 0.06759626418352127
step: 680, loss: 0.03254450857639313
step: 690, loss: 0.11365821212530136
step: 700, loss: 0.022006627172231674
step: 710, loss: 0.012957186438143253
step: 720, loss: 0.05050387978553772
step: 730, loss: 0.02313409559428692
step: 740, loss: 0.03336593508720398
step: 750, loss: 0.01480657048523426
step: 760, loss: 0.002378217177465558
step: 770, loss: 0.00258176913484931
step: 780, loss: 0.021576453000307083
step: 790, loss: 0.030220013111829758
step: 800, loss: 0.012544456869363785
step: 810, loss: 0.060897499322891235
step: 820, loss: 0.07497037947177887
step: 830, loss: 0.012053859420120716
step: 840, loss: 0.03093797154724598
step: 850, loss: 0.0025731464847922325
step: 860, loss: 0.029668636620044708
step: 870, loss: 0.06236600875854492
step: 880, loss: 0.008728095330297947
step: 890, loss: 0.021816814318299294
step: 900, loss: 0.04257335141301155
step: 910, loss: 0.06842917203903198
step: 920, loss: 0.16183936595916748
step: 930, loss: 0.09326647222042084
step: 940, loss: 0.09262821078300476
step: 950, loss: 0.02174311690032482
step: 960, loss: 0.01564917527139187
step: 970, loss: 0.09343889355659485
epoch 12: dev_f1=0.9325946445060019, f1=0.9275229357798166, best_f1=0.9320565435476517
step: 0, loss: 0.02077600359916687
step: 10, loss: 0.01764344796538353
step: 20, loss: 0.0006280173547565937
step: 30, loss: 0.04808826744556427
step: 40, loss: 0.0009138788445852697
step: 50, loss: 0.0527120940387249
step: 60, loss: 0.0416715070605278
step: 70, loss: 0.025792084634304047
step: 80, loss: 0.054785650223493576
step: 90, loss: 0.020875578746199608
step: 100, loss: 0.00721985986456275
step: 110, loss: 0.09747147560119629
step: 120, loss: 0.019957365468144417
step: 130, loss: 0.02047310769557953
step: 140, loss: 0.11043858528137207
step: 150, loss: 0.024871138855814934
step: 160, loss: 0.0005745244561694562
step: 170, loss: 0.07283014059066772
step: 180, loss: 0.0264071524143219
step: 190, loss: 0.031762246042490005
step: 200, loss: 0.017976047471165657
step: 210, loss: 0.0003316747897770256
step: 220, loss: 0.0027033507358282804
step: 230, loss: 0.02678176946938038
step: 240, loss: 0.0005083880969323218
step: 250, loss: 0.007254316471517086
step: 260, loss: 0.024815835058689117
step: 270, loss: 0.008524957112967968
step: 280, loss: 0.005551174283027649
step: 290, loss: 0.018299341201782227
step: 300, loss: 0.025474846363067627
step: 310, loss: 0.05846935138106346
step: 320, loss: 0.04424167424440384
step: 330, loss: 0.06361663341522217
step: 340, loss: 0.06091034412384033
step: 350, loss: 0.01762048713862896
step: 360, loss: 1.1522211934789084e-05
step: 370, loss: 0.042784687131643295
step: 380, loss: 0.07376690208911896
step: 390, loss: 0.04574201628565788
step: 400, loss: 0.23953545093536377
step: 410, loss: 0.03214816004037857
step: 420, loss: 0.0009073323453776538
step: 430, loss: 0.07336585968732834
step: 440, loss: 0.027691515162587166
step: 450, loss: 0.032285481691360474
step: 460, loss: 0.004359646234661341
step: 470, loss: 0.04030928015708923
step: 480, loss: 0.09107143431901932
step: 490, loss: 0.05057506635785103
step: 500, loss: 0.03124530240893364
step: 510, loss: 0.033962737768888474
step: 520, loss: 0.03376366198062897
step: 530, loss: 0.05922381579875946
step: 540, loss: 0.06173384189605713
step: 550, loss: 0.002764776349067688
step: 560, loss: 0.05627996101975441
step: 570, loss: 0.08896059542894363
step: 580, loss: 0.052446555346250534
step: 590, loss: 0.07432502508163452
step: 600, loss: 0.1889481097459793
step: 610, loss: 0.0019274306250736117
step: 620, loss: 0.013081471435725689
step: 630, loss: 0.05174599587917328
step: 640, loss: 0.017059436067938805
step: 650, loss: 0.021450582891702652
step: 660, loss: 0.015127110294997692
step: 670, loss: 0.056427787989377975
step: 680, loss: 0.08762302994728088
step: 690, loss: 0.0018436596728861332
step: 700, loss: 0.0691566988825798
step: 710, loss: 0.036093439906835556
step: 720, loss: 0.010416476987302303
step: 730, loss: 0.0009264123509638011
step: 740, loss: 0.0038495257031172514
step: 750, loss: 0.013046272099018097
step: 760, loss: 0.04639573022723198
step: 770, loss: 0.0023867622949182987
step: 780, loss: 0.05426529049873352
step: 790, loss: 0.0003579777549020946
step: 800, loss: 0.001670114346779883
step: 810, loss: 0.014567617326974869
step: 820, loss: 0.02275313436985016
step: 830, loss: 0.00040316695231013
step: 840, loss: 0.06927323341369629
step: 850, loss: 0.04240693524479866
step: 860, loss: 0.023497674614191055
step: 870, loss: 0.02294885367155075
step: 880, loss: 0.06646405905485153
step: 890, loss: 8.804897515801713e-05
step: 900, loss: 0.02639615163207054
step: 910, loss: 0.07898719608783722
step: 920, loss: 0.07342812418937683
step: 930, loss: 0.026963133364915848
step: 940, loss: 0.01486295647919178
step: 950, loss: 0.0011710283579304814
step: 960, loss: 0.03740591183304787
step: 970, loss: 0.019552189856767654
epoch 13: dev_f1=0.9333333333333335, f1=0.9297445255474452, best_f1=0.9320565435476517
step: 0, loss: 0.05596044659614563
step: 10, loss: 0.07245364785194397
step: 20, loss: 0.02661227062344551
step: 30, loss: 0.015600989572703838
step: 40, loss: 0.014594526030123234
step: 50, loss: 0.062409259378910065
step: 60, loss: 0.024707691743969917
step: 70, loss: 0.04903850331902504
step: 80, loss: 0.026988741010427475
step: 90, loss: 1.292285287490813e-05
step: 100, loss: 0.027360988780856133
step: 110, loss: 0.09737283736467361
step: 120, loss: 0.004673587158322334
step: 130, loss: 0.00029354458092711866
step: 140, loss: 0.02829851396381855
step: 150, loss: 0.001070517930202186
step: 160, loss: 0.05609988793730736
step: 170, loss: 0.09668242186307907
step: 180, loss: 0.0800418108701706
step: 190, loss: 0.0006439520511776209
step: 200, loss: 0.09061441570520401
step: 210, loss: 0.09600464254617691
step: 220, loss: 4.86606550111901e-05
step: 230, loss: 0.020824087783694267
step: 240, loss: 0.026139630004763603
step: 250, loss: 0.031116249039769173
step: 260, loss: 0.03523249179124832
step: 270, loss: 0.07345372438430786
step: 280, loss: 0.021624203771352768
step: 290, loss: 0.04679713398218155
step: 300, loss: 0.0750369280576706
step: 310, loss: 0.030045492574572563
step: 320, loss: 0.00018967829237226397
step: 330, loss: 0.07924238592386246
step: 340, loss: 0.05400645360350609
step: 350, loss: 0.006574624218046665
step: 360, loss: 0.0311354398727417
step: 370, loss: 0.03161291033029556
step: 380, loss: 0.053606051951646805
step: 390, loss: 0.03293557092547417
step: 400, loss: 0.026510324329137802
step: 410, loss: 5.267445158096962e-05
step: 420, loss: 0.00011186034680576995
step: 430, loss: 0.004024193622171879
step: 440, loss: 0.0691085234284401
step: 450, loss: 0.06825833022594452
step: 460, loss: 0.0814984068274498
step: 470, loss: 0.0026224965695291758
step: 480, loss: 0.028332406654953957
step: 490, loss: 0.00021165442012716085
step: 500, loss: 0.11474229395389557
step: 510, loss: 0.020882021635770798
step: 520, loss: 0.0024690364953130484
step: 530, loss: 0.02106541022658348
step: 540, loss: 0.03680825233459473
step: 550, loss: 0.006455983500927687
step: 560, loss: 0.037196237593889236
step: 570, loss: 0.057194922119379044
step: 580, loss: 0.04573585465550423
step: 590, loss: 0.0025211975444108248
step: 600, loss: 0.020046507939696312
step: 610, loss: 0.002150228712707758
step: 620, loss: 0.04563869163393974
step: 630, loss: 0.014174307696521282
step: 640, loss: 0.00024881958961486816
step: 650, loss: 0.02590653859078884
step: 660, loss: 0.0012855739332735538
step: 670, loss: 0.08461222797632217
step: 680, loss: 0.07633639872074127
step: 690, loss: 0.0006912443786859512
step: 700, loss: 0.0011713459389284253
step: 710, loss: 0.027366459369659424
step: 720, loss: 0.05020428076386452
step: 730, loss: 0.002340923761948943
step: 740, loss: 0.16211456060409546
step: 750, loss: 0.030227191746234894
step: 760, loss: 0.0011871348833665252
step: 770, loss: 0.05790407955646515
step: 780, loss: 0.0009079929441213608
step: 790, loss: 0.06833267956972122
step: 800, loss: 0.002370205707848072
step: 810, loss: 0.045663509517908096
step: 820, loss: 0.04461504518985748
step: 830, loss: 0.022304339334368706
step: 840, loss: 0.07172361761331558
step: 850, loss: 0.010058405809104443
step: 860, loss: 0.016327328979969025
step: 870, loss: 0.03752237558364868
step: 880, loss: 0.06124460697174072
step: 890, loss: 0.060617443174123764
step: 900, loss: 0.019540632143616676
step: 910, loss: 0.012533605098724365
step: 920, loss: 0.06305792927742004
step: 930, loss: 0.0025004963390529156
step: 940, loss: 0.10880383849143982
step: 950, loss: 0.029590167105197906
step: 960, loss: 0.023551618680357933
step: 970, loss: 0.06605324894189835
epoch 14: dev_f1=0.9330889092575618, f1=0.92616226071103, best_f1=0.9320565435476517
step: 0, loss: 0.01404652465134859
step: 10, loss: 0.018722068518400192
step: 20, loss: 0.017626535147428513
step: 30, loss: 0.01940790005028248
step: 40, loss: 0.01819724030792713
step: 50, loss: 0.02203145995736122
step: 60, loss: 0.0006739660166203976
step: 70, loss: 0.0776853859424591
step: 80, loss: 0.025758150964975357
step: 90, loss: 0.038919076323509216
step: 100, loss: 0.0706680491566658
step: 110, loss: 0.06620397418737411
step: 120, loss: 0.017993077635765076
step: 130, loss: 0.01958545669913292
step: 140, loss: 0.051857415586709976
step: 150, loss: 0.0411720871925354
step: 160, loss: 0.019638972356915474
step: 170, loss: 0.0514613538980484
step: 180, loss: 0.0320545919239521
step: 190, loss: 0.0008779621566645801
step: 200, loss: 0.01744585856795311
step: 210, loss: 0.022903192788362503
step: 220, loss: 0.035983115434646606
step: 230, loss: 0.09677170217037201
step: 240, loss: 0.03426017984747887
step: 250, loss: 8.607240306446329e-05
step: 260, loss: 0.07069234549999237
step: 270, loss: 0.06413808465003967
step: 280, loss: 0.00012074244295945391
step: 290, loss: 0.05501334369182587
step: 300, loss: 0.06255842745304108
step: 310, loss: 0.012720875442028046
step: 320, loss: 0.09226865321397781
step: 330, loss: 0.0021600634790956974
step: 340, loss: 0.05664549022912979
step: 350, loss: 0.011694498360157013
step: 360, loss: 0.04971814528107643
step: 370, loss: 8.013067599677015e-06
step: 380, loss: 0.04204487055540085
step: 390, loss: 0.016250811517238617
step: 400, loss: 0.05969180539250374
step: 410, loss: 0.0007326111663132906
step: 420, loss: 0.011340981349349022
step: 430, loss: 0.0017048516310751438
step: 440, loss: 0.08742909878492355
step: 450, loss: 0.0021912644151598215
step: 460, loss: 0.032801274210214615
step: 470, loss: 0.021065417677164078
step: 480, loss: 0.005998600274324417
step: 490, loss: 0.01437140628695488
step: 500, loss: 0.027381593361496925
step: 510, loss: 0.025358548387885094
step: 520, loss: 0.02031608298420906
step: 530, loss: 0.016185132786631584
step: 540, loss: 0.0009141949703916907
step: 550, loss: 0.02201886661350727
step: 560, loss: 0.00024331403255928308
step: 570, loss: 0.028885336592793465
step: 580, loss: 0.007680446840822697
step: 590, loss: 0.022169427946209908
step: 600, loss: 0.03334838151931763
step: 610, loss: 0.0035232100635766983
step: 620, loss: 0.000789302634075284
step: 630, loss: 0.06317475438117981
step: 640, loss: 0.056424204260110855
step: 650, loss: 0.04931433126330376
step: 660, loss: 3.4528333344496787e-05
step: 670, loss: 0.018352828919887543
step: 680, loss: 0.0001857970200944692
step: 690, loss: 0.03501555323600769
step: 700, loss: 0.041365865617990494
step: 710, loss: 0.007822319865226746
step: 720, loss: 0.029534805566072464
step: 730, loss: 0.026794737204909325
step: 740, loss: 0.06280495226383209
step: 750, loss: 0.06509078294038773
step: 760, loss: 0.09206878393888474
step: 770, loss: 0.019577573984861374
step: 780, loss: 0.06209729611873627
step: 790, loss: 0.021450888365507126
step: 800, loss: 0.00011647091741906479
step: 810, loss: 0.01161669846624136
step: 820, loss: 0.042643576860427856
step: 830, loss: 0.08234433829784393
step: 840, loss: 0.012354085221886635
step: 850, loss: 0.001010420615784824
step: 860, loss: 0.000348820467479527
step: 870, loss: 0.05951444432139397
step: 880, loss: 0.02028355561196804
step: 890, loss: 0.0004440841730684042
step: 900, loss: 0.015888512134552002
step: 910, loss: 0.0430414117872715
step: 920, loss: 0.02843663841485977
step: 930, loss: 0.01869680918753147
step: 940, loss: 0.017805762588977814
step: 950, loss: 4.841194095206447e-05
step: 960, loss: 0.047839369624853134
step: 970, loss: 0.005075955763459206
epoch 15: dev_f1=0.9319664492078286, f1=0.9255663430420712, best_f1=0.9320565435476517
step: 0, loss: 0.07366090267896652
step: 10, loss: 0.056545939296483994
step: 20, loss: 0.023366857320070267
step: 30, loss: 0.09591546654701233
step: 40, loss: 0.0016741006402298808
step: 50, loss: 0.02286028303205967
step: 60, loss: 0.07621961832046509
step: 70, loss: 0.06081901490688324
step: 80, loss: 0.041304364800453186
step: 90, loss: 0.08984290808439255
step: 100, loss: 0.0001979705411940813
step: 110, loss: 0.015743466094136238
step: 120, loss: 0.06740918755531311
step: 130, loss: 0.03692770376801491
step: 140, loss: 0.03936704620718956
step: 150, loss: 2.393619251961354e-05
step: 160, loss: 0.16798089444637299
step: 170, loss: 0.002171998145058751
step: 180, loss: 0.008731055073440075
step: 190, loss: 0.0763900950551033
step: 200, loss: 0.025537965819239616
step: 210, loss: 3.5520773963071406e-05
step: 220, loss: 0.04504534974694252
step: 230, loss: 0.01785355620086193
step: 240, loss: 0.00354243372566998
step: 250, loss: 9.26875727600418e-05
step: 260, loss: 0.016454728320240974
step: 270, loss: 6.811904313508421e-05
step: 280, loss: 0.03806428611278534
step: 290, loss: 0.04447326809167862
step: 300, loss: 0.00015846772294025868
step: 310, loss: 0.022467080503702164
step: 320, loss: 0.0004070088325534016
step: 330, loss: 6.972210394451395e-05
step: 340, loss: 0.015142957679927349
step: 350, loss: 0.05562517046928406
step: 360, loss: 0.030926939100027084
step: 370, loss: 0.0070928167551755905
step: 380, loss: 0.05523529276251793
step: 390, loss: 0.0003109387180302292
step: 400, loss: 0.17731787264347076
step: 410, loss: 0.0008731989655643702
step: 420, loss: 0.023182915523648262
step: 430, loss: 4.966054257238284e-05
step: 440, loss: 0.043077610433101654
step: 450, loss: 0.00043660757364705205
step: 460, loss: 0.07227801531553268
step: 470, loss: 0.0904170498251915
step: 480, loss: 0.0006239450303837657
step: 490, loss: 0.06151133030653
step: 500, loss: 0.04946175217628479
step: 510, loss: 0.0393882617354393
step: 520, loss: 0.07383114844560623
step: 530, loss: 0.010097221471369267
step: 540, loss: 0.031171685084700584
step: 550, loss: 5.793084346805699e-05
step: 560, loss: 0.04684877395629883
step: 570, loss: 0.018295295536518097
step: 580, loss: 0.0004590327152982354
step: 590, loss: 0.041435882449150085
step: 600, loss: 0.047334060072898865
step: 610, loss: 0.036683499813079834
step: 620, loss: 1.6874726497917436e-05
step: 630, loss: 0.08963296562433243
step: 640, loss: 8.84880282683298e-05
step: 650, loss: 0.042103081941604614
step: 660, loss: 0.04954534024000168
step: 670, loss: 0.04724055901169777
step: 680, loss: 3.1139068596530706e-05
step: 690, loss: 2.3880713342805393e-05
step: 700, loss: 0.028151968494057655
step: 710, loss: 0.07180538773536682
step: 720, loss: 0.031072812154889107
step: 730, loss: 0.04593224823474884
step: 740, loss: 0.00805630348622799
step: 750, loss: 0.04384651035070419
step: 760, loss: 0.05683750659227371
step: 770, loss: 0.057930104434490204
step: 780, loss: 0.08677899837493896
step: 790, loss: 1.1328455912007485e-05
step: 800, loss: 0.00031134221353568137
step: 810, loss: 0.06579449027776718
step: 820, loss: 0.02415149286389351
step: 830, loss: 0.03750629350543022
step: 840, loss: 0.005147083662450314
step: 850, loss: 0.047092583030462265
step: 860, loss: 0.03413204848766327
step: 870, loss: 0.0013537266058847308
step: 880, loss: 0.028708579018712044
step: 890, loss: 0.02947583608329296
step: 900, loss: 0.01990997977554798
step: 910, loss: 0.04971819370985031
step: 920, loss: 0.030819041654467583
step: 930, loss: 0.02327907644212246
step: 940, loss: 0.024593885987997055
step: 950, loss: 0.022791873663663864
step: 960, loss: 0.030295055359601974
step: 970, loss: 0.021549159660935402
epoch 16: dev_f1=0.9342657342657343, f1=0.9298245614035088, best_f1=0.9320565435476517
step: 0, loss: 0.022642068564891815
step: 10, loss: 0.059781648218631744
step: 20, loss: 0.009467844851315022
step: 30, loss: 0.018211549147963524
step: 40, loss: 0.000461163988802582
step: 50, loss: 4.3180491047678515e-05
step: 60, loss: 0.0011560814455151558
step: 70, loss: 0.08213232457637787
step: 80, loss: 0.04871344193816185
step: 90, loss: 0.03571584075689316
step: 100, loss: 0.00023005409457255155
step: 110, loss: 0.047186046838760376
step: 120, loss: 0.08799248188734055
step: 130, loss: 0.019483957439661026
step: 140, loss: 0.011369633488357067
step: 150, loss: 6.988763198023662e-05
step: 160, loss: 0.00024293313617818058
step: 170, loss: 0.017503604292869568
step: 180, loss: 0.04445568472146988
step: 190, loss: 0.04089120030403137
step: 200, loss: 0.02386394701898098
step: 210, loss: 0.0012823613360524178
step: 220, loss: 0.04178832843899727
step: 230, loss: 0.00037240402889437973
step: 240, loss: 0.013202494010329247
step: 250, loss: 0.04892122745513916
step: 260, loss: 0.03951921686530113
step: 270, loss: 0.000181659561349079
step: 280, loss: 0.00025645963614806533
step: 290, loss: 0.042852409183979034
step: 300, loss: 0.0001208189787575975
step: 310, loss: 0.05915287137031555
step: 320, loss: 0.06024882197380066
step: 330, loss: 0.01844012551009655
step: 340, loss: 0.04986587539315224
step: 350, loss: 0.0006913313991390169
step: 360, loss: 0.06845536828041077
step: 370, loss: 0.00010988512804033235
step: 380, loss: 0.09029089659452438
step: 390, loss: 0.05139966309070587
step: 400, loss: 0.041848815977573395
step: 410, loss: 0.0002952388022094965
step: 420, loss: 0.03818428888916969
step: 430, loss: 0.0009392587235197425
step: 440, loss: 0.025459308177232742
step: 450, loss: 0.023963697254657745
step: 460, loss: 0.07763130217790604
step: 470, loss: 0.08478261530399323
step: 480, loss: 7.214814831968397e-05
step: 490, loss: 0.002527607372030616
step: 500, loss: 6.943012704141438e-05
step: 510, loss: 9.827237590798177e-06
step: 520, loss: 0.023283902555704117
step: 530, loss: 0.003265269799157977
step: 540, loss: 0.025037551298737526
step: 550, loss: 0.02050505205988884
step: 560, loss: 8.956524106906727e-05
step: 570, loss: 0.020077930763363838
step: 580, loss: 0.021272558718919754
step: 590, loss: 0.002677856246009469
step: 600, loss: 0.0008853033650666475
step: 610, loss: 0.020891856402158737
step: 620, loss: 0.09461364150047302
step: 630, loss: 0.006274332292377949
step: 640, loss: 0.04197053238749504
step: 650, loss: 0.04160917550325394
step: 660, loss: 0.030866021290421486
step: 670, loss: 0.08634032309055328
step: 680, loss: 0.02841522917151451
step: 690, loss: 8.47695700940676e-05
step: 700, loss: 4.692359652835876e-05
step: 710, loss: 0.0007278415141627192
step: 720, loss: 0.023017525672912598
step: 730, loss: 0.002639565384015441
step: 740, loss: 0.07914380729198456
step: 750, loss: 0.05705856531858444
step: 760, loss: 4.842652197112329e-05
step: 770, loss: 0.024698778986930847
step: 780, loss: 0.0293139535933733
step: 790, loss: 0.02092612162232399
step: 800, loss: 0.04609779641032219
step: 810, loss: 0.08481945842504501
step: 820, loss: 2.8759794076904655e-05
step: 830, loss: 0.02650357596576214
step: 840, loss: 0.022147206589579582
step: 850, loss: 3.635454777395353e-05
step: 860, loss: 0.02266625501215458
step: 870, loss: 0.060117121785879135
step: 880, loss: 0.019655689597129822
step: 890, loss: 0.061513811349868774
step: 900, loss: 0.01646583341062069
step: 910, loss: 4.020276537630707e-05
step: 920, loss: 0.0024964609183371067
step: 930, loss: 0.03862374275922775
step: 940, loss: 0.05768035724759102
step: 950, loss: 0.0005107042379677296
step: 960, loss: 0.056062404066324234
step: 970, loss: 0.022529009729623795
epoch 17: dev_f1=0.9363086936308694, f1=0.9253456221198156, best_f1=0.9320565435476517
step: 0, loss: 0.0008720764890313148
step: 10, loss: 0.040235139429569244
step: 20, loss: 0.06648886203765869
step: 30, loss: 0.08860962092876434
step: 40, loss: 0.01939299702644348
step: 50, loss: 0.09753145277500153
step: 60, loss: 0.01828470081090927
step: 70, loss: 0.04081356152892113
step: 80, loss: 0.03127075359225273
step: 90, loss: 0.006221211515367031
step: 100, loss: 0.029586046934127808
step: 110, loss: 0.06216471269726753
step: 120, loss: 0.07279103249311447
step: 130, loss: 0.0431986078619957
step: 140, loss: 0.04080323129892349
step: 150, loss: 0.05663144960999489
step: 160, loss: 0.020837612450122833
step: 170, loss: 0.02516849711537361
step: 180, loss: 0.0002166753401979804
step: 190, loss: 0.04377082362771034
step: 200, loss: 0.06254056096076965
step: 210, loss: 0.020290043205022812
step: 220, loss: 0.026212487369775772
step: 230, loss: 0.0002117468393407762
step: 240, loss: 0.042280323803424835
step: 250, loss: 0.00030783007969148457
step: 260, loss: 0.026056421920657158
step: 270, loss: 0.00010839456808753312
step: 280, loss: 0.030677400529384613
step: 290, loss: 0.0001425546797690913
step: 300, loss: 0.0009855730459094048
step: 310, loss: 0.0002301486092619598
step: 320, loss: 0.0002640029997564852
step: 330, loss: 0.03992071375250816
step: 340, loss: 0.00017152552027255297
step: 350, loss: 0.05812036618590355
step: 360, loss: 0.009997369721531868
step: 370, loss: 0.0413825623691082
step: 380, loss: 0.06274972856044769
step: 390, loss: 0.0001401641929987818
step: 400, loss: 0.13663874566555023
step: 410, loss: 0.04433348402380943
step: 420, loss: 0.05085644870996475
step: 430, loss: 0.04307055473327637
step: 440, loss: 0.013040412217378616
step: 450, loss: 0.03943346440792084
step: 460, loss: 8.068892930168658e-05
step: 470, loss: 0.00016006336954887956
step: 480, loss: 0.05086590349674225
step: 490, loss: 0.14236830174922943
step: 500, loss: 0.053526632487773895
step: 510, loss: 0.021671349182724953
step: 520, loss: 0.0015456952387467027
step: 530, loss: 0.11998893320560455
step: 540, loss: 0.0003241836093366146
step: 550, loss: 0.024606572464108467
step: 560, loss: 0.00021688072592951357
step: 570, loss: 0.023626351729035378
step: 580, loss: 0.0423433743417263
step: 590, loss: 0.10361611843109131
step: 600, loss: 0.038707949221134186
step: 610, loss: 0.021508358418941498
step: 620, loss: 0.10018225759267807
step: 630, loss: 0.02023548074066639
step: 640, loss: 0.02605579048395157
step: 650, loss: 0.01776178739964962
step: 660, loss: 0.050169937312603
step: 670, loss: 0.05765824392437935
step: 680, loss: 0.049104344099760056
step: 690, loss: 0.060486096888780594
step: 700, loss: 0.0035213963128626347
step: 710, loss: 0.03134136274456978
step: 720, loss: 1.1361933502485044e-05
step: 730, loss: 0.010261170566082
step: 740, loss: 5.798565689474344e-05
step: 750, loss: 0.000784883217420429
step: 760, loss: 0.06392521411180496
step: 770, loss: 0.05563068762421608
step: 780, loss: 0.03981276974081993
step: 790, loss: 0.018158262595534325
step: 800, loss: 0.010280977003276348
step: 810, loss: 3.43701867677737e-05
step: 820, loss: 0.00036385178100317717
step: 830, loss: 0.0251756738871336
step: 840, loss: 0.047787122428417206
step: 850, loss: 0.0008510993211530149
step: 860, loss: 0.021311305463314056
step: 870, loss: 0.0008983692969195545
step: 880, loss: 4.434716174728237e-05
step: 890, loss: 0.00016588701691944152
step: 900, loss: 0.06397880613803864
step: 910, loss: 0.020494719967246056
step: 920, loss: 0.059840984642505646
step: 930, loss: 0.0785849317908287
step: 940, loss: 0.06667212396860123
step: 950, loss: 2.2021309632691555e-05
step: 960, loss: 0.029115233570337296
step: 970, loss: 2.8531991119962186e-05
epoch 18: dev_f1=0.9315960912052118, f1=0.9249193919852603, best_f1=0.9320565435476517
step: 0, loss: 0.045513760298490524
step: 10, loss: 0.07151345163583755
step: 20, loss: 0.018559934571385384
step: 30, loss: 0.017780877649784088
step: 40, loss: 0.023507099598646164
step: 50, loss: 0.023689042776823044
step: 60, loss: 2.3787764803273603e-05
step: 70, loss: 0.043187085539102554
step: 80, loss: 0.04461761564016342
step: 90, loss: 2.1571924662566744e-05
step: 100, loss: 0.11205676943063736
step: 110, loss: 0.11438877135515213
step: 120, loss: 0.0312082227319479
step: 130, loss: 0.07767538726329803
step: 140, loss: 0.07116570323705673
step: 150, loss: 0.0022740671411156654
step: 160, loss: 0.0004300643049646169
step: 170, loss: 3.676673804875463e-05
step: 180, loss: 0.019921857863664627
step: 190, loss: 2.1671467038686387e-05
step: 200, loss: 0.01867518201470375
step: 210, loss: 0.028998104855418205
step: 220, loss: 0.06731286644935608
step: 230, loss: 0.027383185923099518
step: 240, loss: 3.506746361381374e-05
step: 250, loss: 0.017730548977851868
step: 260, loss: 0.04895171523094177
step: 270, loss: 3.762713458854705e-05
step: 280, loss: 0.07511863857507706
step: 290, loss: 0.0009592759306542575
step: 300, loss: 0.0003782327985391021
step: 310, loss: 0.022158877924084663
step: 320, loss: 0.0006482123280875385
step: 330, loss: 0.03677444905042648
step: 340, loss: 0.04314197599887848
step: 350, loss: 0.018483782187104225
step: 360, loss: 0.04114888235926628
step: 370, loss: 0.019419241696596146
step: 380, loss: 0.062094248831272125
step: 390, loss: 0.02665666677057743
step: 400, loss: 0.0026795975863933563
step: 410, loss: 1.3611734175356105e-05
step: 420, loss: 9.98743053060025e-06
step: 430, loss: 0.055120836943387985
step: 440, loss: 0.019634099677205086
step: 450, loss: 0.09160174429416656
step: 460, loss: 0.0842181146144867
step: 470, loss: 0.00011759003245970234
step: 480, loss: 0.0659506693482399
step: 490, loss: 2.255107210658025e-05
step: 500, loss: 0.010769427753984928
step: 510, loss: 0.0009367317543365061
step: 520, loss: 0.04670260474085808
step: 530, loss: 7.401184120681137e-05
step: 540, loss: 0.025122420862317085
step: 550, loss: 0.0002695566799957305
step: 560, loss: 6.450251385103911e-05
step: 570, loss: 8.845888078212738e-05
step: 580, loss: 0.017932888120412827
step: 590, loss: 0.016909487545490265
step: 600, loss: 0.016652468591928482
step: 610, loss: 0.08824840188026428
step: 620, loss: 0.019119055941700935
step: 630, loss: 3.278984149801545e-05
step: 640, loss: 0.02028455212712288
step: 650, loss: 0.026540091261267662
step: 660, loss: 8.277613233076409e-05
step: 670, loss: 3.82857360818889e-05
step: 680, loss: 0.020771274343132973
step: 690, loss: 0.02264248952269554
step: 700, loss: 0.05051584169268608
step: 710, loss: 8.389275899389759e-05
step: 720, loss: 0.020660612732172012
step: 730, loss: 0.057742103934288025
step: 740, loss: 7.4468262027949095e-06
step: 750, loss: 0.017897633835673332
step: 760, loss: 0.05998244881629944
step: 770, loss: 0.04155367612838745
step: 780, loss: 1.377939224767033e-05
step: 790, loss: 0.034735675901174545
step: 800, loss: 3.324143108329736e-05
step: 810, loss: 0.024442704394459724
step: 820, loss: 0.03255594149231911
step: 830, loss: 0.06656760722398758
step: 840, loss: 0.009425263851881027
step: 850, loss: 0.001840067794546485
step: 860, loss: 0.023460524156689644
step: 870, loss: 0.01785711571574211
step: 880, loss: 5.682807750417851e-05
step: 890, loss: 0.03969863802194595
step: 900, loss: 0.00011638217984000221
step: 910, loss: 0.0037197722122073174
step: 920, loss: 0.024381164461374283
step: 930, loss: 2.917652454925701e-05
step: 940, loss: 0.022080352529883385
step: 950, loss: 2.3522625269833952e-05
step: 960, loss: 0.042674608528614044
step: 970, loss: 0.04949238523840904
epoch 19: dev_f1=0.9299482839680301, f1=0.9226467847157502, best_f1=0.9320565435476517
step: 0, loss: 0.02250586450099945
step: 10, loss: 0.024679454043507576
step: 20, loss: 0.07524628937244415
step: 30, loss: 4.517786510405131e-05
step: 40, loss: 0.05046689137816429
step: 50, loss: 0.02143719233572483
step: 60, loss: 0.0007390512037090957
step: 70, loss: 0.02278057113289833
step: 80, loss: 0.02336709573864937
step: 90, loss: 0.020872989669442177
step: 100, loss: 4.5656674046767876e-05
step: 110, loss: 8.289461402455345e-05
step: 120, loss: 0.04332149028778076
step: 130, loss: 0.023617971688508987
step: 140, loss: 0.06307359039783478
step: 150, loss: 0.022010428830981255
step: 160, loss: 0.018808262422680855
step: 170, loss: 0.18893025815486908
step: 180, loss: 0.05473977327346802
step: 190, loss: 5.321572461980395e-05
step: 200, loss: 0.06989248096942902
step: 210, loss: 0.019476033747196198
step: 220, loss: 0.020951898768544197
step: 230, loss: 0.04325120151042938
step: 240, loss: 0.030550368130207062
step: 250, loss: 0.039270929992198944
step: 260, loss: 0.021368952468037605
step: 270, loss: 0.028626039624214172
step: 280, loss: 0.042890746146440506
step: 290, loss: 0.02252238802611828
step: 300, loss: 6.28092820988968e-05
step: 310, loss: 2.1973848561174236e-05
step: 320, loss: 0.012280330993235111
step: 330, loss: 0.021224740892648697
step: 340, loss: 6.378398393280804e-05
step: 350, loss: 8.065319707384333e-05
step: 360, loss: 0.0202786922454834
step: 370, loss: 0.0175179336220026
step: 380, loss: 0.029332498088479042
step: 390, loss: 3.4875974961323664e-05
step: 400, loss: 0.018760455772280693
step: 410, loss: 0.018491478636860847
step: 420, loss: 0.045787300914525986
step: 430, loss: 1.9809474906651303e-05
step: 440, loss: 0.021433386951684952
step: 450, loss: 9.759352542459965e-05
step: 460, loss: 0.050606295466423035
step: 470, loss: 0.07301385700702667
step: 480, loss: 0.09273805469274521
step: 490, loss: 0.06881463527679443
step: 500, loss: 9.813271753955632e-05
step: 510, loss: 0.03778418153524399
step: 520, loss: 0.029706576839089394
step: 530, loss: 0.00011068871390307322
step: 540, loss: 0.03565867990255356
step: 550, loss: 0.019122544676065445
step: 560, loss: 0.08681688457727432
step: 570, loss: 0.08371015638113022
step: 580, loss: 0.030765270814299583
step: 590, loss: 0.042604099959135056
step: 600, loss: 0.03491716459393501
step: 610, loss: 0.015597634017467499
step: 620, loss: 0.06557196378707886
step: 630, loss: 0.048438914120197296
step: 640, loss: 0.06427908688783646
step: 650, loss: 0.04962955415248871
step: 660, loss: 1.8464801541995257e-05
step: 670, loss: 0.016513904556632042
step: 680, loss: 0.05611139535903931
step: 690, loss: 9.669157589087263e-05
step: 700, loss: 0.002103798557072878
step: 710, loss: 0.0066749644465744495
step: 720, loss: 0.001097172498703003
step: 730, loss: 0.04226794093847275
step: 740, loss: 0.02551235444843769
step: 750, loss: 0.0399286225438118
step: 760, loss: 7.417884626192972e-05
step: 770, loss: 0.017850331962108612
step: 780, loss: 0.02037821151316166
step: 790, loss: 0.01203115750104189
step: 800, loss: 0.01065080612897873
step: 810, loss: 0.0646122395992279
step: 820, loss: 0.03462131321430206
step: 830, loss: 0.01971757598221302
step: 840, loss: 2.349680107727181e-05
step: 850, loss: 0.06407896429300308
step: 860, loss: 8.001704554772004e-05
step: 870, loss: 0.021308381110429764
step: 880, loss: 0.04109321907162666
step: 890, loss: 0.12498166412115097
step: 900, loss: 0.0008193834219127893
step: 910, loss: 0.026990938931703568
step: 920, loss: 0.0990230143070221
step: 930, loss: 0.0035869095008820295
step: 940, loss: 0.038119133561849594
step: 950, loss: 0.0025721597485244274
step: 960, loss: 0.0163173396140337
step: 970, loss: 2.1399186152848415e-05
epoch 20: dev_f1=0.9311969839773798, f1=0.921999065857076, best_f1=0.9320565435476517
