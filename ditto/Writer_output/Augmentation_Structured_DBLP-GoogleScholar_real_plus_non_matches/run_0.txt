cuda
Device: cuda
step: 0, loss: 0.4800160229206085
step: 10, loss: 0.2122691124677658
step: 20, loss: 0.5025649666786194
step: 30, loss: 0.18784239888191223
step: 40, loss: 0.30290094017982483
step: 50, loss: 0.06299348920583725
step: 60, loss: 0.33158618211746216
step: 70, loss: 0.05250110104680061
step: 80, loss: 0.11616476625204086
step: 90, loss: 0.22233642637729645
step: 100, loss: 0.34464582800865173
step: 110, loss: 0.14451964199543
step: 120, loss: 0.3303951919078827
step: 130, loss: 0.2156708538532257
step: 140, loss: 0.0954691618680954
step: 150, loss: 0.17124001681804657
step: 160, loss: 0.026786258444190025
step: 170, loss: 0.09061498194932938
step: 180, loss: 0.10407491773366928
step: 190, loss: 0.14436465501785278
step: 200, loss: 0.31870782375335693
step: 210, loss: 0.297020822763443
step: 220, loss: 0.1331276148557663
step: 230, loss: 0.2471463531255722
step: 240, loss: 0.05710665136575699
step: 250, loss: 0.21594461798667908
step: 260, loss: 0.1022079735994339
step: 270, loss: 0.14078737795352936
step: 280, loss: 0.1567617654800415
step: 290, loss: 0.210243359208107
step: 300, loss: 0.18251320719718933
step: 310, loss: 0.12302379310131073
step: 320, loss: 0.12852683663368225
step: 330, loss: 0.15501675009727478
step: 340, loss: 0.11162379384040833
step: 350, loss: 0.22934553027153015
step: 360, loss: 0.12874628603458405
step: 370, loss: 0.12663790583610535
step: 380, loss: 0.10234610736370087
step: 390, loss: 0.11745105683803558
step: 400, loss: 0.08797234296798706
step: 410, loss: 0.0373506098985672
step: 420, loss: 0.17489029467105865
step: 430, loss: 0.1126452311873436
step: 440, loss: 0.15167319774627686
step: 450, loss: 0.22949010133743286
step: 460, loss: 0.22904855012893677
step: 470, loss: 0.12209640443325043
step: 480, loss: 0.07403632253408432
step: 490, loss: 0.025067264214158058
step: 500, loss: 0.11878331750631332
step: 510, loss: 0.16762098670005798
step: 520, loss: 0.1200639009475708
step: 530, loss: 0.06537403166294098
step: 540, loss: 0.12807650864124298
step: 550, loss: 0.09986227750778198
step: 560, loss: 0.01899024285376072
step: 570, loss: 0.12443234771490097
step: 580, loss: 0.20452463626861572
step: 590, loss: 0.03573989123106003
step: 600, loss: 0.17665515840053558
step: 610, loss: 0.012903613969683647
step: 620, loss: 0.12208463251590729
step: 630, loss: 0.01737094856798649
step: 640, loss: 0.17032897472381592
step: 650, loss: 0.09562162309885025
step: 660, loss: 0.12704616785049438
step: 670, loss: 0.24658802151679993
step: 680, loss: 0.16682901978492737
step: 690, loss: 0.05033812299370766
step: 700, loss: 0.13906292617321014
step: 710, loss: 0.0992390364408493
step: 720, loss: 0.1205858513712883
step: 730, loss: 0.12403047829866409
step: 740, loss: 0.02989036962389946
step: 750, loss: 0.04319433122873306
step: 760, loss: 0.11204566806554794
step: 770, loss: 0.10868117213249207
step: 780, loss: 0.1359471082687378
step: 790, loss: 0.13780002295970917
step: 800, loss: 0.09451191872358322
step: 810, loss: 0.10345729440450668
step: 820, loss: 0.10956268757581711
step: 830, loss: 0.0417104996740818
step: 840, loss: 0.01890113763511181
step: 850, loss: 0.07297829538583755
step: 860, loss: 0.10778573900461197
step: 870, loss: 0.08427802473306656
step: 880, loss: 0.09792300313711166
step: 890, loss: 0.13506914675235748
step: 900, loss: 0.014734940603375435
step: 910, loss: 0.19983582198619843
step: 920, loss: 0.16942237317562103
step: 930, loss: 0.07924867421388626
step: 940, loss: 0.07136328518390656
step: 950, loss: 0.03867579251527786
step: 960, loss: 0.15253864228725433
step: 970, loss: 0.03070281445980072
epoch 1: dev_f1=0.9259944495837188, f1=0.9268738574040218, best_f1=0.9268738574040218
step: 0, loss: 0.14508472383022308
step: 10, loss: 0.06646212190389633
step: 20, loss: 0.13667696714401245
step: 30, loss: 0.02791018970310688
step: 40, loss: 0.10216528922319412
step: 50, loss: 0.038098592311143875
step: 60, loss: 0.059874277561903
step: 70, loss: 0.16365493834018707
step: 80, loss: 0.059891730546951294
step: 90, loss: 0.029938315972685814
step: 100, loss: 0.1005389466881752
step: 110, loss: 0.030727766454219818
step: 120, loss: 0.1073022335767746
step: 130, loss: 0.09607300907373428
step: 140, loss: 0.053700920194387436
step: 150, loss: 0.02287585847079754
step: 160, loss: 0.1960834413766861
step: 170, loss: 0.03360786288976669
step: 180, loss: 0.1556701958179474
step: 190, loss: 0.05442841723561287
step: 200, loss: 0.19986939430236816
step: 210, loss: 0.10270059108734131
step: 220, loss: 0.057289838790893555
step: 230, loss: 0.03483255207538605
step: 240, loss: 0.05387643724679947
step: 250, loss: 0.01876685954630375
step: 260, loss: 0.012105440720915794
step: 270, loss: 0.2076645791530609
step: 280, loss: 0.04633011668920517
step: 290, loss: 0.11884184181690216
step: 300, loss: 0.10273415595293045
step: 310, loss: 0.08725050836801529
step: 320, loss: 0.07270374894142151
step: 330, loss: 0.05413787439465523
step: 340, loss: 0.06735092401504517
step: 350, loss: 0.14545764029026031
step: 360, loss: 0.08972886949777603
step: 370, loss: 0.04253118112683296
step: 380, loss: 0.052243318408727646
step: 390, loss: 0.06190960109233856
step: 400, loss: 0.026899268850684166
step: 410, loss: 0.07548115402460098
step: 420, loss: 0.0631605014204979
step: 430, loss: 0.275247186422348
step: 440, loss: 0.029817629605531693
step: 450, loss: 0.13933654129505157
step: 460, loss: 0.12431798875331879
step: 470, loss: 0.09363691508769989
step: 480, loss: 0.17453718185424805
step: 490, loss: 0.11467152088880539
step: 500, loss: 0.020819606259465218
step: 510, loss: 0.06669703871011734
step: 520, loss: 0.019783098250627518
step: 530, loss: 0.044388748705387115
step: 540, loss: 0.08080866932868958
step: 550, loss: 0.0878581628203392
step: 560, loss: 0.10667964816093445
step: 570, loss: 0.05511274188756943
step: 580, loss: 0.09516274183988571
step: 590, loss: 0.08172214776277542
step: 600, loss: 0.12384888529777527
step: 610, loss: 0.10939985513687134
step: 620, loss: 0.20227275788784027
step: 630, loss: 0.07317936420440674
step: 640, loss: 0.14466950297355652
step: 650, loss: 0.04185093939304352
step: 660, loss: 0.015407935716211796
step: 670, loss: 0.09386332333087921
step: 680, loss: 0.17752772569656372
step: 690, loss: 0.2025921493768692
step: 700, loss: 0.004899537656456232
step: 710, loss: 0.009743457660079002
step: 720, loss: 0.03976546972990036
step: 730, loss: 0.12838155031204224
step: 740, loss: 0.13018067181110382
step: 750, loss: 0.022724047303199768
step: 760, loss: 0.13431072235107422
step: 770, loss: 0.07806570082902908
step: 780, loss: 0.0975145548582077
step: 790, loss: 0.06881356984376907
step: 800, loss: 0.04950472712516785
step: 810, loss: 0.12414327263832092
step: 820, loss: 0.06648565083742142
step: 830, loss: 0.012579360976815224
step: 840, loss: 0.02217795141041279
step: 850, loss: 0.06275607645511627
step: 860, loss: 0.04830523952841759
step: 870, loss: 0.1139848604798317
step: 880, loss: 0.1828078180551529
step: 890, loss: 0.09047829359769821
step: 900, loss: 0.09219445288181305
step: 910, loss: 0.14589549601078033
step: 920, loss: 0.16713501513004303
step: 930, loss: 0.05445970594882965
step: 940, loss: 0.04364774003624916
step: 950, loss: 0.0776880607008934
step: 960, loss: 0.08509615063667297
step: 970, loss: 0.029263509437441826
epoch 2: dev_f1=0.9239280774550485, f1=0.9296551724137931, best_f1=0.9268738574040218
step: 0, loss: 0.09816812723875046
step: 10, loss: 0.03552323207259178
step: 20, loss: 0.22500301897525787
step: 30, loss: 0.009225201793015003
step: 40, loss: 0.02622903883457184
step: 50, loss: 0.05413414537906647
step: 60, loss: 0.06494007259607315
step: 70, loss: 0.053816765546798706
step: 80, loss: 0.0611119270324707
step: 90, loss: 0.035036079585552216
step: 100, loss: 0.1179276630282402
step: 110, loss: 0.09768765419721603
step: 120, loss: 0.15180087089538574
step: 130, loss: 0.13454973697662354
step: 140, loss: 0.045677557587623596
step: 150, loss: 0.02041397988796234
step: 160, loss: 0.23464316129684448
step: 170, loss: 0.04400772973895073
step: 180, loss: 0.06658139824867249
step: 190, loss: 0.03755336254835129
step: 200, loss: 0.010971369221806526
step: 210, loss: 0.017339522019028664
step: 220, loss: 0.02180790714919567
step: 230, loss: 0.06435812264680862
step: 240, loss: 0.17180877923965454
step: 250, loss: 0.019769903272390366
step: 260, loss: 0.11388951539993286
step: 270, loss: 0.016569511964917183
step: 280, loss: 0.020785890519618988
step: 290, loss: 0.27216535806655884
step: 300, loss: 0.08259597420692444
step: 310, loss: 0.17505143582820892
step: 320, loss: 0.14231128990650177
step: 330, loss: 0.09885949641466141
step: 340, loss: 0.13337598741054535
step: 350, loss: 0.010917307808995247
step: 360, loss: 0.07885303348302841
step: 370, loss: 0.2892444133758545
step: 380, loss: 0.0851466953754425
step: 390, loss: 0.12720103561878204
step: 400, loss: 0.06321325153112411
step: 410, loss: 0.02499479241669178
step: 420, loss: 0.06428468972444534
step: 430, loss: 0.08159791678190231
step: 440, loss: 0.09598110616207123
step: 450, loss: 0.015567932277917862
step: 460, loss: 0.08490961045026779
step: 470, loss: 0.10028257220983505
step: 480, loss: 0.057448577135801315
step: 490, loss: 0.06587500870227814
step: 500, loss: 0.10608383268117905
step: 510, loss: 0.11359000951051712
step: 520, loss: 0.08965538442134857
step: 530, loss: 0.1262006163597107
step: 540, loss: 0.1420867145061493
step: 550, loss: 0.01568906381726265
step: 560, loss: 0.13768963515758514
step: 570, loss: 0.08819041401147842
step: 580, loss: 0.08785919100046158
step: 590, loss: 0.06755436956882477
step: 600, loss: 0.03642791509628296
step: 610, loss: 0.08211975544691086
step: 620, loss: 0.061724480241537094
step: 630, loss: 0.18190975487232208
step: 640, loss: 0.07531896233558655
step: 650, loss: 0.07739060372114182
step: 660, loss: 0.1364523470401764
step: 670, loss: 0.05412261188030243
step: 680, loss: 0.074697345495224
step: 690, loss: 0.07517675310373306
step: 700, loss: 0.1289961040019989
step: 710, loss: 0.16862879693508148
step: 720, loss: 0.10673021525144577
step: 730, loss: 0.16608726978302002
step: 740, loss: 0.15426014363765717
step: 750, loss: 0.10987092554569244
step: 760, loss: 0.12077957391738892
step: 770, loss: 0.05453911051154137
step: 780, loss: 0.06609256565570831
step: 790, loss: 0.13397087156772614
step: 800, loss: 0.1378013640642166
step: 810, loss: 0.08583477139472961
step: 820, loss: 0.07705716788768768
step: 830, loss: 0.06269118189811707
step: 840, loss: 0.025462325662374496
step: 850, loss: 0.01871647499501705
step: 860, loss: 0.013583341613411903
step: 870, loss: 0.09836436808109283
step: 880, loss: 0.12616027891635895
step: 890, loss: 0.22576911747455597
step: 900, loss: 0.09249403327703476
step: 910, loss: 0.07913555949926376
step: 920, loss: 0.07601610571146011
step: 930, loss: 0.042294055223464966
step: 940, loss: 0.10627949237823486
step: 950, loss: 0.06660924106836319
step: 960, loss: 0.03895656019449234
step: 970, loss: 0.32721421122550964
epoch 3: dev_f1=0.9296037296037295, f1=0.9358736059479555, best_f1=0.9358736059479555
step: 0, loss: 0.04976831376552582
step: 10, loss: 0.0299795251339674
step: 20, loss: 0.09814319759607315
step: 30, loss: 0.016795454546809196
step: 40, loss: 0.0145781971514225
step: 50, loss: 0.043200042098760605
step: 60, loss: 0.051407650113105774
step: 70, loss: 0.12256523221731186
step: 80, loss: 0.06767362356185913
step: 90, loss: 0.1299402266740799
step: 100, loss: 0.028440315276384354
step: 110, loss: 0.039277784526348114
step: 120, loss: 0.03159382566809654
step: 130, loss: 0.052446238696575165
step: 140, loss: 0.06084275245666504
step: 150, loss: 0.2303255796432495
step: 160, loss: 0.028039349243044853
step: 170, loss: 0.019542619585990906
step: 180, loss: 0.03986747935414314
step: 190, loss: 0.13937023282051086
step: 200, loss: 0.27036842703819275
step: 210, loss: 0.01829521544277668
step: 220, loss: 0.08116404712200165
step: 230, loss: 0.16012358665466309
step: 240, loss: 0.03527059778571129
step: 250, loss: 0.01330255065113306
step: 260, loss: 0.02056007832288742
step: 270, loss: 0.08026086539030075
step: 280, loss: 0.1296079158782959
step: 290, loss: 0.11908753961324692
step: 300, loss: 0.051350656896829605
step: 310, loss: 0.17820896208286285
step: 320, loss: 0.09290198236703873
step: 330, loss: 0.025322020053863525
step: 340, loss: 0.09444785863161087
step: 350, loss: 0.14946293830871582
step: 360, loss: 0.07038663327693939
step: 370, loss: 0.07052791118621826
step: 380, loss: 0.07966584712266922
step: 390, loss: 0.03502173721790314
step: 400, loss: 0.008196641691029072
step: 410, loss: 0.15473340451717377
step: 420, loss: 0.05195467546582222
step: 430, loss: 0.0780961811542511
step: 440, loss: 0.06463252007961273
step: 450, loss: 0.10616713762283325
step: 460, loss: 0.048028476536273956
step: 470, loss: 0.1144460067152977
step: 480, loss: 0.12484937161207199
step: 490, loss: 0.014162305742502213
step: 500, loss: 0.1292470544576645
step: 510, loss: 0.04798782989382744
step: 520, loss: 0.09282399713993073
step: 530, loss: 0.009357793256640434
step: 540, loss: 0.05317821353673935
step: 550, loss: 0.03822433575987816
step: 560, loss: 0.02064337208867073
step: 570, loss: 0.06865157932043076
step: 580, loss: 0.07115231454372406
step: 590, loss: 0.02591489814221859
step: 600, loss: 0.048519548028707504
step: 610, loss: 0.00015535327838733792
step: 620, loss: 0.0329207181930542
step: 630, loss: 0.023275382816791534
step: 640, loss: 0.06472929567098618
step: 650, loss: 0.06770964711904526
step: 660, loss: 0.025598660111427307
step: 670, loss: 0.09513621032238007
step: 680, loss: 0.035522107034921646
step: 690, loss: 0.054898809641599655
step: 700, loss: 0.10683970153331757
step: 710, loss: 0.13887012004852295
step: 720, loss: 0.004443347454071045
step: 730, loss: 0.03407313674688339
step: 740, loss: 0.025049377232789993
step: 750, loss: 0.057461753487586975
step: 760, loss: 0.26209044456481934
step: 770, loss: 0.06921713799238205
step: 780, loss: 0.03314530849456787
step: 790, loss: 0.019576510414481163
step: 800, loss: 0.03305831179022789
step: 810, loss: 0.037475451827049255
step: 820, loss: 0.1426927000284195
step: 830, loss: 0.16817335784435272
step: 840, loss: 0.022022299468517303
step: 850, loss: 0.027746867388486862
step: 860, loss: 0.014267480932176113
step: 870, loss: 0.060513924807310104
step: 880, loss: 0.014974802732467651
step: 890, loss: 0.07959108799695969
step: 900, loss: 0.0450545959174633
step: 910, loss: 0.1857868880033493
step: 920, loss: 0.13407452404499054
step: 930, loss: 0.08507587015628815
step: 940, loss: 0.04334428906440735
step: 950, loss: 0.010552548803389072
step: 960, loss: 0.08736734837293625
step: 970, loss: 0.12263770401477814
epoch 4: dev_f1=0.9341983317886932, f1=0.9341317365269461, best_f1=0.9341317365269461
step: 0, loss: 0.05965631827712059
step: 10, loss: 0.05576494708657265
step: 20, loss: 0.03801271691918373
step: 30, loss: 0.10283531248569489
step: 40, loss: 0.02986249141395092
step: 50, loss: 0.07146041840314865
step: 60, loss: 0.07454761862754822
step: 70, loss: 0.08651445806026459
step: 80, loss: 0.007007174659520388
step: 90, loss: 0.0352107398211956
step: 100, loss: 0.018223734572529793
step: 110, loss: 0.03347468748688698
step: 120, loss: 0.01953156851232052
step: 130, loss: 0.013644302263855934
step: 140, loss: 0.02680400386452675
step: 150, loss: 0.012463588267564774
step: 160, loss: 0.01499495841562748
step: 170, loss: 0.008567871525883675
step: 180, loss: 0.041086435317993164
step: 190, loss: 0.08379730582237244
step: 200, loss: 0.03592697158455849
step: 210, loss: 0.016570448875427246
step: 220, loss: 0.05627018213272095
step: 230, loss: 0.0019200784154236317
step: 240, loss: 0.1155766025185585
step: 250, loss: 0.11852853000164032
step: 260, loss: 0.14738599956035614
step: 270, loss: 0.03760392963886261
step: 280, loss: 0.06867353618144989
step: 290, loss: 0.051487669348716736
step: 300, loss: 0.04417050629854202
step: 310, loss: 0.10088858008384705
step: 320, loss: 0.0258113332092762
step: 330, loss: 0.061687108129262924
step: 340, loss: 0.15556097030639648
step: 350, loss: 0.07062963396310806
step: 360, loss: 0.03552471473813057
step: 370, loss: 0.028131620958447456
step: 380, loss: 0.037311576306819916
step: 390, loss: 0.0731954500079155
step: 400, loss: 0.013921145349740982
step: 410, loss: 0.07431583106517792
step: 420, loss: 0.1774715632200241
step: 430, loss: 0.01889999397099018
step: 440, loss: 0.03976035118103027
step: 450, loss: 0.006272533908486366
step: 460, loss: 0.06088372692465782
step: 470, loss: 0.2932855486869812
step: 480, loss: 0.005775758996605873
step: 490, loss: 0.2474658489227295
step: 500, loss: 0.019698452204465866
step: 510, loss: 0.02617155760526657
step: 520, loss: 0.02166120521724224
step: 530, loss: 0.09011378139257431
step: 540, loss: 0.07493512332439423
step: 550, loss: 0.001114692073315382
step: 560, loss: 0.11235625296831131
step: 570, loss: 0.0999038964509964
step: 580, loss: 0.0316309928894043
step: 590, loss: 0.019459687173366547
step: 600, loss: 0.03095843270421028
step: 610, loss: 0.06867486983537674
step: 620, loss: 0.14860156178474426
step: 630, loss: 0.007933462038636208
step: 640, loss: 0.015469536185264587
step: 650, loss: 0.06099030375480652
step: 660, loss: 0.03914780542254448
step: 670, loss: 0.12075412273406982
step: 680, loss: 0.0763871967792511
step: 690, loss: 0.15422756969928741
step: 700, loss: 0.006093299016356468
step: 710, loss: 0.09868162125349045
step: 720, loss: 0.02635342627763748
step: 730, loss: 0.004597118590027094
step: 740, loss: 0.014297563582658768
step: 750, loss: 0.011862272396683693
step: 760, loss: 0.020900826901197433
step: 770, loss: 0.10297621041536331
step: 780, loss: 0.03872549161314964
step: 790, loss: 0.13413767516613007
step: 800, loss: 0.05455968156456947
step: 810, loss: 0.05216655880212784
step: 820, loss: 0.03618646040558815
step: 830, loss: 0.17476026713848114
step: 840, loss: 0.15147824585437775
step: 850, loss: 0.016934357583522797
step: 860, loss: 0.023667629808187485
step: 870, loss: 0.030011625960469246
step: 880, loss: 0.006001572124660015
step: 890, loss: 0.06930513679981232
step: 900, loss: 0.09394318610429764
step: 910, loss: 0.09358880668878555
step: 920, loss: 0.021729860454797745
step: 930, loss: 0.05462217703461647
step: 940, loss: 0.0320139043033123
step: 950, loss: 0.05724703148007393
step: 960, loss: 0.07650768011808395
step: 970, loss: 0.042749155312776566
epoch 5: dev_f1=0.9353369763205829, f1=0.9354545454545454, best_f1=0.9354545454545454
step: 0, loss: 0.03755868598818779
step: 10, loss: 0.03710184618830681
step: 20, loss: 0.09931829571723938
step: 30, loss: 0.07928095757961273
step: 40, loss: 0.018610142171382904
step: 50, loss: 0.003871383611112833
step: 60, loss: 0.09538803994655609
step: 70, loss: 0.08431731909513474
step: 80, loss: 0.0520390085875988
step: 90, loss: 0.0906195193529129
step: 100, loss: 0.00847988948225975
step: 110, loss: 0.10169074684381485
step: 120, loss: 0.07705992460250854
step: 130, loss: 0.15234048664569855
step: 140, loss: 0.05391892418265343
step: 150, loss: 0.021361367776989937
step: 160, loss: 0.045761704444885254
step: 170, loss: 0.002928478177636862
step: 180, loss: 0.023838259279727936
step: 190, loss: 0.01539916917681694
step: 200, loss: 0.009130429476499557
step: 210, loss: 0.08691637217998505
step: 220, loss: 0.053531330078840256
step: 230, loss: 0.03712383657693863
step: 240, loss: 0.023438695818185806
step: 250, loss: 0.013085326179862022
step: 260, loss: 0.020007001236081123
step: 270, loss: 0.07080084830522537
step: 280, loss: 0.11791971325874329
step: 290, loss: 0.006380593869835138
step: 300, loss: 0.1332131326198578
step: 310, loss: 0.06567484885454178
step: 320, loss: 0.10828110575675964
step: 330, loss: 0.018687231466174126
step: 340, loss: 0.01701812446117401
step: 350, loss: 0.15547771751880646
step: 360, loss: 0.025553729385137558
step: 370, loss: 0.0662970095872879
step: 380, loss: 0.08229825645685196
step: 390, loss: 0.03168664127588272
step: 400, loss: 0.17690449953079224
step: 410, loss: 0.02915981598198414
step: 420, loss: 0.14479397237300873
step: 430, loss: 0.051193006336688995
step: 440, loss: 0.10648351162672043
step: 450, loss: 0.008773761801421642
step: 460, loss: 0.30844229459762573
step: 470, loss: 0.030764250084757805
step: 480, loss: 0.05273931473493576
step: 490, loss: 0.0457582101225853
step: 500, loss: 0.0743216872215271
step: 510, loss: 0.09005079418420792
step: 520, loss: 0.05302955210208893
step: 530, loss: 0.12194067984819412
step: 540, loss: 0.005558381788432598
step: 550, loss: 0.07878580689430237
step: 560, loss: 0.18732883036136627
step: 570, loss: 0.10187897831201553
step: 580, loss: 0.1148306131362915
step: 590, loss: 0.05415375903248787
step: 600, loss: 0.010320533066987991
step: 610, loss: 0.06921985000371933
step: 620, loss: 0.08650492876768112
step: 630, loss: 0.05316910147666931
step: 640, loss: 0.11134784668684006
step: 650, loss: 0.08183205872774124
step: 660, loss: 0.14536002278327942
step: 670, loss: 0.11239435523748398
step: 680, loss: 0.011442640796303749
step: 690, loss: 0.01324812788516283
step: 700, loss: 0.09294456988573074
step: 710, loss: 0.09077851474285126
step: 720, loss: 0.0257458183914423
step: 730, loss: 0.11590541154146194
step: 740, loss: 0.019740162417292595
step: 750, loss: 0.027220850810408592
step: 760, loss: 0.055396389216184616
step: 770, loss: 0.10242903232574463
step: 780, loss: 0.05552464351058006
step: 790, loss: 0.03270377218723297
step: 800, loss: 0.1298576444387436
step: 810, loss: 0.04906875267624855
step: 820, loss: 0.033289410173892975
step: 830, loss: 0.019340451806783676
step: 840, loss: 0.06719326972961426
step: 850, loss: 0.13229447603225708
step: 860, loss: 0.0632406547665596
step: 870, loss: 0.11326658725738525
step: 880, loss: 0.10240663588047028
step: 890, loss: 0.09703676402568817
step: 900, loss: 0.006448222789913416
step: 910, loss: 0.005284079350531101
step: 920, loss: 0.002958374097943306
step: 930, loss: 0.1443660855293274
step: 940, loss: 0.08566537499427795
step: 950, loss: 0.12293985486030579
step: 960, loss: 0.08656007796525955
step: 970, loss: 0.0058437190018594265
epoch 6: dev_f1=0.9341923607915326, f1=0.9398704902867714, best_f1=0.9354545454545454
step: 0, loss: 0.027564380317926407
step: 10, loss: 0.02861165814101696
step: 20, loss: 0.06671369820833206
step: 30, loss: 0.07352099567651749
step: 40, loss: 0.2206009477376938
step: 50, loss: 0.008658770471811295
step: 60, loss: 0.032582275569438934
step: 70, loss: 0.06305678933858871
step: 80, loss: 0.0029091385658830404
step: 90, loss: 0.08210618048906326
step: 100, loss: 0.05475390702486038
step: 110, loss: 0.01597684435546398
step: 120, loss: 0.025228086858987808
step: 130, loss: 0.05684443563222885
step: 140, loss: 0.07021200656890869
step: 150, loss: 0.06835968792438507
step: 160, loss: 0.10963745415210724
step: 170, loss: 0.057142019271850586
step: 180, loss: 0.10657677054405212
step: 190, loss: 0.02802985906600952
step: 200, loss: 0.01730058342218399
step: 210, loss: 0.10774774104356766
step: 220, loss: 0.0008242428302764893
step: 230, loss: 0.009972437284886837
step: 240, loss: 0.037915728986263275
step: 250, loss: 0.03625226393342018
step: 260, loss: 0.14361049234867096
step: 270, loss: 0.03547564521431923
step: 280, loss: 0.10656749457120895
step: 290, loss: 0.07759595662355423
step: 300, loss: 0.07165504992008209
step: 310, loss: 0.017921429127454758
step: 320, loss: 0.06511705368757248
step: 330, loss: 0.01414400152862072
step: 340, loss: 0.0548190213739872
step: 350, loss: 0.022333962842822075
step: 360, loss: 0.112195685505867
step: 370, loss: 0.0745057463645935
step: 380, loss: 0.030071867629885674
step: 390, loss: 0.10829710960388184
step: 400, loss: 0.1392492949962616
step: 410, loss: 0.05845706909894943
step: 420, loss: 0.04643436148762703
step: 430, loss: 0.03957205265760422
step: 440, loss: 0.011567467823624611
step: 450, loss: 0.12866917252540588
step: 460, loss: 0.01690409705042839
step: 470, loss: 0.19345225393772125
step: 480, loss: 0.06024406850337982
step: 490, loss: 0.09595482796430588
step: 500, loss: 0.056784991174936295
step: 510, loss: 0.07312501221895218
step: 520, loss: 0.09098386019468307
step: 530, loss: 0.10533065348863602
step: 540, loss: 0.08409363031387329
step: 550, loss: 0.05120513588190079
step: 560, loss: 0.03863045573234558
step: 570, loss: 0.09718838334083557
step: 580, loss: 0.0656326562166214
step: 590, loss: 0.1460612416267395
step: 600, loss: 0.06442374736070633
step: 610, loss: 0.01767977513372898
step: 620, loss: 0.018313856795430183
step: 630, loss: 0.004004080779850483
step: 640, loss: 0.013172418810427189
step: 650, loss: 0.012837028130888939
step: 660, loss: 0.01863936148583889
step: 670, loss: 0.013306529261171818
step: 680, loss: 0.0024852242786437273
step: 690, loss: 0.09097936749458313
step: 700, loss: 0.13069838285446167
step: 710, loss: 0.09802183508872986
step: 720, loss: 0.036901701241731644
step: 730, loss: 0.06295406818389893
step: 740, loss: 0.026095595210790634
step: 750, loss: 0.12463218718767166
step: 760, loss: 0.01471045520156622
step: 770, loss: 0.01755261793732643
step: 780, loss: 0.029046356678009033
step: 790, loss: 0.03608901798725128
step: 800, loss: 0.16364793479442596
step: 810, loss: 0.1269676685333252
step: 820, loss: 0.06111880764365196
step: 830, loss: 0.02158856764435768
step: 840, loss: 0.09487680345773697
step: 850, loss: 0.05569962412118912
step: 860, loss: 0.13536320626735687
step: 870, loss: 0.09476523846387863
step: 880, loss: 0.038861315697431564
step: 890, loss: 0.017058569937944412
step: 900, loss: 0.09488631039857864
step: 910, loss: 0.10643132030963898
step: 920, loss: 0.037469811737537384
step: 930, loss: 0.09585833549499512
step: 940, loss: 0.022983916103839874
step: 950, loss: 0.012834569439291954
step: 960, loss: 0.0792950838804245
step: 970, loss: 0.04272759333252907
epoch 7: dev_f1=0.9348025711662074, f1=0.93646408839779, best_f1=0.9354545454545454
step: 0, loss: 0.10256940871477127
step: 10, loss: 0.03706253319978714
step: 20, loss: 0.06173308938741684
step: 30, loss: 0.010085240937769413
step: 40, loss: 0.07988151162862778
step: 50, loss: 0.051509469747543335
step: 60, loss: 0.005384831689298153
step: 70, loss: 0.1133517473936081
step: 80, loss: 0.02293987199664116
step: 90, loss: 0.1404462605714798
step: 100, loss: 0.08397401124238968
step: 110, loss: 0.010410669259727001
step: 120, loss: 0.012316811829805374
step: 130, loss: 0.09154649078845978
step: 140, loss: 0.06886911392211914
step: 150, loss: 0.060017332434654236
step: 160, loss: 0.10090281069278717
step: 170, loss: 0.01658850722014904
step: 180, loss: 0.019228212535381317
step: 190, loss: 0.015612278133630753
step: 200, loss: 0.07516245543956757
step: 210, loss: 0.07245617359876633
step: 220, loss: 0.031191667541861534
step: 230, loss: 0.008725274354219437
step: 240, loss: 0.015570946037769318
step: 250, loss: 0.014461377635598183
step: 260, loss: 0.07794906198978424
step: 270, loss: 0.07657895237207413
step: 280, loss: 0.15073782205581665
step: 290, loss: 0.08861624449491501
step: 300, loss: 0.01887415163218975
step: 310, loss: 0.10419114679098129
step: 320, loss: 0.08273475617170334
step: 330, loss: 0.08100463449954987
step: 340, loss: 0.1495790332555771
step: 350, loss: 0.025971831753849983
step: 360, loss: 0.014993859454989433
step: 370, loss: 0.016816427931189537
step: 380, loss: 0.01537221111357212
step: 390, loss: 0.17390108108520508
step: 400, loss: 0.0903378576040268
step: 410, loss: 0.10035762190818787
step: 420, loss: 0.07201743870973587
step: 430, loss: 0.07778453081846237
step: 440, loss: 0.058445412665605545
step: 450, loss: 0.05713113397359848
step: 460, loss: 0.015512918122112751
step: 470, loss: 0.02050046995282173
step: 480, loss: 0.019665170460939407
step: 490, loss: 0.03594904765486717
step: 500, loss: 0.014047078788280487
step: 510, loss: 0.0627688616514206
step: 520, loss: 0.037748225033283234
step: 530, loss: 0.06135587766766548
step: 540, loss: 0.026173753663897514
step: 550, loss: 0.007115515880286694
step: 560, loss: 0.04211484268307686
step: 570, loss: 0.06304868310689926
step: 580, loss: 0.0656265839934349
step: 590, loss: 0.08430816978216171
step: 600, loss: 0.005657353438436985
step: 610, loss: 0.0598054975271225
step: 620, loss: 0.0893218144774437
step: 630, loss: 0.017960038036108017
step: 640, loss: 0.08488622307777405
step: 650, loss: 0.03274783864617348
step: 660, loss: 0.07541530579328537
step: 670, loss: 0.19984978437423706
step: 680, loss: 0.15708528459072113
step: 690, loss: 0.013163583353161812
step: 700, loss: 0.027897639200091362
step: 710, loss: 0.028995515778660774
step: 720, loss: 0.12659715116024017
step: 730, loss: 0.008886482566595078
step: 740, loss: 0.004110054578632116
step: 750, loss: 0.06789347529411316
step: 760, loss: 0.02075490914285183
step: 770, loss: 0.06490729749202728
step: 780, loss: 0.017542151734232903
step: 790, loss: 0.03318480774760246
step: 800, loss: 0.039758916944265366
step: 810, loss: 0.029567673802375793
step: 820, loss: 0.02953079156577587
step: 830, loss: 0.025700142607092857
step: 840, loss: 0.02076549455523491
step: 850, loss: 0.021519150584936142
step: 860, loss: 0.026035413146018982
step: 870, loss: 0.09676900506019592
step: 880, loss: 0.032337550073862076
step: 890, loss: 0.0036135839764028788
step: 900, loss: 0.09324155002832413
step: 910, loss: 0.04879438504576683
step: 920, loss: 0.018398407846689224
step: 930, loss: 0.03593429550528526
step: 940, loss: 0.018553806468844414
step: 950, loss: 0.08386644721031189
step: 960, loss: 0.06989681720733643
step: 970, loss: 0.09780732542276382
epoch 8: dev_f1=0.937847866419295, f1=0.9353187529083293, best_f1=0.9353187529083293
step: 0, loss: 0.06907245516777039
step: 10, loss: 0.07588852196931839
step: 20, loss: 0.0048943450674414635
step: 30, loss: 0.0751049816608429
step: 40, loss: 0.013621172867715359
step: 50, loss: 0.04276411235332489
step: 60, loss: 0.0006753269699402153
step: 70, loss: 0.03984355553984642
step: 80, loss: 0.08910234272480011
step: 90, loss: 0.041854310780763626
step: 100, loss: 0.007828178815543652
step: 110, loss: 0.0626656785607338
step: 120, loss: 0.173806831240654
step: 130, loss: 0.012835373170673847
step: 140, loss: 0.006807385943830013
step: 150, loss: 0.12237457931041718
step: 160, loss: 0.08562235534191132
step: 170, loss: 0.028474930673837662
step: 180, loss: 0.06242818385362625
step: 190, loss: 0.025642693042755127
step: 200, loss: 0.07502754777669907
step: 210, loss: 0.05130217224359512
step: 220, loss: 0.05309085547924042
step: 230, loss: 0.044613081961870193
step: 240, loss: 0.09066233038902283
step: 250, loss: 0.033475443720817566
step: 260, loss: 0.08388573676347733
step: 270, loss: 0.03588264808058739
step: 280, loss: 0.011527957394719124
step: 290, loss: 0.1011812761425972
step: 300, loss: 0.1086997389793396
step: 310, loss: 0.06712730973958969
step: 320, loss: 0.03192651644349098
step: 330, loss: 0.14119195938110352
step: 340, loss: 0.01991169899702072
step: 350, loss: 0.0036448787432163954
step: 360, loss: 0.02858118526637554
step: 370, loss: 0.005338637158274651
step: 380, loss: 0.012935352511703968
step: 390, loss: 0.12194135040044785
step: 400, loss: 0.14706823229789734
step: 410, loss: 0.008858615532517433
step: 420, loss: 0.010255418717861176
step: 430, loss: 0.07408886402845383
step: 440, loss: 0.004794630687683821
step: 450, loss: 0.011361398734152317
step: 460, loss: 0.05433628335595131
step: 470, loss: 0.02687142603099346
step: 480, loss: 0.12496225535869598
step: 490, loss: 0.0075811101123690605
step: 500, loss: 0.02209017053246498
step: 510, loss: 0.05937383696436882
step: 520, loss: 0.003981804009526968
step: 530, loss: 0.06358560919761658
step: 540, loss: 0.018673179671168327
step: 550, loss: 0.08514905720949173
step: 560, loss: 0.08754923194646835
step: 570, loss: 0.042908526957035065
step: 580, loss: 0.03252022713422775
step: 590, loss: 0.041446149349212646
step: 600, loss: 0.11406920105218887
step: 610, loss: 0.08061139285564423
step: 620, loss: 0.025655895471572876
step: 630, loss: 0.10712329298257828
step: 640, loss: 0.021695226430892944
step: 650, loss: 0.004516075365245342
step: 660, loss: 0.03036925196647644
step: 670, loss: 0.005567406304180622
step: 680, loss: 0.04087038338184357
step: 690, loss: 0.0025221111718565226
step: 700, loss: 0.04482441022992134
step: 710, loss: 0.01824592798948288
step: 720, loss: 0.06881264597177505
step: 730, loss: 0.014571811072528362
step: 740, loss: 0.005800370126962662
step: 750, loss: 0.05858471244573593
step: 760, loss: 0.012336095795035362
step: 770, loss: 0.009557022713124752
step: 780, loss: 0.0656149685382843
step: 790, loss: 0.05519717559218407
step: 800, loss: 0.046842310577631
step: 810, loss: 0.008721952326595783
step: 820, loss: 0.02673085406422615
step: 830, loss: 0.08229434490203857
step: 840, loss: 0.08825536072254181
step: 850, loss: 0.002829380566254258
step: 860, loss: 0.026868417859077454
step: 870, loss: 0.013447029516100883
step: 880, loss: 0.03013993613421917
step: 890, loss: 0.021038683131337166
step: 900, loss: 0.07874014973640442
step: 910, loss: 0.10530037432909012
step: 920, loss: 0.06624879688024521
step: 930, loss: 0.08103754371404648
step: 940, loss: 0.09030389785766602
step: 950, loss: 0.0061766160652041435
step: 960, loss: 0.07363833487033844
step: 970, loss: 0.06597808003425598
epoch 9: dev_f1=0.9335810496980957, f1=0.9314942528735632, best_f1=0.9353187529083293
step: 0, loss: 0.04091670364141464
step: 10, loss: 0.13158586621284485
step: 20, loss: 0.0834374725818634
step: 30, loss: 0.044346883893013
step: 40, loss: 0.002524671610444784
step: 50, loss: 0.06345747411251068
step: 60, loss: 0.09764459729194641
step: 70, loss: 0.004601237364113331
step: 80, loss: 0.05236544832587242
step: 90, loss: 0.00406182836741209
step: 100, loss: 0.006635613739490509
step: 110, loss: 0.0011467961594462395
step: 120, loss: 0.010158969089388847
step: 130, loss: 0.03492583706974983
step: 140, loss: 0.0592544861137867
step: 150, loss: 0.01734769530594349
step: 160, loss: 0.002880462910979986
step: 170, loss: 0.016066016629338264
step: 180, loss: 0.07969115674495697
step: 190, loss: 0.013153735548257828
step: 200, loss: 0.06931240856647491
step: 210, loss: 0.07068853080272675
step: 220, loss: 0.01340918056666851
step: 230, loss: 0.0724034234881401
step: 240, loss: 0.03529047593474388
step: 250, loss: 0.1444835215806961
step: 260, loss: 0.08944239467382431
step: 270, loss: 0.05563503876328468
step: 280, loss: 0.03498053550720215
step: 290, loss: 0.0006228852435015142
step: 300, loss: 0.0005850816378369927
step: 310, loss: 0.011356962844729424
step: 320, loss: 0.019772063940763474
step: 330, loss: 0.022111831232905388
step: 340, loss: 0.049206286668777466
step: 350, loss: 0.0945650264620781
step: 360, loss: 0.044221118092536926
step: 370, loss: 0.030926145613193512
step: 380, loss: 0.07456715404987335
step: 390, loss: 0.041979629546403885
step: 400, loss: 0.1242678239941597
step: 410, loss: 0.01612219400703907
step: 420, loss: 0.13738489151000977
step: 430, loss: 0.05587462708353996
step: 440, loss: 0.0009038639254868031
step: 450, loss: 0.010402539744973183
step: 460, loss: 0.0029851989820599556
step: 470, loss: 0.00773004163056612
step: 480, loss: 0.1738111674785614
step: 490, loss: 0.05979936197400093
step: 500, loss: 0.05394414812326431
step: 510, loss: 0.041361261159181595
step: 520, loss: 0.0015192264690995216
step: 530, loss: 0.06118512526154518
step: 540, loss: 0.08370434492826462
step: 550, loss: 0.11283019930124283
step: 560, loss: 0.0344684012234211
step: 570, loss: 0.017846819013357162
step: 580, loss: 0.06893862038850784
step: 590, loss: 0.04453181475400925
step: 600, loss: 0.0009770543547347188
step: 610, loss: 0.00614309124648571
step: 620, loss: 0.02086220681667328
step: 630, loss: 0.015450351871550083
step: 640, loss: 0.10904431343078613
step: 650, loss: 0.025747500360012054
step: 660, loss: 0.020753217861056328
step: 670, loss: 0.20831507444381714
step: 680, loss: 0.07549747079610825
step: 690, loss: 0.0905960202217102
step: 700, loss: 0.03855613246560097
step: 710, loss: 0.016769403591752052
step: 720, loss: 0.022779719904065132
step: 730, loss: 0.05332765728235245
step: 740, loss: 0.039166852831840515
step: 750, loss: 0.020989572629332542
step: 760, loss: 0.09192079305648804
step: 770, loss: 0.002196480520069599
step: 780, loss: 0.014723896980285645
step: 790, loss: 0.20587602257728577
step: 800, loss: 0.05674698203802109
step: 810, loss: 0.019292928278446198
step: 820, loss: 0.032994356006383896
step: 830, loss: 0.07666780054569244
step: 840, loss: 0.026094339787960052
step: 850, loss: 0.011293189600110054
step: 860, loss: 0.07848057895898819
step: 870, loss: 0.0036910695489495993
step: 880, loss: 0.24255439639091492
step: 890, loss: 0.011620650067925453
step: 900, loss: 0.04039705917239189
step: 910, loss: 0.02356003411114216
step: 920, loss: 0.0474214106798172
step: 930, loss: 0.03329212963581085
step: 940, loss: 0.03348483517765999
step: 950, loss: 0.0018103780457749963
step: 960, loss: 0.03560960292816162
step: 970, loss: 0.06308924406766891
epoch 10: dev_f1=0.9314179796107508, f1=0.9314814814814815, best_f1=0.9353187529083293
step: 0, loss: 0.015334010124206543
step: 10, loss: 0.06755276769399643
step: 20, loss: 0.061856526881456375
step: 30, loss: 0.05407502129673958
step: 40, loss: 0.03527949005365372
step: 50, loss: 0.011028037406504154
step: 60, loss: 0.024614660069346428
step: 70, loss: 0.039334531873464584
step: 80, loss: 0.04333548992872238
step: 90, loss: 0.06653346866369247
step: 100, loss: 0.013581979088485241
step: 110, loss: 0.05805826932191849
step: 120, loss: 0.09182967990636826
step: 130, loss: 0.005143826827406883
step: 140, loss: 0.01656949147582054
step: 150, loss: 0.04020955041050911
step: 160, loss: 0.08035094290971756
step: 170, loss: 0.011348606087267399
step: 180, loss: 0.01140533946454525
step: 190, loss: 0.06479544192552567
step: 200, loss: 0.08304998278617859
step: 210, loss: 0.06599435955286026
step: 220, loss: 0.030458221212029457
step: 230, loss: 0.07581396400928497
step: 240, loss: 0.02411848120391369
step: 250, loss: 0.0017540294211357832
step: 260, loss: 0.019826503470540047
step: 270, loss: 0.001508776331320405
step: 280, loss: 0.00025223460397683084
step: 290, loss: 0.034431397914886475
step: 300, loss: 0.016991781070828438
step: 310, loss: 0.004185194615274668
step: 320, loss: 0.054834701120853424
step: 330, loss: 0.05318979173898697
step: 340, loss: 0.05785643309354782
step: 350, loss: 0.07800931483507156
step: 360, loss: 0.029872072860598564
step: 370, loss: 0.15061362087726593
step: 380, loss: 0.006249088793992996
step: 390, loss: 0.01357816532254219
step: 400, loss: 0.09002269059419632
step: 410, loss: 0.023385079577565193
step: 420, loss: 0.01717972755432129
step: 430, loss: 0.027484752237796783
step: 440, loss: 0.024584589526057243
step: 450, loss: 0.0041216579265892506
step: 460, loss: 0.02984006516635418
step: 470, loss: 0.04829835519194603
step: 480, loss: 0.030076807364821434
step: 490, loss: 0.012827682308852673
step: 500, loss: 0.07718341052532196
step: 510, loss: 0.03044755384325981
step: 520, loss: 0.05417415127158165
step: 530, loss: 0.08426893502473831
step: 540, loss: 0.16154694557189941
step: 550, loss: 0.019189707934856415
step: 560, loss: 0.03371462598443031
step: 570, loss: 0.03712009638547897
step: 580, loss: 0.07244184613227844
step: 590, loss: 0.027098413556814194
step: 600, loss: 0.0069801826030015945
step: 610, loss: 0.029322156682610512
step: 620, loss: 0.05134318396449089
step: 630, loss: 0.09841058403253555
step: 640, loss: 0.05580764263868332
step: 650, loss: 0.03205432370305061
step: 660, loss: 0.010982390493154526
step: 670, loss: 0.00778964813798666
step: 680, loss: 0.0982099175453186
step: 690, loss: 0.029516475275158882
step: 700, loss: 0.04246162623167038
step: 710, loss: 0.069889597594738
step: 720, loss: 0.02345886453986168
step: 730, loss: 0.04279593750834465
step: 740, loss: 0.0004633092903532088
step: 750, loss: 0.006373856216669083
step: 760, loss: 0.056696679443120956
step: 770, loss: 0.00670459121465683
step: 780, loss: 0.05144727975130081
step: 790, loss: 0.0011965627782046795
step: 800, loss: 0.2140938937664032
step: 810, loss: 0.020693572238087654
step: 820, loss: 0.04556455463171005
step: 830, loss: 0.06090766564011574
step: 840, loss: 0.08114080876111984
step: 850, loss: 0.07977869361639023
step: 860, loss: 0.0007544952095486224
step: 870, loss: 0.013710412196815014
step: 880, loss: 0.11002865433692932
step: 890, loss: 0.13369141519069672
step: 900, loss: 0.07392393052577972
step: 910, loss: 0.0242190919816494
step: 920, loss: 0.007071620784699917
step: 930, loss: 0.0442180261015892
step: 940, loss: 0.08521877974271774
step: 950, loss: 0.08710746467113495
step: 960, loss: 0.0636843740940094
step: 970, loss: 0.012418711557984352
epoch 11: dev_f1=0.930939226519337, f1=0.9262966333030028, best_f1=0.9353187529083293
step: 0, loss: 0.0018670603167265654
step: 10, loss: 0.06765688210725784
step: 20, loss: 0.05706656724214554
step: 30, loss: 0.027822962030768394
step: 40, loss: 0.08852119743824005
step: 50, loss: 0.02872643619775772
step: 60, loss: 0.05022081732749939
step: 70, loss: 0.0060295904986560345
step: 80, loss: 0.031779881566762924
step: 90, loss: 0.040034934878349304
step: 100, loss: 0.1124122366309166
step: 110, loss: 0.02945706993341446
step: 120, loss: 0.04296012595295906
step: 130, loss: 0.03265129402279854
step: 140, loss: 0.044453106820583344
step: 150, loss: 0.014544877223670483
step: 160, loss: 0.013342151418328285
step: 170, loss: 0.04770808294415474
step: 180, loss: 0.05670666694641113
step: 190, loss: 0.07255131751298904
step: 200, loss: 0.007235819008201361
step: 210, loss: 0.003836733056232333
step: 220, loss: 0.02732611820101738
step: 230, loss: 0.03102947771549225
step: 240, loss: 0.0006338079110719264
step: 250, loss: 0.10135328024625778
step: 260, loss: 0.007194965612143278
step: 270, loss: 0.0012048444477841258
step: 280, loss: 0.001365293050184846
step: 290, loss: 0.04461346194148064
step: 300, loss: 0.021869298070669174
step: 310, loss: 0.0010263517033308744
step: 320, loss: 0.039354413747787476
step: 330, loss: 0.034952789545059204
step: 340, loss: 0.08053532987833023
step: 350, loss: 0.009331298060715199
step: 360, loss: 0.08424761891365051
step: 370, loss: 2.1364032363635488e-05
step: 380, loss: 0.01976163685321808
step: 390, loss: 0.015624194405972958
step: 400, loss: 0.007406786549836397
step: 410, loss: 0.025248628109693527
step: 420, loss: 0.003539569675922394
step: 430, loss: 0.01587104983627796
step: 440, loss: 0.04804801568388939
step: 450, loss: 0.0411309078335762
step: 460, loss: 0.014796928502619267
step: 470, loss: 0.06377773731946945
step: 480, loss: 0.1566692590713501
step: 490, loss: 0.01625531166791916
step: 500, loss: 0.06153259053826332
step: 510, loss: 0.045878417789936066
step: 520, loss: 0.007436874322593212
step: 530, loss: 0.041916169226169586
step: 540, loss: 0.03330788016319275
step: 550, loss: 0.0064511364325881
step: 560, loss: 0.09463150054216385
step: 570, loss: 0.03355972841382027
step: 580, loss: 0.022808315232396126
step: 590, loss: 0.0011653625406324863
step: 600, loss: 0.04973839968442917
step: 610, loss: 0.08942656964063644
step: 620, loss: 0.09465518593788147
step: 630, loss: 0.06697601079940796
step: 640, loss: 0.06365379691123962
step: 650, loss: 0.005111320875585079
step: 660, loss: 0.08256004005670547
step: 670, loss: 0.05121520534157753
step: 680, loss: 0.07556995004415512
step: 690, loss: 0.007331206928938627
step: 700, loss: 0.0014460802776739001
step: 710, loss: 0.06830563396215439
step: 720, loss: 0.09270501136779785
step: 730, loss: 0.033644407987594604
step: 740, loss: 0.023082934319972992
step: 750, loss: 6.512654363177717e-05
step: 760, loss: 0.03916222229599953
step: 770, loss: 0.031016314402222633
step: 780, loss: 0.08580059558153152
step: 790, loss: 0.0013892012648284435
step: 800, loss: 0.05261535197496414
step: 810, loss: 0.023349296301603317
step: 820, loss: 0.036723870784044266
step: 830, loss: 0.05485053360462189
step: 840, loss: 0.00018180820916313678
step: 850, loss: 0.0419393852353096
step: 860, loss: 0.007560335099697113
step: 870, loss: 0.07036570459604263
step: 880, loss: 0.05131854489445686
step: 890, loss: 0.06142820790410042
step: 900, loss: 0.09043627232313156
step: 910, loss: 0.09543782472610474
step: 920, loss: 0.015909411013126373
step: 930, loss: 0.0055631776340305805
step: 940, loss: 0.017861537635326385
step: 950, loss: 0.037652432918548584
step: 960, loss: 0.046262405812740326
step: 970, loss: 0.036281973123550415
epoch 12: dev_f1=0.9349112426035502, f1=0.927470534904805, best_f1=0.9353187529083293
step: 0, loss: 0.049991805106401443
step: 10, loss: 0.03148774802684784
step: 20, loss: 0.08383109420537949
step: 30, loss: 0.0251239612698555
step: 40, loss: 0.005232152529060841
step: 50, loss: 0.04863489419221878
step: 60, loss: 0.005443282425403595
step: 70, loss: 0.024345334619283676
step: 80, loss: 0.06440351158380508
step: 90, loss: 0.0014788395492359996
step: 100, loss: 0.009155887179076672
step: 110, loss: 0.054833345115184784
step: 120, loss: 0.052028387784957886
step: 130, loss: 0.0376766175031662
step: 140, loss: 0.007151570171117783
step: 150, loss: 0.0035304720513522625
step: 160, loss: 0.011289701797068119
step: 170, loss: 0.11658329516649246
step: 180, loss: 0.07675417512655258
step: 190, loss: 0.08841415494680405
step: 200, loss: 0.0002663816267158836
step: 210, loss: 0.040556494146585464
step: 220, loss: 0.09677829593420029
step: 230, loss: 0.0004422480706125498
step: 240, loss: 0.007427341304719448
step: 250, loss: 0.0039046225138008595
step: 260, loss: 0.0007063754601404071
step: 270, loss: 0.000244564755121246
step: 280, loss: 0.00025722288410179317
step: 290, loss: 0.06573165953159332
step: 300, loss: 0.02216196246445179
step: 310, loss: 0.04597105830907822
step: 320, loss: 0.0052031828090548515
step: 330, loss: 0.017052747309207916
step: 340, loss: 0.02573820762336254
step: 350, loss: 0.12094507366418839
step: 360, loss: 0.01693853735923767
step: 370, loss: 0.0026938177179545164
step: 380, loss: 0.02637390047311783
step: 390, loss: 0.045286014676094055
step: 400, loss: 0.034896012395620346
step: 410, loss: 0.01934552937746048
step: 420, loss: 0.0010971322190016508
step: 430, loss: 0.008626111783087254
step: 440, loss: 0.044904254376888275
step: 450, loss: 0.22869136929512024
step: 460, loss: 0.06567586958408356
step: 470, loss: 0.06945584714412689
step: 480, loss: 0.03168339654803276
step: 490, loss: 0.0003483830951154232
step: 500, loss: 0.018057744950056076
step: 510, loss: 0.0012578285532072186
step: 520, loss: 0.04074876755475998
step: 530, loss: 0.010408775880932808
step: 540, loss: 0.05802760645747185
step: 550, loss: 0.007589033339172602
step: 560, loss: 0.0008360508945770562
step: 570, loss: 0.013100383803248405
step: 580, loss: 0.04459824040532112
step: 590, loss: 0.07956808805465698
step: 600, loss: 0.022831590846180916
step: 610, loss: 0.07336819171905518
step: 620, loss: 0.05527323856949806
step: 630, loss: 0.003444862551987171
step: 640, loss: 0.0006752493791282177
step: 650, loss: 0.00559117691591382
step: 660, loss: 0.0004671884817071259
step: 670, loss: 0.005042792297899723
step: 680, loss: 0.04868729040026665
step: 690, loss: 0.04344227537512779
step: 700, loss: 0.023409930989146233
step: 710, loss: 0.0024340692907571793
step: 720, loss: 0.08674761652946472
step: 730, loss: 0.09741047024726868
step: 740, loss: 0.017323032021522522
step: 750, loss: 4.3626889237202704e-05
step: 760, loss: 0.04483933746814728
step: 770, loss: 0.040893033146858215
step: 780, loss: 0.05978693068027496
step: 790, loss: 0.06455358117818832
step: 800, loss: 0.001340884598903358
step: 810, loss: 0.016519315540790558
step: 820, loss: 0.04768729209899902
step: 830, loss: 0.004367212299257517
step: 840, loss: 0.09693637490272522
step: 850, loss: 0.04103115200996399
step: 860, loss: 0.07607191801071167
step: 870, loss: 0.012382032349705696
step: 880, loss: 0.012176857329905033
step: 890, loss: 0.06684950739145279
step: 900, loss: 0.05144716054201126
step: 910, loss: 0.02490217424929142
step: 920, loss: 0.05760222300887108
step: 930, loss: 0.13926920294761658
step: 940, loss: 0.08440306782722473
step: 950, loss: 0.033905647695064545
step: 960, loss: 0.028027057647705078
step: 970, loss: 0.026412418112158775
epoch 13: dev_f1=0.9337094499294781, f1=0.9325267566309912, best_f1=0.9353187529083293
step: 0, loss: 0.021209996193647385
step: 10, loss: 0.024973366409540176
step: 20, loss: 0.05269218981266022
step: 30, loss: 0.05347456783056259
step: 40, loss: 0.0017275442369282246
step: 50, loss: 0.0004971252055838704
step: 60, loss: 0.05711151659488678
step: 70, loss: 0.0016582469688728452
step: 80, loss: 0.005512376315891743
step: 90, loss: 0.07961155474185944
step: 100, loss: 8.419940422754735e-05
step: 110, loss: 0.09587656706571579
step: 120, loss: 0.00014339949120767415
step: 130, loss: 0.05064285919070244
step: 140, loss: 0.014499209821224213
step: 150, loss: 0.02135719731450081
step: 160, loss: 0.058285363018512726
step: 170, loss: 0.0415896512567997
step: 180, loss: 0.04119099676609039
step: 190, loss: 0.0013370894594118
step: 200, loss: 0.0014824693789705634
step: 210, loss: 0.0002196584828197956
step: 220, loss: 0.08713488280773163
step: 230, loss: 0.027850091457366943
step: 240, loss: 0.07339893281459808
step: 250, loss: 0.061183251440525055
step: 260, loss: 0.029792331159114838
step: 270, loss: 0.06360625475645065
step: 280, loss: 0.07589872181415558
step: 290, loss: 0.058122701942920685
step: 300, loss: 0.016126861795783043
step: 310, loss: 0.0003865645849145949
step: 320, loss: 0.015132278203964233
step: 330, loss: 0.0460931621491909
step: 340, loss: 0.03423801064491272
step: 350, loss: 0.034350767731666565
step: 360, loss: 0.00015276011254172772
step: 370, loss: 0.11228073388338089
step: 380, loss: 0.14012783765792847
step: 390, loss: 0.03335116431117058
step: 400, loss: 0.01757490262389183
step: 410, loss: 0.00019897065067198128
step: 420, loss: 0.10617926716804504
step: 430, loss: 0.04182569682598114
step: 440, loss: 0.008090510033071041
step: 450, loss: 0.018252069130539894
step: 460, loss: 0.05002304166555405
step: 470, loss: 0.026230167597532272
step: 480, loss: 0.09255801886320114
step: 490, loss: 0.03665003180503845
step: 500, loss: 9.59749158937484e-05
step: 510, loss: 0.055017683655023575
step: 520, loss: 0.04987582191824913
step: 530, loss: 0.0009481706656515598
step: 540, loss: 0.038161810487508774
step: 550, loss: 0.0062434920109808445
step: 560, loss: 0.07370731979608536
step: 570, loss: 0.040745463222265244
step: 580, loss: 0.05832107737660408
step: 590, loss: 0.03798928111791611
step: 600, loss: 0.02782261185348034
step: 610, loss: 0.03183143213391304
step: 620, loss: 0.0006057656719349325
step: 630, loss: 0.027803955599665642
step: 640, loss: 0.01799902506172657
step: 650, loss: 0.05496769770979881
step: 660, loss: 0.040660642087459564
step: 670, loss: 0.07249929010868073
step: 680, loss: 0.04290636256337166
step: 690, loss: 0.018096327781677246
step: 700, loss: 0.1630624234676361
step: 710, loss: 0.03379358723759651
step: 720, loss: 0.013707960955798626
step: 730, loss: 0.019529808312654495
step: 740, loss: 0.018997130915522575
step: 750, loss: 0.02185487002134323
step: 760, loss: 0.060537178069353104
step: 770, loss: 0.013316215015947819
step: 780, loss: 0.028927214443683624
step: 790, loss: 0.04305781424045563
step: 800, loss: 0.06777642667293549
step: 810, loss: 0.00016262427379842848
step: 820, loss: 0.13966567814350128
step: 830, loss: 0.013164503499865532
step: 840, loss: 0.13208512961864471
step: 850, loss: 0.03700185939669609
step: 860, loss: 0.05582530423998833
step: 870, loss: 0.028556719422340393
step: 880, loss: 0.0008072095224633813
step: 890, loss: 0.04911218583583832
step: 900, loss: 0.039072342216968536
step: 910, loss: 0.0935063436627388
step: 920, loss: 0.014231615699827671
step: 930, loss: 0.04587920382618904
step: 940, loss: 0.0001854376750998199
step: 950, loss: 0.02226405404508114
step: 960, loss: 0.009429514408111572
step: 970, loss: 0.0012450029607862234
epoch 14: dev_f1=0.9311926605504587, f1=0.9361896080218779, best_f1=0.9353187529083293
step: 0, loss: 0.0009437990956939757
step: 10, loss: 0.02636112831532955
step: 20, loss: 0.0004883497604168952
step: 30, loss: 0.010570103302598
step: 40, loss: 0.010838611982762814
step: 50, loss: 0.002496347762644291
step: 60, loss: 5.029172825743444e-05
step: 70, loss: 0.021373938769102097
step: 80, loss: 0.04357658699154854
step: 90, loss: 0.06452889740467072
step: 100, loss: 0.004191718064248562
step: 110, loss: 0.0218652430921793
step: 120, loss: 0.05420435592532158
step: 130, loss: 0.06613560020923615
step: 140, loss: 3.8398131437134e-05
step: 150, loss: 9.91408305708319e-05
step: 160, loss: 0.06962750852108002
step: 170, loss: 0.0533573217689991
step: 180, loss: 0.1709652841091156
step: 190, loss: 0.08707671612501144
step: 200, loss: 0.04279574751853943
step: 210, loss: 0.058878738433122635
step: 220, loss: 0.0005035271169617772
step: 230, loss: 0.022224752232432365
step: 240, loss: 5.784532186225988e-05
step: 250, loss: 1.1026770152966492e-05
step: 260, loss: 0.01296747662127018
step: 270, loss: 0.06703551858663559
step: 280, loss: 9.313168447988573e-06
step: 290, loss: 0.027410391718149185
step: 300, loss: 0.018489142879843712
step: 310, loss: 0.012212781235575676
step: 320, loss: 0.00010649477917468175
step: 330, loss: 0.041034940630197525
step: 340, loss: 0.015950698405504227
step: 350, loss: 0.060259781777858734
step: 360, loss: 0.02910730428993702
step: 370, loss: 0.023864801973104477
step: 380, loss: 0.025616055354475975
step: 390, loss: 0.0598563551902771
step: 400, loss: 0.0703529417514801
step: 410, loss: 0.041890401393175125
step: 420, loss: 0.0003241650410927832
step: 430, loss: 0.06300069391727448
step: 440, loss: 0.05129203572869301
step: 450, loss: 0.0009088072110898793
step: 460, loss: 0.04224129766225815
step: 470, loss: 0.003158890875056386
step: 480, loss: 0.04387785494327545
step: 490, loss: 0.0064599099569022655
step: 500, loss: 0.022786177694797516
step: 510, loss: 0.029753342270851135
step: 520, loss: 0.053301166743040085
step: 530, loss: 0.05239191651344299
step: 540, loss: 0.0853271335363388
step: 550, loss: 0.0027084043249487877
step: 560, loss: 0.022421352565288544
step: 570, loss: 0.07406122982501984
step: 580, loss: 0.00026470530428923666
step: 590, loss: 0.0007604777347296476
step: 600, loss: 0.052442517131567
step: 610, loss: 5.2074934501433745e-05
step: 620, loss: 0.019426235929131508
step: 630, loss: 0.042123302817344666
step: 640, loss: 0.0003766277222894132
step: 650, loss: 0.06127963587641716
step: 660, loss: 0.03593882545828819
step: 670, loss: 0.04844541847705841
step: 680, loss: 0.05777169391512871
step: 690, loss: 0.0005995347164571285
step: 700, loss: 0.031189600005745888
step: 710, loss: 0.04646000638604164
step: 720, loss: 0.048846062272787094
step: 730, loss: 0.027157438918948174
step: 740, loss: 0.01884586364030838
step: 750, loss: 0.02550925500690937
step: 760, loss: 0.07233897596597672
step: 770, loss: 0.067080058157444
step: 780, loss: 0.00022758677368983626
step: 790, loss: 0.0019361278973519802
step: 800, loss: 0.024518698453903198
step: 810, loss: 0.030708743259310722
step: 820, loss: 0.037591949105262756
step: 830, loss: 0.04475019872188568
step: 840, loss: 0.0639616921544075
step: 850, loss: 0.11449426412582397
step: 860, loss: 0.04048554226756096
step: 870, loss: 0.03974828124046326
step: 880, loss: 0.00017058601952157915
step: 890, loss: 0.02204401046037674
step: 900, loss: 0.029598573222756386
step: 910, loss: 0.02214892953634262
step: 920, loss: 0.00014931103214621544
step: 930, loss: 0.030911821871995926
step: 940, loss: 0.06456844508647919
step: 950, loss: 0.02709420584142208
step: 960, loss: 1.1257687219767831e-05
step: 970, loss: 0.024514537304639816
epoch 15: dev_f1=0.9323515876668201, f1=0.9329073482428115, best_f1=0.9353187529083293
step: 0, loss: 0.04798895865678787
step: 10, loss: 0.03871864825487137
step: 20, loss: 0.027621861547231674
step: 30, loss: 0.05130535364151001
step: 40, loss: 4.438706309883855e-05
step: 50, loss: 0.034283023327589035
step: 60, loss: 0.015798555687069893
step: 70, loss: 0.00022414233535528183
step: 80, loss: 0.0197041817009449
step: 90, loss: 0.06280284374952316
step: 100, loss: 0.05522032082080841
step: 110, loss: 0.008316298015415668
step: 120, loss: 0.011756387539207935
step: 130, loss: 0.00015483250899706036
step: 140, loss: 0.05204413831233978
step: 150, loss: 0.022995412349700928
step: 160, loss: 0.0002801936643663794
step: 170, loss: 0.04600493982434273
step: 180, loss: 0.022049473598599434
step: 190, loss: 0.003041629446670413
step: 200, loss: 8.740287739783525e-05
step: 210, loss: 0.02148192748427391
step: 220, loss: 0.002092060400173068
step: 230, loss: 9.575224248692393e-05
step: 240, loss: 0.031384505331516266
step: 250, loss: 0.02226758748292923
step: 260, loss: 0.0035991682671010494
step: 270, loss: 0.04989956319332123
step: 280, loss: 0.07154877483844757
step: 290, loss: 0.045440081506967545
step: 300, loss: 0.018036313354969025
step: 310, loss: 0.05839535593986511
step: 320, loss: 0.04156985133886337
step: 330, loss: 0.028513463214039803
step: 340, loss: 0.016856418922543526
step: 350, loss: 0.031105760484933853
step: 360, loss: 0.00012452699593268335
step: 370, loss: 0.021808257326483727
step: 380, loss: 0.0049807606264948845
step: 390, loss: 0.02894049696624279
step: 400, loss: 0.03000185824930668
step: 410, loss: 0.07320691645145416
step: 420, loss: 0.058285195380449295
step: 430, loss: 0.019169295206665993
step: 440, loss: 0.0009809831390157342
step: 450, loss: 0.006198423448950052
step: 460, loss: 0.0028422276955097914
step: 470, loss: 0.0001233801303897053
step: 480, loss: 5.1349848945392296e-05
step: 490, loss: 0.032890625298023224
step: 500, loss: 0.0006627539405599236
step: 510, loss: 0.034949082881212234
step: 520, loss: 0.018629740923643112
step: 530, loss: 0.0002876675862353295
step: 540, loss: 0.09661919623613358
step: 550, loss: 0.02189142256975174
step: 560, loss: 0.004046390764415264
step: 570, loss: 4.9995585868600756e-05
step: 580, loss: 0.02263171598315239
step: 590, loss: 0.012796257622539997
step: 600, loss: 0.006471902597695589
step: 610, loss: 0.007388480938971043
step: 620, loss: 0.07774476706981659
step: 630, loss: 0.03801378235220909
step: 640, loss: 0.10211291909217834
step: 650, loss: 0.06873449683189392
step: 660, loss: 0.02047954499721527
step: 670, loss: 0.013244624249637127
step: 680, loss: 0.020630137994885445
step: 690, loss: 3.572525383788161e-05
step: 700, loss: 0.03895489498972893
step: 710, loss: 0.0018986128270626068
step: 720, loss: 0.043851155787706375
step: 730, loss: 0.03045915998518467
step: 740, loss: 0.049361247569322586
step: 750, loss: 0.00012755942589137703
step: 760, loss: 0.00016543202218599617
step: 770, loss: 0.00032437019399367273
step: 780, loss: 0.0483730211853981
step: 790, loss: 0.061350226402282715
step: 800, loss: 0.04338954761624336
step: 810, loss: 0.001108116121031344
step: 820, loss: 0.021350892260670662
step: 830, loss: 0.02278071828186512
step: 840, loss: 0.04947425052523613
step: 850, loss: 0.02003110572695732
step: 860, loss: 0.015315891243517399
step: 870, loss: 0.03325743228197098
step: 880, loss: 0.00010761396697489545
step: 890, loss: 0.017967943102121353
step: 900, loss: 0.04331407696008682
step: 910, loss: 0.03624168783426285
step: 920, loss: 0.04056227579712868
step: 930, loss: 0.03375944495201111
step: 940, loss: 0.05285225436091423
step: 950, loss: 0.020441286265850067
step: 960, loss: 0.00016809467342682183
step: 970, loss: 0.03522084280848503
epoch 16: dev_f1=0.9298162976919454, f1=0.9307583608101743, best_f1=0.9353187529083293
step: 0, loss: 0.03933767229318619
step: 10, loss: 0.033875178545713425
step: 20, loss: 4.9521710025146604e-05
step: 30, loss: 0.020971938967704773
step: 40, loss: 0.00021473804372362792
step: 50, loss: 0.03810488060116768
step: 60, loss: 0.049563683569431305
step: 70, loss: 0.023748591542243958
step: 80, loss: 0.00013600499369204044
step: 90, loss: 0.022926582023501396
step: 100, loss: 0.042700063437223434
step: 110, loss: 0.02740383893251419
step: 120, loss: 0.00028996128821745515
step: 130, loss: 0.00015623246144969016
step: 140, loss: 0.0012659610947594047
step: 150, loss: 0.04486110806465149
step: 160, loss: 0.004737205803394318
step: 170, loss: 0.013703235425055027
step: 180, loss: 0.02400929108262062
step: 190, loss: 0.028987515717744827
step: 200, loss: 0.03275720775127411
step: 210, loss: 0.007808346766978502
step: 220, loss: 0.03471696749329567
step: 230, loss: 0.02752126380801201
step: 240, loss: 0.022455379366874695
step: 250, loss: 0.03386315703392029
step: 260, loss: 0.0503721609711647
step: 270, loss: 0.018219348043203354
step: 280, loss: 0.02772366628050804
step: 290, loss: 0.027369152754545212
step: 300, loss: 0.036102596670389175
step: 310, loss: 0.05363001674413681
step: 320, loss: 0.0011403239332139492
step: 330, loss: 0.00029059822554700077
step: 340, loss: 0.04901094362139702
step: 350, loss: 0.002522988710552454
step: 360, loss: 0.06006545573472977
step: 370, loss: 0.016001468524336815
step: 380, loss: 0.025405293330550194
step: 390, loss: 0.04926399886608124
step: 400, loss: 0.005506184883415699
step: 410, loss: 0.018433794379234314
step: 420, loss: 0.03405734896659851
step: 430, loss: 0.007132645696401596
step: 440, loss: 0.04076632484793663
step: 450, loss: 0.019707515835762024
step: 460, loss: 0.05882447585463524
step: 470, loss: 0.051913220435380936
step: 480, loss: 0.024687567725777626
step: 490, loss: 0.0002683858620002866
step: 500, loss: 0.05471452698111534
step: 510, loss: 6.213508459040895e-05
step: 520, loss: 0.037920400500297546
step: 530, loss: 0.021943002939224243
step: 540, loss: 0.0007227719761431217
step: 550, loss: 0.015919670462608337
step: 560, loss: 0.08110717684030533
step: 570, loss: 0.0002229511010227725
step: 580, loss: 0.048345014452934265
step: 590, loss: 0.07914511859416962
step: 600, loss: 0.01767747476696968
step: 610, loss: 0.0048600416630506516
step: 620, loss: 0.04754872992634773
step: 630, loss: 0.04022454842925072
step: 640, loss: 0.06007924675941467
step: 650, loss: 0.0020313861314207315
step: 660, loss: 0.00028283268329687417
step: 670, loss: 0.06314748525619507
step: 680, loss: 0.028763022273778915
step: 690, loss: 0.008661849424242973
step: 700, loss: 0.06722050905227661
step: 710, loss: 0.022033032029867172
step: 720, loss: 8.012411853997037e-05
step: 730, loss: 0.019999368116259575
step: 740, loss: 0.04658423364162445
step: 750, loss: 0.03879528120160103
step: 760, loss: 4.839662869926542e-05
step: 770, loss: 0.01767621748149395
step: 780, loss: 0.09813285619020462
step: 790, loss: 0.0338352769613266
step: 800, loss: 0.06738827377557755
step: 810, loss: 0.11773490905761719
step: 820, loss: 0.024234216660261154
step: 830, loss: 0.04402735084295273
step: 840, loss: 0.029235273599624634
step: 850, loss: 0.0015006058383733034
step: 860, loss: 0.08879925310611725
step: 870, loss: 2.2442314730142243e-05
step: 880, loss: 0.05868017300963402
step: 890, loss: 0.030211515724658966
step: 900, loss: 0.08687443286180496
step: 910, loss: 0.046551503241062164
step: 920, loss: 0.0027459170669317245
step: 930, loss: 0.020944803953170776
step: 940, loss: 0.038571421056985855
step: 950, loss: 0.009961741976439953
step: 960, loss: 0.0727681815624237
step: 970, loss: 0.0016364011680707335
epoch 17: dev_f1=0.9348729792147806, f1=0.9387379087977891, best_f1=0.9353187529083293
step: 0, loss: 0.013304824009537697
step: 10, loss: 0.024908307939767838
step: 20, loss: 0.009319973178207874
step: 30, loss: 0.07744662463665009
step: 40, loss: 0.049812257289886475
step: 50, loss: 0.0014011143939569592
step: 60, loss: 0.03437997028231621
step: 70, loss: 0.08911236375570297
step: 80, loss: 0.004535794723778963
step: 90, loss: 0.018316512927412987
step: 100, loss: 0.016085628420114517
step: 110, loss: 0.017592549324035645
step: 120, loss: 0.033800702542066574
step: 130, loss: 0.027422912418842316
step: 140, loss: 0.0018248117994517088
step: 150, loss: 0.09281133115291595
step: 160, loss: 0.05135739594697952
step: 170, loss: 0.05099135637283325
step: 180, loss: 0.02425430528819561
step: 190, loss: 0.0031488852109760046
step: 200, loss: 0.0002288496762048453
step: 210, loss: 0.0802028626203537
step: 220, loss: 0.017214911058545113
step: 230, loss: 0.07233399152755737
step: 240, loss: 0.04700352996587753
step: 250, loss: 0.029774613678455353
step: 260, loss: 0.0186699740588665
step: 270, loss: 0.03881382942199707
step: 280, loss: 0.001889471779577434
step: 290, loss: 0.06715298444032669
step: 300, loss: 0.07845374196767807
step: 310, loss: 0.04681270197033882
step: 320, loss: 0.015847500413656235
step: 330, loss: 8.482595876557752e-05
step: 340, loss: 0.025238024070858955
step: 350, loss: 0.05475146323442459
step: 360, loss: 0.07928872108459473
step: 370, loss: 0.014680873602628708
step: 380, loss: 6.360583938658237e-05
step: 390, loss: 0.03511905297636986
step: 400, loss: 4.026530950795859e-05
step: 410, loss: 5.954170774202794e-05
step: 420, loss: 0.023665396496653557
step: 430, loss: 0.0001541043893666938
step: 440, loss: 0.032894473522901535
step: 450, loss: 0.11105437576770782
step: 460, loss: 0.00010820653551490977
step: 470, loss: 0.032860808074474335
step: 480, loss: 0.023517124354839325
step: 490, loss: 0.0985754132270813
step: 500, loss: 0.05210838466882706
step: 510, loss: 0.015988342463970184
step: 520, loss: 0.0411984920501709
step: 530, loss: 0.01959255523979664
step: 540, loss: 0.0005347370752133429
step: 550, loss: 0.025740869343280792
step: 560, loss: 0.035229943692684174
step: 570, loss: 0.04561087489128113
step: 580, loss: 0.046413399279117584
step: 590, loss: 0.02031458355486393
step: 600, loss: 0.0002887292066588998
step: 610, loss: 5.9161975514143705e-05
step: 620, loss: 0.025643369182944298
step: 630, loss: 0.020993243902921677
step: 640, loss: 0.008894676342606544
step: 650, loss: 0.04305024445056915
step: 660, loss: 0.015561657957732677
step: 670, loss: 0.017685601487755775
step: 680, loss: 0.04916448891162872
step: 690, loss: 0.00023212215455714613
step: 700, loss: 0.00014869622827973217
step: 710, loss: 0.0017398280324414372
step: 720, loss: 0.0007614815840497613
step: 730, loss: 0.032119255512952805
step: 740, loss: 0.04824948310852051
step: 750, loss: 0.04881750047206879
step: 760, loss: 0.00999458972364664
step: 770, loss: 0.04030624404549599
step: 780, loss: 0.0194497499614954
step: 790, loss: 0.03909345716238022
step: 800, loss: 0.03133728355169296
step: 810, loss: 0.04602799192070961
step: 820, loss: 0.01705123670399189
step: 830, loss: 0.02627566084265709
step: 840, loss: 0.02833765372633934
step: 850, loss: 0.00020681778551079333
step: 860, loss: 0.006593705620616674
step: 870, loss: 0.003736491547897458
step: 880, loss: 3.2051953894551843e-05
step: 890, loss: 0.04156101495027542
step: 900, loss: 0.02118874154984951
step: 910, loss: 0.04124230891466141
step: 920, loss: 0.022275729104876518
step: 930, loss: 0.038923997431993484
step: 940, loss: 0.14641964435577393
step: 950, loss: 0.03926101326942444
step: 960, loss: 0.034211672842502594
step: 970, loss: 6.156021117931232e-05
epoch 18: dev_f1=0.9329582747304266, f1=0.9351376574895007, best_f1=0.9353187529083293
step: 0, loss: 0.020199012011289597
step: 10, loss: 0.02917552925646305
step: 20, loss: 0.12015791982412338
step: 30, loss: 0.000823348353151232
step: 40, loss: 4.123812323086895e-05
step: 50, loss: 4.854125654674135e-05
step: 60, loss: 0.02432136982679367
step: 70, loss: 0.024604810401797295
step: 80, loss: 1.176434125227388e-05
step: 90, loss: 0.01721235364675522
step: 100, loss: 0.03673603758215904
step: 110, loss: 0.09152938425540924
step: 120, loss: 0.023384293541312218
step: 130, loss: 0.0019039522157981992
step: 140, loss: 0.0303965974599123
step: 150, loss: 0.019344473257660866
step: 160, loss: 0.0006259707151912153
step: 170, loss: 0.0005661395261995494
step: 180, loss: 0.021542193368077278
step: 190, loss: 0.05403759330511093
step: 200, loss: 0.00014220154844224453
step: 210, loss: 0.0011512178461998701
step: 220, loss: 0.026010263711214066
step: 230, loss: 0.020658787339925766
step: 240, loss: 7.727952470304444e-05
step: 250, loss: 0.0012499361764639616
step: 260, loss: 0.0005220271996222436
step: 270, loss: 0.018387289717793465
step: 280, loss: 0.025345567613840103
step: 290, loss: 0.012892195023596287
step: 300, loss: 0.02369733527302742
step: 310, loss: 0.026449184864759445
step: 320, loss: 0.040039367973804474
step: 330, loss: 0.027204209938645363
step: 340, loss: 0.030612509697675705
step: 350, loss: 0.020536763593554497
step: 360, loss: 0.009613349102437496
step: 370, loss: 0.09503001719713211
step: 380, loss: 0.023236244916915894
step: 390, loss: 0.06352415680885315
step: 400, loss: 0.048583995550870895
step: 410, loss: 0.018383914604783058
step: 420, loss: 0.02335168793797493
step: 430, loss: 0.05275041610002518
step: 440, loss: 8.172124216798693e-05
step: 450, loss: 0.053646985441446304
step: 460, loss: 0.08248697221279144
step: 470, loss: 0.05754975974559784
step: 480, loss: 0.002087677363306284
step: 490, loss: 0.025298619642853737
step: 500, loss: 0.04506213590502739
step: 510, loss: 0.02787882089614868
step: 520, loss: 0.06018723547458649
step: 530, loss: 0.0029240285512059927
step: 540, loss: 0.03961116820573807
step: 550, loss: 0.022308925166726112
step: 560, loss: 0.059972308576107025
step: 570, loss: 7.149644079618156e-05
step: 580, loss: 0.028058260679244995
step: 590, loss: 0.039628852158784866
step: 600, loss: 0.003763783723115921
step: 610, loss: 0.00010610032768454403
step: 620, loss: 0.014973673969507217
step: 630, loss: 4.549568620859645e-05
step: 640, loss: 4.506999539444223e-05
step: 650, loss: 9.763911293703131e-06
step: 660, loss: 0.04762362316250801
step: 670, loss: 0.03605036810040474
step: 680, loss: 0.02612835355103016
step: 690, loss: 4.336738129495643e-05
step: 700, loss: 0.014046290889382362
step: 710, loss: 0.014226791448891163
step: 720, loss: 0.00021716745686717331
step: 730, loss: 5.587436680798419e-05
step: 740, loss: 9.271043381886557e-05
step: 750, loss: 0.04052892327308655
step: 760, loss: 5.242525367066264e-05
step: 770, loss: 0.07747887820005417
step: 780, loss: 0.010872704908251762
step: 790, loss: 0.0005248094093985856
step: 800, loss: 0.13271662592887878
step: 810, loss: 0.062260113656520844
step: 820, loss: 0.034232862293720245
step: 830, loss: 0.14478451013565063
step: 840, loss: 0.03446425870060921
step: 850, loss: 0.04946312680840492
step: 860, loss: 8.479208190692589e-05
step: 870, loss: 0.014829565770924091
step: 880, loss: 0.03626013919711113
step: 890, loss: 0.00015097775030881166
step: 900, loss: 0.23817309737205505
step: 910, loss: 0.0001275137037737295
step: 920, loss: 0.03890145197510719
step: 930, loss: 0.05111789330840111
step: 940, loss: 0.025892212986946106
step: 950, loss: 0.019930370151996613
step: 960, loss: 3.0096727641648613e-05
step: 970, loss: 0.04693837836384773
epoch 19: dev_f1=0.9335827876520112, f1=0.9377901578458682, best_f1=0.9353187529083293
step: 0, loss: 0.03742911294102669
step: 10, loss: 0.0524391271173954
step: 20, loss: 2.1577792722382583e-05
step: 30, loss: 0.07560388743877411
step: 40, loss: 0.0751829668879509
step: 50, loss: 0.04505075514316559
step: 60, loss: 0.05773623287677765
step: 70, loss: 0.0627104714512825
step: 80, loss: 0.04037025570869446
step: 90, loss: 0.02099708467721939
step: 100, loss: 0.03943010792136192
step: 110, loss: 0.06880231946706772
step: 120, loss: 0.0001812548580346629
step: 130, loss: 0.021626342087984085
step: 140, loss: 0.040069330483675
step: 150, loss: 0.039526522159576416
step: 160, loss: 0.0026206234470009804
step: 170, loss: 0.06379377841949463
step: 180, loss: 0.025133412331342697
step: 190, loss: 0.03635827824473381
step: 200, loss: 0.02308754064142704
step: 210, loss: 0.033752791583538055
step: 220, loss: 0.050027113407850266
step: 230, loss: 3.7424884794745594e-05
step: 240, loss: 0.027095109224319458
step: 250, loss: 0.041865237057209015
step: 260, loss: 2.9838314731023274e-05
step: 270, loss: 5.403549948823638e-05
step: 280, loss: 2.1551441022893414e-05
step: 290, loss: 0.08290673047304153
step: 300, loss: 0.07095497101545334
step: 310, loss: 0.05129169672727585
step: 320, loss: 0.020814523100852966
step: 330, loss: 1.1268882190051954e-05
step: 340, loss: 0.036970555782318115
step: 350, loss: 0.02196262590587139
step: 360, loss: 0.024432215839624405
step: 370, loss: 0.06345467269420624
step: 380, loss: 0.10103708505630493
step: 390, loss: 9.985280485125259e-05
step: 400, loss: 0.023681433871388435
step: 410, loss: 0.024303987622261047
step: 420, loss: 0.09653666615486145
step: 430, loss: 0.053398340940475464
step: 440, loss: 0.008658328093588352
step: 450, loss: 0.05866531282663345
step: 460, loss: 0.024088939651846886
step: 470, loss: 0.024739744141697884
step: 480, loss: 0.04397174343466759
step: 490, loss: 0.0011021115351468325
step: 500, loss: 0.043769970536231995
step: 510, loss: 0.016167184337973595
step: 520, loss: 0.0013771167723461986
step: 530, loss: 0.023502808064222336
step: 540, loss: 0.043257661163806915
step: 550, loss: 0.053019195795059204
step: 560, loss: 0.016336865723133087
step: 570, loss: 0.0018948480719700456
step: 580, loss: 3.561747507774271e-05
step: 590, loss: 0.06926178932189941
step: 600, loss: 0.030764536932110786
step: 610, loss: 0.04330398887395859
step: 620, loss: 0.03865319490432739
step: 630, loss: 0.05996251478791237
step: 640, loss: 4.854896178585477e-05
step: 650, loss: 0.006207149010151625
step: 660, loss: 0.021190879866480827
step: 670, loss: 0.014115608297288418
step: 680, loss: 0.01787530444562435
step: 690, loss: 0.023827454075217247
step: 700, loss: 0.026862628757953644
step: 710, loss: 0.02916969731450081
step: 720, loss: 0.01864446885883808
step: 730, loss: 6.65049665258266e-05
step: 740, loss: 0.030110660940408707
step: 750, loss: 0.01847675070166588
step: 760, loss: 0.04026089608669281
step: 770, loss: 0.00014226931671146303
step: 780, loss: 0.05935059115290642
step: 790, loss: 0.02413972280919552
step: 800, loss: 0.00012289619189687073
step: 810, loss: 0.01822282187640667
step: 820, loss: 0.09064333140850067
step: 830, loss: 0.014699756167829037
step: 840, loss: 0.00021261742222122848
step: 850, loss: 0.0004895202000625432
step: 860, loss: 4.858982720179483e-05
step: 870, loss: 0.0002045131113845855
step: 880, loss: 0.02725045569241047
step: 890, loss: 2.5105382519541308e-05
step: 900, loss: 0.053276509046554565
step: 910, loss: 0.030885450541973114
step: 920, loss: 0.028150955215096474
step: 930, loss: 0.008407640270888805
step: 940, loss: 0.02539181523025036
step: 950, loss: 0.028911493718624115
step: 960, loss: 0.0798640251159668
step: 970, loss: 0.002521680435165763
epoch 20: dev_f1=0.9321478708469817, f1=0.9377901578458682, best_f1=0.9353187529083293
