cuda
Device: cuda
step: 0, loss: 0.7022861838340759
step: 10, loss: 0.42201557755470276
step: 20, loss: 0.276090145111084
step: 30, loss: 0.23778775334358215
step: 40, loss: 0.3626590371131897
step: 50, loss: 0.26691263914108276
step: 60, loss: 0.34413471817970276
step: 70, loss: 0.08236964046955109
step: 80, loss: 0.2114233821630478
step: 90, loss: 0.3322184085845947
step: 100, loss: 0.2858487665653229
step: 110, loss: 0.09002175182104111
step: 120, loss: 0.12314137816429138
step: 130, loss: 0.11580827087163925
step: 140, loss: 0.15047591924667358
step: 150, loss: 0.14663544297218323
step: 160, loss: 0.12891888618469238
step: 170, loss: 0.32512935996055603
step: 180, loss: 0.11960011720657349
step: 190, loss: 0.13188868761062622
step: 200, loss: 0.23118892312049866
step: 210, loss: 0.25700703263282776
step: 220, loss: 0.056179411709308624
step: 230, loss: 0.1532297283411026
step: 240, loss: 0.08809603005647659
step: 250, loss: 0.1956091672182083
step: 260, loss: 0.28432759642601013
step: 270, loss: 0.06765472888946533
step: 280, loss: 0.16490332782268524
step: 290, loss: 0.15712936222553253
step: 300, loss: 0.20895452797412872
step: 310, loss: 0.1915784776210785
step: 320, loss: 0.13761074841022491
step: 330, loss: 0.2901889979839325
step: 340, loss: 0.1941603720188141
step: 350, loss: 0.07206876575946808
step: 360, loss: 0.21587029099464417
step: 370, loss: 0.0847533568739891
step: 380, loss: 0.12136483192443848
step: 390, loss: 0.1006009429693222
step: 400, loss: 0.11769982427358627
step: 410, loss: 0.09663061797618866
step: 420, loss: 0.12334686517715454
step: 430, loss: 0.13201773166656494
step: 440, loss: 0.1292685866355896
step: 450, loss: 0.053874239325523376
step: 460, loss: 0.13188113272190094
step: 470, loss: 0.2759650647640228
step: 480, loss: 0.28919243812561035
step: 490, loss: 0.21646538376808167
step: 500, loss: 0.18932493031024933
step: 510, loss: 0.27799615263938904
step: 520, loss: 0.14542455971240997
step: 530, loss: 0.14812156558036804
step: 540, loss: 0.052496831864118576
step: 550, loss: 0.1977936327457428
step: 560, loss: 0.08674321323633194
step: 570, loss: 0.08116357773542404
step: 580, loss: 0.08713654428720474
step: 590, loss: 0.07759197801351547
step: 600, loss: 0.4241713881492615
step: 610, loss: 0.10221581906080246
step: 620, loss: 0.11432523280382156
step: 630, loss: 0.2361012101173401
step: 640, loss: 0.13136693835258484
step: 650, loss: 0.14700810611248016
step: 660, loss: 0.1324874609708786
step: 670, loss: 0.06668705493211746
step: 680, loss: 0.0984349176287651
step: 690, loss: 0.04585053026676178
step: 700, loss: 0.11417606472969055
step: 710, loss: 0.12456920742988586
step: 720, loss: 0.16400639712810516
step: 730, loss: 0.1373230218887329
step: 740, loss: 0.10288859158754349
step: 750, loss: 0.30431464314460754
step: 760, loss: 0.19325853884220123
step: 770, loss: 0.06382224708795547
step: 780, loss: 0.14352582395076752
step: 790, loss: 0.11190970242023468
step: 800, loss: 0.13999205827713013
step: 810, loss: 0.18198904395103455
step: 820, loss: 0.09789073467254639
step: 830, loss: 0.12147891521453857
step: 840, loss: 0.16447068750858307
step: 850, loss: 0.17136624455451965
step: 860, loss: 0.11980046331882477
step: 870, loss: 0.11620700359344482
step: 880, loss: 0.19295863807201385
step: 890, loss: 0.08614575117826462
step: 900, loss: 0.09947957843542099
step: 910, loss: 0.14239361882209778
step: 920, loss: 0.07175546884536743
step: 930, loss: 0.12418361008167267
step: 940, loss: 0.0936775952577591
step: 950, loss: 0.09900802373886108
step: 960, loss: 0.14689689874649048
step: 970, loss: 0.09592607617378235
epoch 1: dev_f1=0.9089210649229332, f1=0.9073215940685819, best_f1=0.9073215940685819
step: 0, loss: 0.16599403321743011
step: 10, loss: 0.29113084077835083
step: 20, loss: 0.13160207867622375
step: 30, loss: 0.09644944220781326
step: 40, loss: 0.07804498821496964
step: 50, loss: 0.1870967596769333
step: 60, loss: 0.19064942002296448
step: 70, loss: 0.1180301234126091
step: 80, loss: 0.1918821483850479
step: 90, loss: 0.2449607253074646
step: 100, loss: 0.09553622454404831
step: 110, loss: 0.12364653497934341
step: 120, loss: 0.2658933401107788
step: 130, loss: 0.17106004059314728
step: 140, loss: 0.10187136381864548
step: 150, loss: 0.14582382142543793
step: 160, loss: 0.12089333683252335
step: 170, loss: 0.18466214835643768
step: 180, loss: 0.11507593095302582
step: 190, loss: 0.10108830779790878
step: 200, loss: 0.19404587149620056
step: 210, loss: 0.10858359187841415
step: 220, loss: 0.06925678998231888
step: 230, loss: 0.10784873366355896
step: 240, loss: 0.11499760299921036
step: 250, loss: 0.13154765963554382
step: 260, loss: 0.18066653609275818
step: 270, loss: 0.12342952936887741
step: 280, loss: 0.07670868188142776
step: 290, loss: 0.115499347448349
step: 300, loss: 0.1216323971748352
step: 310, loss: 0.04603876918554306
step: 320, loss: 0.08118866384029388
step: 330, loss: 0.08078893274068832
step: 340, loss: 0.18364490568637848
step: 350, loss: 0.1272677183151245
step: 360, loss: 0.19142310321331024
step: 370, loss: 0.1608227640390396
step: 380, loss: 0.1469196230173111
step: 390, loss: 0.18693600594997406
step: 400, loss: 0.08956016600131989
step: 410, loss: 0.1926923543214798
step: 420, loss: 0.1509672850370407
step: 430, loss: 0.12362456321716309
step: 440, loss: 0.08567681908607483
step: 450, loss: 0.15499554574489594
step: 460, loss: 0.17968960106372833
step: 470, loss: 0.13344238698482513
step: 480, loss: 0.1216420829296112
step: 490, loss: 0.056853052228689194
step: 500, loss: 0.11303284019231796
step: 510, loss: 0.05749271810054779
step: 520, loss: 0.22032767534255981
step: 530, loss: 0.19883836805820465
step: 540, loss: 0.1438712328672409
step: 550, loss: 0.18508073687553406
step: 560, loss: 0.12860669195652008
step: 570, loss: 0.1282581090927124
step: 580, loss: 0.10424599796533585
step: 590, loss: 0.24141035974025726
step: 600, loss: 0.15453602373600006
step: 610, loss: 0.22653968632221222
step: 620, loss: 0.17346593737602234
step: 630, loss: 0.15476037561893463
step: 640, loss: 0.061940997838974
step: 650, loss: 0.19451804459095
step: 660, loss: 0.10436071455478668
step: 670, loss: 0.2498968243598938
step: 680, loss: 0.1486087441444397
step: 690, loss: 0.040262620896101
step: 700, loss: 0.08722254633903503
step: 710, loss: 0.1596354842185974
step: 720, loss: 0.12397381663322449
step: 730, loss: 0.07764758169651031
step: 740, loss: 0.08861156553030014
step: 750, loss: 0.13058723509311676
step: 760, loss: 0.12223945558071136
step: 770, loss: 0.04824361950159073
step: 780, loss: 0.20979398488998413
step: 790, loss: 0.11499571800231934
step: 800, loss: 0.026042193174362183
step: 810, loss: 0.23797549307346344
step: 820, loss: 0.11023092269897461
step: 830, loss: 0.05705544725060463
step: 840, loss: 0.2477531135082245
step: 850, loss: 0.07420098781585693
step: 860, loss: 0.13013890385627747
step: 870, loss: 0.09825533628463745
step: 880, loss: 0.14576588571071625
step: 890, loss: 0.11867345124483109
step: 900, loss: 0.17649544775485992
step: 910, loss: 0.0930347889661789
step: 920, loss: 0.054475270211696625
step: 930, loss: 0.12074396759271622
step: 940, loss: 0.12584857642650604
step: 950, loss: 0.24981088936328888
step: 960, loss: 0.1521167904138565
step: 970, loss: 0.10843060165643692
epoch 2: dev_f1=0.925263640531866, f1=0.9253187613843351, best_f1=0.9253187613843351
step: 0, loss: 0.10853512585163116
step: 10, loss: 0.14309880137443542
step: 20, loss: 0.14651088416576385
step: 30, loss: 0.08412031829357147
step: 40, loss: 0.10125840455293655
step: 50, loss: 0.13136257231235504
step: 60, loss: 0.06966279447078705
step: 70, loss: 0.1081906259059906
step: 80, loss: 0.16855350136756897
step: 90, loss: 0.1113477274775505
step: 100, loss: 0.21196192502975464
step: 110, loss: 0.022748498246073723
step: 120, loss: 0.08922582119703293
step: 130, loss: 0.15089792013168335
step: 140, loss: 0.05871791020035744
step: 150, loss: 0.18771052360534668
step: 160, loss: 0.059121035039424896
step: 170, loss: 0.15004010498523712
step: 180, loss: 0.17829006910324097
step: 190, loss: 0.08480111509561539
step: 200, loss: 0.09468845278024673
step: 210, loss: 0.03144915774464607
step: 220, loss: 0.017909221351146698
step: 230, loss: 0.028917143121361732
step: 240, loss: 0.10722050070762634
step: 250, loss: 0.12392137944698334
step: 260, loss: 0.06212930753827095
step: 270, loss: 0.250179648399353
step: 280, loss: 0.13147500157356262
step: 290, loss: 0.04191321134567261
step: 300, loss: 0.09398365020751953
step: 310, loss: 0.1969890147447586
step: 320, loss: 0.20384733378887177
step: 330, loss: 0.11948791891336441
step: 340, loss: 0.13827259838581085
step: 350, loss: 0.13344697654247284
step: 360, loss: 0.17646954953670502
step: 370, loss: 0.06922093778848648
step: 380, loss: 0.07042825222015381
step: 390, loss: 0.19035644829273224
step: 400, loss: 0.0842873752117157
step: 410, loss: 0.20155854523181915
step: 420, loss: 0.203352689743042
step: 430, loss: 0.12412628531455994
step: 440, loss: 0.12200596928596497
step: 450, loss: 0.11432342231273651
step: 460, loss: 0.1327335387468338
step: 470, loss: 0.19388347864151
step: 480, loss: 0.1373450607061386
step: 490, loss: 0.12034609168767929
step: 500, loss: 0.12058311700820923
step: 510, loss: 0.11238302290439606
step: 520, loss: 0.17914122343063354
step: 530, loss: 0.09107816219329834
step: 540, loss: 0.053115688264369965
step: 550, loss: 0.26561006903648376
step: 560, loss: 0.06942451745271683
step: 570, loss: 0.13142643868923187
step: 580, loss: 0.14717282354831696
step: 590, loss: 0.1682717651128769
step: 600, loss: 0.2130749523639679
step: 610, loss: 0.04930464178323746
step: 620, loss: 0.03860294446349144
step: 630, loss: 0.0833425372838974
step: 640, loss: 0.16847296059131622
step: 650, loss: 0.02335316501557827
step: 660, loss: 0.1549120545387268
step: 670, loss: 0.2247646152973175
step: 680, loss: 0.10814899951219559
step: 690, loss: 0.18631863594055176
step: 700, loss: 0.13452063500881195
step: 710, loss: 0.04597330838441849
step: 720, loss: 0.219500333070755
step: 730, loss: 0.11828267574310303
step: 740, loss: 0.030860234051942825
step: 750, loss: 0.10186155140399933
step: 760, loss: 0.06415197253227234
step: 770, loss: 0.19935181736946106
step: 780, loss: 0.16353413462638855
step: 790, loss: 0.11251203715801239
step: 800, loss: 0.10406803339719772
step: 810, loss: 0.11445777863264084
step: 820, loss: 0.1939154714345932
step: 830, loss: 0.12739314138889313
step: 840, loss: 0.05218970403075218
step: 850, loss: 0.1147947609424591
step: 860, loss: 0.08032754808664322
step: 870, loss: 0.12442237138748169
step: 880, loss: 0.17439544200897217
step: 890, loss: 0.1447134017944336
step: 900, loss: 0.11568815261125565
step: 910, loss: 0.045316100120544434
step: 920, loss: 0.08816248923540115
step: 930, loss: 0.05759769305586815
step: 940, loss: 0.19235341250896454
step: 950, loss: 0.16045436263084412
step: 960, loss: 0.06594865024089813
step: 970, loss: 0.14535212516784668
epoch 3: dev_f1=0.9314942528735632, f1=0.9300184162062616, best_f1=0.9300184162062616
step: 0, loss: 0.08134379237890244
step: 10, loss: 0.05057908594608307
step: 20, loss: 0.03850005567073822
step: 30, loss: 0.0581829659640789
step: 40, loss: 0.1024424210190773
step: 50, loss: 0.12239240854978561
step: 60, loss: 0.05657720938324928
step: 70, loss: 0.15783193707466125
step: 80, loss: 0.0446571409702301
step: 90, loss: 0.04713372886180878
step: 100, loss: 0.013667390681803226
step: 110, loss: 0.07303109019994736
step: 120, loss: 0.13405856490135193
step: 130, loss: 0.039886023849248886
step: 140, loss: 0.31846022605895996
step: 150, loss: 0.08427666872739792
step: 160, loss: 0.1608341783285141
step: 170, loss: 0.10680697113275528
step: 180, loss: 0.12964367866516113
step: 190, loss: 0.08249782770872116
step: 200, loss: 0.10879736393690109
step: 210, loss: 0.06079122796654701
step: 220, loss: 0.19660647213459015
step: 230, loss: 0.12624342739582062
step: 240, loss: 0.024279838427901268
step: 250, loss: 0.15234045684337616
step: 260, loss: 0.18211117386817932
step: 270, loss: 0.16431309282779694
step: 280, loss: 0.13643470406532288
step: 290, loss: 0.06312295794487
step: 300, loss: 0.1144571453332901
step: 310, loss: 0.051186807453632355
step: 320, loss: 0.20922033488750458
step: 330, loss: 0.21199968457221985
step: 340, loss: 0.1567332148551941
step: 350, loss: 0.059082962572574615
step: 360, loss: 0.12049196660518646
step: 370, loss: 0.024476751685142517
step: 380, loss: 0.07791771739721298
step: 390, loss: 0.11429260671138763
step: 400, loss: 0.10868334770202637
step: 410, loss: 0.043318796902894974
step: 420, loss: 0.1348869800567627
step: 430, loss: 0.11475871503353119
step: 440, loss: 0.16862131655216217
step: 450, loss: 0.07027538120746613
step: 460, loss: 0.0727182999253273
step: 470, loss: 0.1454779952764511
step: 480, loss: 0.1184995099902153
step: 490, loss: 0.20265479385852814
step: 500, loss: 0.13521058857440948
step: 510, loss: 0.19406186044216156
step: 520, loss: 0.14616736769676208
step: 530, loss: 0.13703802227973938
step: 540, loss: 0.20614732801914215
step: 550, loss: 0.32121509313583374
step: 560, loss: 0.1434822976589203
step: 570, loss: 0.09536391496658325
step: 580, loss: 0.13538570702075958
step: 590, loss: 0.08317781239748001
step: 600, loss: 0.11774858832359314
step: 610, loss: 0.20872709155082703
step: 620, loss: 0.12483929842710495
step: 630, loss: 0.10643831640481949
step: 640, loss: 0.16136150062084198
step: 650, loss: 0.10628130286931992
step: 660, loss: 0.05723634734749794
step: 670, loss: 0.05675951763987541
step: 680, loss: 0.22443467378616333
step: 690, loss: 0.04910057783126831
step: 700, loss: 0.2463255226612091
step: 710, loss: 0.17798136174678802
step: 720, loss: 0.22661960124969482
step: 730, loss: 0.02741343341767788
step: 740, loss: 0.05725063383579254
step: 750, loss: 0.06537173688411713
step: 760, loss: 0.02239336259663105
step: 770, loss: 0.2442784458398819
step: 780, loss: 0.10462073236703873
step: 790, loss: 0.21595069766044617
step: 800, loss: 0.1397581249475479
step: 810, loss: 0.16217242181301117
step: 820, loss: 0.17309536039829254
step: 830, loss: 0.09957744926214218
step: 840, loss: 0.06451855599880219
step: 850, loss: 0.10269496589899063
step: 860, loss: 0.09060753136873245
step: 870, loss: 0.11855299025774002
step: 880, loss: 0.08231430500745773
step: 890, loss: 0.09535277634859085
step: 900, loss: 0.22009071707725525
step: 910, loss: 0.07722560316324234
step: 920, loss: 0.15774813294410706
step: 930, loss: 0.0590992346405983
step: 940, loss: 0.155900239944458
step: 950, loss: 0.09450065344572067
step: 960, loss: 0.13583604991436005
step: 970, loss: 0.14046861231327057
epoch 4: dev_f1=0.9216504404265183, f1=0.9230769230769231, best_f1=0.9300184162062616
step: 0, loss: 0.2465893179178238
step: 10, loss: 0.07786306738853455
step: 20, loss: 0.08779941499233246
step: 30, loss: 0.026941895484924316
step: 40, loss: 0.08456926792860031
step: 50, loss: 0.044951580464839935
step: 60, loss: 0.10143733024597168
step: 70, loss: 0.054288674145936966
step: 80, loss: 0.10551591217517853
step: 90, loss: 0.04470705986022949
step: 100, loss: 0.1427372843027115
step: 110, loss: 0.09907323122024536
step: 120, loss: 0.18960468471050262
step: 130, loss: 0.05986209958791733
step: 140, loss: 0.07093443721532822
step: 150, loss: 0.07586926966905594
step: 160, loss: 0.10059978812932968
step: 170, loss: 0.054892297834157944
step: 180, loss: 0.0786985456943512
step: 190, loss: 0.09876814484596252
step: 200, loss: 0.06827247142791748
step: 210, loss: 0.10038088262081146
step: 220, loss: 0.11064867675304413
step: 230, loss: 0.13925060629844666
step: 240, loss: 0.16039630770683289
step: 250, loss: 0.10523150861263275
step: 260, loss: 0.10631914436817169
step: 270, loss: 0.18885643780231476
step: 280, loss: 0.07757487148046494
step: 290, loss: 0.026439590379595757
step: 300, loss: 0.034656427800655365
step: 310, loss: 0.16798242926597595
step: 320, loss: 0.03933137282729149
step: 330, loss: 0.11856653541326523
step: 340, loss: 0.1796352118253708
step: 350, loss: 0.10545170307159424
step: 360, loss: 0.12875160574913025
step: 370, loss: 0.06432942301034927
step: 380, loss: 0.1130395382642746
step: 390, loss: 0.05570181459188461
step: 400, loss: 0.04057225584983826
step: 410, loss: 0.1080184206366539
step: 420, loss: 0.05158480256795883
step: 430, loss: 0.1438770592212677
step: 440, loss: 0.11038948595523834
step: 450, loss: 0.10696163028478622
step: 460, loss: 0.06587579101324081
step: 470, loss: 0.13230620324611664
step: 480, loss: 0.042166199535131454
step: 490, loss: 0.08037781715393066
step: 500, loss: 0.11655592918395996
step: 510, loss: 0.06937403231859207
step: 520, loss: 0.0390239916741848
step: 530, loss: 0.13260143995285034
step: 540, loss: 0.08292948454618454
step: 550, loss: 0.13364459574222565
step: 560, loss: 0.03966439887881279
step: 570, loss: 0.17003533244132996
step: 580, loss: 0.1272737979888916
step: 590, loss: 0.09449885785579681
step: 600, loss: 0.1456710398197174
step: 610, loss: 0.057010456919670105
step: 620, loss: 0.0933033674955368
step: 630, loss: 0.12408767640590668
step: 640, loss: 0.14836125075817108
step: 650, loss: 0.07097752392292023
step: 660, loss: 0.18524996936321259
step: 670, loss: 0.041665349155664444
step: 680, loss: 0.15083052217960358
step: 690, loss: 0.03791937604546547
step: 700, loss: 0.03600703924894333
step: 710, loss: 0.07926111668348312
step: 720, loss: 0.11153130978345871
step: 730, loss: 0.07832124084234238
step: 740, loss: 0.11541309207677841
step: 750, loss: 0.10702347010374069
step: 760, loss: 0.06594526767730713
step: 770, loss: 0.03484158590435982
step: 780, loss: 0.06703279912471771
step: 790, loss: 0.07294464111328125
step: 800, loss: 0.11244861781597137
step: 810, loss: 0.06311004608869553
step: 820, loss: 0.06666187196969986
step: 830, loss: 0.10268165916204453
step: 840, loss: 0.1188596561551094
step: 850, loss: 0.07795070111751556
step: 860, loss: 0.12381988018751144
step: 870, loss: 0.03282137215137482
step: 880, loss: 0.09807594865560532
step: 890, loss: 0.1304836869239807
step: 900, loss: 0.10665435343980789
step: 910, loss: 0.4276551306247711
step: 920, loss: 0.429691880941391
step: 930, loss: 0.06566048413515091
step: 940, loss: 0.05150081589818001
step: 950, loss: 0.08702770620584488
step: 960, loss: 0.15013745427131653
step: 970, loss: 0.06890909373760223
epoch 5: dev_f1=0.9349330872173512, f1=0.9308118081180812, best_f1=0.9308118081180812
step: 0, loss: 0.08665058761835098
step: 10, loss: 0.06328512728214264
step: 20, loss: 0.09528782218694687
step: 30, loss: 0.0804138109087944
step: 40, loss: 0.07422635704278946
step: 50, loss: 0.21035929024219513
step: 60, loss: 0.08645892888307571
step: 70, loss: 0.17114248871803284
step: 80, loss: 0.06717834621667862
step: 90, loss: 0.05058704689145088
step: 100, loss: 0.040999118238687515
step: 110, loss: 0.025964275002479553
step: 120, loss: 0.058098774403333664
step: 130, loss: 0.0845719650387764
step: 140, loss: 0.1293254941701889
step: 150, loss: 0.23692454397678375
step: 160, loss: 0.017241526395082474
step: 170, loss: 0.15375952422618866
step: 180, loss: 0.029276058077812195
step: 190, loss: 0.09292902797460556
step: 200, loss: 0.04319775849580765
step: 210, loss: 0.022451257333159447
step: 220, loss: 0.03326189145445824
step: 230, loss: 0.09536836296319962
step: 240, loss: 0.1039750948548317
step: 250, loss: 0.14819873869419098
step: 260, loss: 0.03904024139046669
step: 270, loss: 0.11353696137666702
step: 280, loss: 0.08050370961427689
step: 290, loss: 0.10685139894485474
step: 300, loss: 0.10717844218015671
step: 310, loss: 0.11395285278558731
step: 320, loss: 0.150611013174057
step: 330, loss: 0.06315729022026062
step: 340, loss: 0.2743431329727173
step: 350, loss: 0.061926279217004776
step: 360, loss: 0.04286564886569977
step: 370, loss: 0.07204209268093109
step: 380, loss: 0.16353799402713776
step: 390, loss: 0.09783650934696198
step: 400, loss: 0.014405556954443455
step: 410, loss: 0.05041172355413437
step: 420, loss: 0.11522291600704193
step: 430, loss: 0.12742938101291656
step: 440, loss: 0.10530664026737213
step: 450, loss: 0.13713297247886658
step: 460, loss: 0.08830533921718597
step: 470, loss: 0.02560798078775406
step: 480, loss: 0.08969321101903915
step: 490, loss: 0.0274495966732502
step: 500, loss: 0.06311442703008652
step: 510, loss: 0.026614191010594368
step: 520, loss: 0.10991068184375763
step: 530, loss: 0.11355201154947281
step: 540, loss: 0.09137246012687683
step: 550, loss: 0.11684933304786682
step: 560, loss: 0.18473117053508759
step: 570, loss: 0.07711182534694672
step: 580, loss: 0.05027109012007713
step: 590, loss: 0.1454244703054428
step: 600, loss: 0.07866460084915161
step: 610, loss: 0.04524723440408707
step: 620, loss: 0.09933558106422424
step: 630, loss: 0.1256442666053772
step: 640, loss: 0.031697895377874374
step: 650, loss: 0.09305086731910706
step: 660, loss: 0.1391177773475647
step: 670, loss: 0.0634760707616806
step: 680, loss: 0.08742109686136246
step: 690, loss: 0.11611762642860413
step: 700, loss: 0.12478993833065033
step: 710, loss: 0.07256581634283066
step: 720, loss: 0.050921596586704254
step: 730, loss: 0.09042023122310638
step: 740, loss: 0.11864139139652252
step: 750, loss: 0.07791030406951904
step: 760, loss: 0.09960650652647018
step: 770, loss: 0.11714603751897812
step: 780, loss: 0.08355174958705902
step: 790, loss: 0.07381264120340347
step: 800, loss: 0.0992746576666832
step: 810, loss: 0.16468214988708496
step: 820, loss: 0.0774071142077446
step: 830, loss: 0.06966298073530197
step: 840, loss: 0.05935351178050041
step: 850, loss: 0.18916481733322144
step: 860, loss: 0.17632445693016052
step: 870, loss: 0.13852716982364655
step: 880, loss: 0.07742486149072647
step: 890, loss: 0.13783369958400726
step: 900, loss: 0.08778457343578339
step: 910, loss: 0.1312328726053238
step: 920, loss: 0.05602800473570824
step: 930, loss: 0.0338820181787014
step: 940, loss: 0.10312247276306152
step: 950, loss: 0.025637835264205933
step: 960, loss: 0.06384242326021194
step: 970, loss: 0.17823176085948944
epoch 6: dev_f1=0.9309718437783834, f1=0.9315192743764172, best_f1=0.9308118081180812
step: 0, loss: 0.16206862032413483
step: 10, loss: 0.06769641488790512
step: 20, loss: 0.0944395437836647
step: 30, loss: 0.12727251648902893
step: 40, loss: 0.13281066715717316
step: 50, loss: 0.052832357585430145
step: 60, loss: 0.09962126612663269
step: 70, loss: 0.05671244487166405
step: 80, loss: 0.062126047909259796
step: 90, loss: 0.1298554241657257
step: 100, loss: 0.06262925267219543
step: 110, loss: 0.018674597144126892
step: 120, loss: 0.14667212963104248
step: 130, loss: 0.06961327791213989
step: 140, loss: 0.10888117551803589
step: 150, loss: 0.1111278161406517
step: 160, loss: 0.07226905971765518
step: 170, loss: 0.08413359522819519
step: 180, loss: 0.04894571378827095
step: 190, loss: 0.07181540131568909
step: 200, loss: 0.08820842951536179
step: 210, loss: 0.12026398628950119
step: 220, loss: 0.12208811193704605
step: 230, loss: 0.023850679397583008
step: 240, loss: 0.10360196232795715
step: 250, loss: 0.08532878756523132
step: 260, loss: 0.17764966189861298
step: 270, loss: 0.092864029109478
step: 280, loss: 0.07211676239967346
step: 290, loss: 0.06748205423355103
step: 300, loss: 0.10586366057395935
step: 310, loss: 0.031746115535497665
step: 320, loss: 0.0651644915342331
step: 330, loss: 0.1454208642244339
step: 340, loss: 0.1152128279209137
step: 350, loss: 0.17929966747760773
step: 360, loss: 0.1558716893196106
step: 370, loss: 0.14032195508480072
step: 380, loss: 0.028420738875865936
step: 390, loss: 0.1384693831205368
step: 400, loss: 0.034911707043647766
step: 410, loss: 0.11340732872486115
step: 420, loss: 0.04723195731639862
step: 430, loss: 0.15986454486846924
step: 440, loss: 0.21093347668647766
step: 450, loss: 0.11893341690301895
step: 460, loss: 0.03797833248972893
step: 470, loss: 0.07503803819417953
step: 480, loss: 0.11036507040262222
step: 490, loss: 0.07436547428369522
step: 500, loss: 0.0654214546084404
step: 510, loss: 0.08365422487258911
step: 520, loss: 0.028108777478337288
step: 530, loss: 0.08964458853006363
step: 540, loss: 0.16944286227226257
step: 550, loss: 0.06988689303398132
step: 560, loss: 0.023858513683080673
step: 570, loss: 0.05364447087049484
step: 580, loss: 0.06580669432878494
step: 590, loss: 0.1567917913198471
step: 600, loss: 0.07867006957530975
step: 610, loss: 0.12319643795490265
step: 620, loss: 0.06869230419397354
step: 630, loss: 0.09763077646493912
step: 640, loss: 0.08781921118497849
step: 650, loss: 0.07358075678348541
step: 660, loss: 0.025460252538323402
step: 670, loss: 0.1842052936553955
step: 680, loss: 0.048101894557476044
step: 690, loss: 0.027347492054104805
step: 700, loss: 0.06667748838663101
step: 710, loss: 0.22926278412342072
step: 720, loss: 0.13153477013111115
step: 730, loss: 0.07017433643341064
step: 740, loss: 0.08008617162704468
step: 750, loss: 0.0599956177175045
step: 760, loss: 0.009081241674721241
step: 770, loss: 0.07360795885324478
step: 780, loss: 0.08107508718967438
step: 790, loss: 0.1736786812543869
step: 800, loss: 0.2929235100746155
step: 810, loss: 0.07082659751176834
step: 820, loss: 0.16268786787986755
step: 830, loss: 0.04567085951566696
step: 840, loss: 0.07395397871732712
step: 850, loss: 0.049206703901290894
step: 860, loss: 0.2832137942314148
step: 870, loss: 0.10085289180278778
step: 880, loss: 0.10610078275203705
step: 890, loss: 0.07804170250892639
step: 900, loss: 0.13348206877708435
step: 910, loss: 0.020370973274111748
step: 920, loss: 0.18234331905841827
step: 930, loss: 0.16763877868652344
step: 940, loss: 0.11305394023656845
step: 950, loss: 0.10954705625772476
step: 960, loss: 0.09191974997520447
step: 970, loss: 0.3823198080062866
epoch 7: dev_f1=0.9263351749539595, f1=0.922863741339492, best_f1=0.9308118081180812
step: 0, loss: 0.2288779318332672
step: 10, loss: 0.022971902042627335
step: 20, loss: 0.07290519773960114
step: 30, loss: 0.24792684614658356
step: 40, loss: 0.039724331349134445
step: 50, loss: 0.04762021452188492
step: 60, loss: 0.08312958478927612
step: 70, loss: 0.042920056730508804
step: 80, loss: 0.024931665509939194
step: 90, loss: 0.05122943967580795
step: 100, loss: 0.13657504320144653
step: 110, loss: 0.01742996647953987
step: 120, loss: 0.052613694220781326
step: 130, loss: 0.18125256896018982
step: 140, loss: 0.10776077210903168
step: 150, loss: 0.115457683801651
step: 160, loss: 0.0881238728761673
step: 170, loss: 0.043780617415905
step: 180, loss: 0.09302807599306107
step: 190, loss: 0.16318395733833313
step: 200, loss: 0.03256230801343918
step: 210, loss: 0.07908941060304642
step: 220, loss: 0.0062058777548372746
step: 230, loss: 0.027499163523316383
step: 240, loss: 0.11095862835645676
step: 250, loss: 0.033486753702163696
step: 260, loss: 0.10923343151807785
step: 270, loss: 0.21215803921222687
step: 280, loss: 0.0548597015440464
step: 290, loss: 0.055151648819446564
step: 300, loss: 0.11581834405660629
step: 310, loss: 0.11210331320762634
step: 320, loss: 0.1561732441186905
step: 330, loss: 0.11440932005643845
step: 340, loss: 0.05385751649737358
step: 350, loss: 0.07267001271247864
step: 360, loss: 0.19640584290027618
step: 370, loss: 0.07655293494462967
step: 380, loss: 0.10541382431983948
step: 390, loss: 0.13841623067855835
step: 400, loss: 0.0456736758351326
step: 410, loss: 0.1114097535610199
step: 420, loss: 0.15165060758590698
step: 430, loss: 0.059451766312122345
step: 440, loss: 0.03079146146774292
step: 450, loss: 0.05027874559164047
step: 460, loss: 0.08514638245105743
step: 470, loss: 0.026820488274097443
step: 480, loss: 0.17629259824752808
step: 490, loss: 0.05587991327047348
step: 500, loss: 0.09537984430789948
step: 510, loss: 0.1314757764339447
step: 520, loss: 0.08742070943117142
step: 530, loss: 0.06717365235090256
step: 540, loss: 0.015301198698580265
step: 550, loss: 0.07327695190906525
step: 560, loss: 0.09750943630933762
step: 570, loss: 0.032750826328992844
step: 580, loss: 0.08346927911043167
step: 590, loss: 0.1621733009815216
step: 600, loss: 0.1533057540655136
step: 610, loss: 0.04782642051577568
step: 620, loss: 0.10030027478933334
step: 630, loss: 0.0248757041990757
step: 640, loss: 0.13109703361988068
step: 650, loss: 0.1811681091785431
step: 660, loss: 0.030738426372408867
step: 670, loss: 0.09570343047380447
step: 680, loss: 0.07074671983718872
step: 690, loss: 0.04390914738178253
step: 700, loss: 0.14388105273246765
step: 710, loss: 0.09381571412086487
step: 720, loss: 0.05554382875561714
step: 730, loss: 0.06478980928659439
step: 740, loss: 0.13350890576839447
step: 750, loss: 0.20585092902183533
step: 760, loss: 0.06431517004966736
step: 770, loss: 0.06543418020009995
step: 780, loss: 0.13302427530288696
step: 790, loss: 0.09416885673999786
step: 800, loss: 0.07304902374744415
step: 810, loss: 0.16810229420661926
step: 820, loss: 0.06862454861402512
step: 830, loss: 0.12109088897705078
step: 840, loss: 0.19425511360168457
step: 850, loss: 0.16215020418167114
step: 860, loss: 0.10613331198692322
step: 870, loss: 0.033411819487810135
step: 880, loss: 0.07237878441810608
step: 890, loss: 0.019138142466545105
step: 900, loss: 0.06412260979413986
step: 910, loss: 0.027793776243925095
step: 920, loss: 0.08249707520008087
step: 930, loss: 0.08359097689390182
step: 940, loss: 0.05275731161236763
step: 950, loss: 0.09084625542163849
step: 960, loss: 0.08809762448072433
step: 970, loss: 0.05253627151250839
epoch 8: dev_f1=0.9274965800273598, f1=0.9254545454545454, best_f1=0.9308118081180812
step: 0, loss: 0.10503166168928146
step: 10, loss: 0.03797586262226105
step: 20, loss: 0.042679961770772934
step: 30, loss: 0.021553128957748413
step: 40, loss: 0.07161305844783783
step: 50, loss: 0.10172226279973984
step: 60, loss: 0.0028655279893428087
step: 70, loss: 0.09039368480443954
step: 80, loss: 0.11714476346969604
step: 90, loss: 0.023641541600227356
step: 100, loss: 0.0787392258644104
step: 110, loss: 0.0017393914749845862
step: 120, loss: 0.06547436118125916
step: 130, loss: 0.08777515590190887
step: 140, loss: 0.08077560365200043
step: 150, loss: 0.14325428009033203
step: 160, loss: 0.07127691805362701
step: 170, loss: 0.017078781500458717
step: 180, loss: 0.034935060888528824
step: 190, loss: 0.1454060971736908
step: 200, loss: 0.025519538670778275
step: 210, loss: 0.1054452508687973
step: 220, loss: 0.09594442695379257
step: 230, loss: 0.006947021000087261
step: 240, loss: 0.038469623774290085
step: 250, loss: 0.07875578105449677
step: 260, loss: 0.15257233381271362
step: 270, loss: 0.16307619214057922
step: 280, loss: 0.09516492486000061
step: 290, loss: 0.03083169087767601
step: 300, loss: 0.12449538707733154
step: 310, loss: 0.12919510900974274
step: 320, loss: 0.050282418727874756
step: 330, loss: 0.04504159465432167
step: 340, loss: 0.1488097906112671
step: 350, loss: 0.033472940325737
step: 360, loss: 0.05996932461857796
step: 370, loss: 0.026745298877358437
step: 380, loss: 0.12618547677993774
step: 390, loss: 0.28161823749542236
step: 400, loss: 0.16811774671077728
step: 410, loss: 0.048273779451847076
step: 420, loss: 0.08319224417209625
step: 430, loss: 0.09066786617040634
step: 440, loss: 0.08803868293762207
step: 450, loss: 0.03924500569701195
step: 460, loss: 0.09201644361019135
step: 470, loss: 0.17110824584960938
step: 480, loss: 0.08589756488800049
step: 490, loss: 0.054566800594329834
step: 500, loss: 0.1215703934431076
step: 510, loss: 0.023280348628759384
step: 520, loss: 0.12546709179878235
step: 530, loss: 0.05431697145104408
step: 540, loss: 0.08447062969207764
step: 550, loss: 0.18282268941402435
step: 560, loss: 0.1100572943687439
step: 570, loss: 0.05502818152308464
step: 580, loss: 0.11317970603704453
step: 590, loss: 0.014882239513099194
step: 600, loss: 0.11249912530183792
step: 610, loss: 0.0970928892493248
step: 620, loss: 0.12836578488349915
step: 630, loss: 0.12472864240407944
step: 640, loss: 0.06316426396369934
step: 650, loss: 0.1280806064605713
step: 660, loss: 0.03279181942343712
step: 670, loss: 0.0749015137553215
step: 680, loss: 0.14083369076251984
step: 690, loss: 0.034511029720306396
step: 700, loss: 0.014520660042762756
step: 710, loss: 0.18599306046962738
step: 720, loss: 0.085238516330719
step: 730, loss: 0.06740298867225647
step: 740, loss: 0.09838005900382996
step: 750, loss: 0.18832892179489136
step: 760, loss: 0.06178516894578934
step: 770, loss: 0.04856351763010025
step: 780, loss: 0.09845160692930222
step: 790, loss: 0.11720053851604462
step: 800, loss: 0.14153578877449036
step: 810, loss: 0.22210317850112915
step: 820, loss: 0.11249715834856033
step: 830, loss: 0.07632927596569061
step: 840, loss: 0.14541207253932953
step: 850, loss: 0.23613779246807098
step: 860, loss: 0.05239243432879448
step: 870, loss: 0.1983838528394699
step: 880, loss: 0.09739701449871063
step: 890, loss: 0.11117278784513474
step: 900, loss: 0.12467215210199356
step: 910, loss: 0.05071599781513214
step: 920, loss: 0.12447481602430344
step: 930, loss: 0.14219829440116882
step: 940, loss: 0.03771797567605972
step: 950, loss: 0.12119105458259583
step: 960, loss: 0.10820552706718445
step: 970, loss: 0.115252785384655
epoch 9: dev_f1=0.926463700234192, f1=0.9282385834109972, best_f1=0.9308118081180812
step: 0, loss: 0.08328008651733398
step: 10, loss: 0.09089621901512146
step: 20, loss: 0.07053826004266739
step: 30, loss: 0.13502584397792816
step: 40, loss: 0.03815174475312233
step: 50, loss: 0.054358623921871185
step: 60, loss: 0.04749026894569397
step: 70, loss: 3.7230169255053625e-05
step: 80, loss: 0.012772120535373688
step: 90, loss: 0.1481630653142929
step: 100, loss: 0.06687334179878235
step: 110, loss: 0.0800306648015976
step: 120, loss: 0.1102919802069664
step: 130, loss: 0.06758172810077667
step: 140, loss: 0.09462722390890121
step: 150, loss: 0.13191130757331848
step: 160, loss: 0.07791700959205627
step: 170, loss: 0.08897894620895386
step: 180, loss: 0.04286981001496315
step: 190, loss: 0.015546307899057865
step: 200, loss: 0.11054366081953049
step: 210, loss: 0.10488259792327881
step: 220, loss: 0.062008172273635864
step: 230, loss: 0.046805933117866516
step: 240, loss: 0.042004458606243134
step: 250, loss: 0.04137261211872101
step: 260, loss: 0.09277353435754776
step: 270, loss: 0.10122097283601761
step: 280, loss: 0.0035767792724072933
step: 290, loss: 0.03954857960343361
step: 300, loss: 0.0310520026832819
step: 310, loss: 0.07938238978385925
step: 320, loss: 0.02332586608827114
step: 330, loss: 0.07213970273733139
step: 340, loss: 0.009011564776301384
step: 350, loss: 0.04275861382484436
step: 360, loss: 0.21793100237846375
step: 370, loss: 0.10890459269285202
step: 380, loss: 0.0006495912675745785
step: 390, loss: 0.03587953373789787
step: 400, loss: 0.0682578906416893
step: 410, loss: 0.08051899075508118
step: 420, loss: 0.07373768091201782
step: 430, loss: 0.01990622468292713
step: 440, loss: 0.07486209273338318
step: 450, loss: 0.10798516869544983
step: 460, loss: 0.037641849368810654
step: 470, loss: 0.13690365850925446
step: 480, loss: 0.10643786191940308
step: 490, loss: 0.05296589061617851
step: 500, loss: 0.13004416227340698
step: 510, loss: 0.025443773716688156
step: 520, loss: 0.0059697870165109634
step: 530, loss: 0.046321723610162735
step: 540, loss: 0.031283508986234665
step: 550, loss: 0.078789122402668
step: 560, loss: 0.06637752056121826
step: 570, loss: 0.0954219251871109
step: 580, loss: 0.17776130139827728
step: 590, loss: 0.10110760480165482
step: 600, loss: 0.1716964989900589
step: 610, loss: 0.1280362904071808
step: 620, loss: 0.14251862466335297
step: 630, loss: 0.04519923776388168
step: 640, loss: 0.1717158854007721
step: 650, loss: 0.011586958542466164
step: 660, loss: 0.00737758819013834
step: 670, loss: 0.1630788892507553
step: 680, loss: 0.08955005556344986
step: 690, loss: 0.0311612356454134
step: 700, loss: 0.044492900371551514
step: 710, loss: 0.08938020467758179
step: 720, loss: 0.15326513350009918
step: 730, loss: 0.1268678456544876
step: 740, loss: 0.12650930881500244
step: 750, loss: 0.0892544835805893
step: 760, loss: 0.10310287028551102
step: 770, loss: 0.1227262020111084
step: 780, loss: 0.060635410249233246
step: 790, loss: 0.07065141946077347
step: 800, loss: 0.07204344868659973
step: 810, loss: 0.048596665263175964
step: 820, loss: 0.04536987841129303
step: 830, loss: 0.08569358289241791
step: 840, loss: 0.014628049917519093
step: 850, loss: 0.11893805116415024
step: 860, loss: 0.07516773045063019
step: 870, loss: 0.04775374382734299
step: 880, loss: 0.1689278483390808
step: 890, loss: 0.12546482682228088
step: 900, loss: 0.08471997082233429
step: 910, loss: 0.08709076792001724
step: 920, loss: 0.0816214308142662
step: 930, loss: 0.0938677191734314
step: 940, loss: 0.15659821033477783
step: 950, loss: 0.17637942731380463
step: 960, loss: 0.09475070238113403
step: 970, loss: 0.0817141979932785
epoch 10: dev_f1=0.9303391384051329, f1=0.9296160877513711, best_f1=0.9308118081180812
step: 0, loss: 0.03863830491900444
step: 10, loss: 0.06100236624479294
step: 20, loss: 0.12331825494766235
step: 30, loss: 0.03929583728313446
step: 40, loss: 0.08878164738416672
step: 50, loss: 0.03379322215914726
step: 60, loss: 0.09813632071018219
step: 70, loss: 0.10562878102064133
step: 80, loss: 0.1359330415725708
step: 90, loss: 0.11528344452381134
step: 100, loss: 0.07142244279384613
step: 110, loss: 0.11734245717525482
step: 120, loss: 0.040060196071863174
step: 130, loss: 0.07986290752887726
step: 140, loss: 0.03794457018375397
step: 150, loss: 0.061678532510995865
step: 160, loss: 0.14519177377223969
step: 170, loss: 0.056704796850681305
step: 180, loss: 0.060300786048173904
step: 190, loss: 0.05650420859456062
step: 200, loss: 0.036961525678634644
step: 210, loss: 0.03271118924021721
step: 220, loss: 0.022542854771018028
step: 230, loss: 0.14187368750572205
step: 240, loss: 0.0679672509431839
step: 250, loss: 0.031623777002096176
step: 260, loss: 0.11172216385602951
step: 270, loss: 0.05244210362434387
step: 280, loss: 0.010938053019344807
step: 290, loss: 0.11508449912071228
step: 300, loss: 0.025833148509263992
step: 310, loss: 0.11772728711366653
step: 320, loss: 0.15638571977615356
step: 330, loss: 0.05806807056069374
step: 340, loss: 0.04697199910879135
step: 350, loss: 0.061435434967279434
step: 360, loss: 0.28975456953048706
step: 370, loss: 0.03847796842455864
step: 380, loss: 0.044502630829811096
step: 390, loss: 0.014583153650164604
step: 400, loss: 0.0333426408469677
step: 410, loss: 0.08804205060005188
step: 420, loss: 0.07753879576921463
step: 430, loss: 0.020879866555333138
step: 440, loss: 0.10147229582071304
step: 450, loss: 0.07847683876752853
step: 460, loss: 0.11605913937091827
step: 470, loss: 0.02966654859483242
step: 480, loss: 0.22690527141094208
step: 490, loss: 0.05906889960169792
step: 500, loss: 0.16469475626945496
step: 510, loss: 0.05245614051818848
step: 520, loss: 0.06774801015853882
step: 530, loss: 0.13496072590351105
step: 540, loss: 0.02588607557117939
step: 550, loss: 0.1268799602985382
step: 560, loss: 0.08020422607660294
step: 570, loss: 0.05657980591058731
step: 580, loss: 0.07059670239686966
step: 590, loss: 0.06067155301570892
step: 600, loss: 0.08392547816038132
step: 610, loss: 0.13411009311676025
step: 620, loss: 0.09141062945127487
step: 630, loss: 0.025014163926243782
step: 640, loss: 0.03287697210907936
step: 650, loss: 0.03449234366416931
step: 660, loss: 0.1413339227437973
step: 670, loss: 0.19485129415988922
step: 680, loss: 0.11850373446941376
step: 690, loss: 0.10487924516201019
step: 700, loss: 0.03242018446326256
step: 710, loss: 0.0489688403904438
step: 720, loss: 0.05632249265909195
step: 730, loss: 0.05860493332147598
step: 740, loss: 0.07776331901550293
step: 750, loss: 0.08674964308738708
step: 760, loss: 0.05668902397155762
step: 770, loss: 0.07306428998708725
step: 780, loss: 0.05478272587060928
step: 790, loss: 0.060954805463552475
step: 800, loss: 0.05490761250257492
step: 810, loss: 0.11600998789072037
step: 820, loss: 0.20326067507266998
step: 830, loss: 0.05061667412519455
step: 840, loss: 0.1069079115986824
step: 850, loss: 0.05496939271688461
step: 860, loss: 0.00018674346210900694
step: 870, loss: 0.11625780910253525
step: 880, loss: 0.09046540409326553
step: 890, loss: 0.011339893564581871
step: 900, loss: 0.06794586777687073
step: 910, loss: 0.08036254346370697
step: 920, loss: 0.031685587018728256
step: 930, loss: 0.06747454404830933
step: 940, loss: 0.1475834846496582
step: 950, loss: 0.13392341136932373
step: 960, loss: 0.030704261735081673
step: 970, loss: 0.16047941148281097
epoch 11: dev_f1=0.9270307480495641, f1=0.919908466819222, best_f1=0.9308118081180812
step: 0, loss: 0.052786100655794144
step: 10, loss: 0.12037546187639236
step: 20, loss: 0.01044920552521944
step: 30, loss: 0.058334723114967346
step: 40, loss: 0.13901475071907043
step: 50, loss: 0.05318065732717514
step: 60, loss: 0.038094282150268555
step: 70, loss: 0.06046895682811737
step: 80, loss: 0.1007571667432785
step: 90, loss: 0.04432036355137825
step: 100, loss: 0.017209067940711975
step: 110, loss: 0.03363053500652313
step: 120, loss: 0.08907502889633179
step: 130, loss: 0.04565225541591644
step: 140, loss: 0.057980019599199295
step: 150, loss: 0.1574506163597107
step: 160, loss: 0.05437082424759865
step: 170, loss: 0.019733095541596413
step: 180, loss: 0.004554910119622946
step: 190, loss: 0.10319013148546219
step: 200, loss: 0.07191786915063858
step: 210, loss: 0.03601919859647751
step: 220, loss: 0.020589197054505348
step: 230, loss: 0.030811579897999763
step: 240, loss: 0.017466368153691292
step: 250, loss: 0.031678613275289536
step: 260, loss: 0.08143290132284164
step: 270, loss: 0.034117139875888824
step: 280, loss: 0.057436056435108185
step: 290, loss: 0.08526790887117386
step: 300, loss: 0.04220922291278839
step: 310, loss: 0.027140481397509575
step: 320, loss: 0.041192684322595596
step: 330, loss: 0.16765737533569336
step: 340, loss: 0.04953911155462265
step: 350, loss: 0.06997857987880707
step: 360, loss: 0.0377156063914299
step: 370, loss: 0.07841189205646515
step: 380, loss: 0.06003984436392784
step: 390, loss: 0.11364578455686569
step: 400, loss: 0.06191012263298035
step: 410, loss: 0.04248926043510437
step: 420, loss: 0.04822177439928055
step: 430, loss: 0.09859476238489151
step: 440, loss: 0.1262625902891159
step: 450, loss: 0.09589583426713943
step: 460, loss: 0.03242064639925957
step: 470, loss: 0.07847093790769577
step: 480, loss: 0.08294069021940231
step: 490, loss: 0.06573811918497086
step: 500, loss: 0.05546843260526657
step: 510, loss: 0.060544952750205994
step: 520, loss: 0.15397699177265167
step: 530, loss: 0.050409428775310516
step: 540, loss: 0.01846686750650406
step: 550, loss: 0.09549173712730408
step: 560, loss: 0.040432002395391464
step: 570, loss: 0.12738782167434692
step: 580, loss: 0.07349862158298492
step: 590, loss: 0.04704221338033676
step: 600, loss: 0.09269095212221146
step: 610, loss: 0.03907958045601845
step: 620, loss: 0.058998383581638336
step: 630, loss: 0.09704357385635376
step: 640, loss: 0.07234283536672592
step: 650, loss: 0.09288205206394196
step: 660, loss: 0.038830216974020004
step: 670, loss: 0.10131785273551941
step: 680, loss: 0.026214411482214928
step: 690, loss: 0.00020468175353016704
step: 700, loss: 0.06802210211753845
step: 710, loss: 0.019035808742046356
step: 720, loss: 0.11136572808027267
step: 730, loss: 0.09587006270885468
step: 740, loss: 0.06495754420757294
step: 750, loss: 0.13123014569282532
step: 760, loss: 0.03866337239742279
step: 770, loss: 0.03752538561820984
step: 780, loss: 0.0743299126625061
step: 790, loss: 0.046874143183231354
step: 800, loss: 0.050832249224185944
step: 810, loss: 0.08479485660791397
step: 820, loss: 0.08995551615953445
step: 830, loss: 0.10772982239723206
step: 840, loss: 0.05142620578408241
step: 850, loss: 0.09694761782884598
step: 860, loss: 0.14963752031326294
step: 870, loss: 0.08806464821100235
step: 880, loss: 0.052462782710790634
step: 890, loss: 0.013185552321374416
step: 900, loss: 0.08527427166700363
step: 910, loss: 0.06213945150375366
step: 920, loss: 0.03675086051225662
step: 930, loss: 0.05485966056585312
step: 940, loss: 0.09031572937965393
step: 950, loss: 0.03176233172416687
step: 960, loss: 0.04046059399843216
step: 970, loss: 0.04430609941482544
epoch 12: dev_f1=0.9274563820018366, f1=0.920402561756633, best_f1=0.9308118081180812
step: 0, loss: 0.08445277065038681
step: 10, loss: 0.00877651758491993
step: 20, loss: 0.028714144602417946
step: 30, loss: 0.006528888829052448
step: 40, loss: 0.018230494111776352
step: 50, loss: 0.028577350080013275
step: 60, loss: 0.015416386537253857
step: 70, loss: 0.029856331646442413
step: 80, loss: 0.06984874606132507
step: 90, loss: 0.027299124747514725
step: 100, loss: 0.04805196449160576
step: 110, loss: 0.010789759457111359
step: 120, loss: 0.05995217338204384
step: 130, loss: 0.07860618829727173
step: 140, loss: 0.04318990930914879
step: 150, loss: 0.08559741079807281
step: 160, loss: 0.12484017014503479
step: 170, loss: 0.08012837171554565
step: 180, loss: 0.07319852709770203
step: 190, loss: 0.006695065647363663
step: 200, loss: 0.08074405789375305
step: 210, loss: 0.08131415396928787
step: 220, loss: 0.10787342488765717
step: 230, loss: 0.027447326108813286
step: 240, loss: 0.02421102486550808
step: 250, loss: 0.047677505761384964
step: 260, loss: 0.029086995869874954
step: 270, loss: 0.0016157488571479917
step: 280, loss: 0.1503458172082901
step: 290, loss: 0.12887582182884216
step: 300, loss: 0.019523561000823975
step: 310, loss: 0.09378457814455032
step: 320, loss: 0.03555663675069809
step: 330, loss: 0.09523753076791763
step: 340, loss: 0.012987479567527771
step: 350, loss: 0.012068996205925941
step: 360, loss: 0.05830090865492821
step: 370, loss: 0.05904002487659454
step: 380, loss: 0.02447447180747986
step: 390, loss: 0.050654202699661255
step: 400, loss: 0.062169209122657776
step: 410, loss: 0.043074287474155426
step: 420, loss: 0.03687863051891327
step: 430, loss: 0.06738866865634918
step: 440, loss: 0.1083870530128479
step: 450, loss: 0.10226216912269592
step: 460, loss: 0.09862056374549866
step: 470, loss: 0.08433791249990463
step: 480, loss: 0.014209826476871967
step: 490, loss: 0.03014419972896576
step: 500, loss: 0.019372252747416496
step: 510, loss: 0.1889529526233673
step: 520, loss: 0.0375477597117424
step: 530, loss: 0.04462939128279686
step: 540, loss: 0.11823467910289764
step: 550, loss: 0.1818443387746811
step: 560, loss: 0.030492832884192467
step: 570, loss: 0.0018104364862665534
step: 580, loss: 0.1006615161895752
step: 590, loss: 0.06203746423125267
step: 600, loss: 0.15934470295906067
step: 610, loss: 0.08325329422950745
step: 620, loss: 0.05651583895087242
step: 630, loss: 0.06998956948518753
step: 640, loss: 0.038862381130456924
step: 650, loss: 0.049831572920084
step: 660, loss: 0.07277873158454895
step: 670, loss: 0.07642737776041031
step: 680, loss: 0.08229891955852509
step: 690, loss: 0.11270531266927719
step: 700, loss: 0.02043132856488228
step: 710, loss: 0.06195898354053497
step: 720, loss: 0.039788614958524704
step: 730, loss: 0.02755468711256981
step: 740, loss: 0.007570467423647642
step: 750, loss: 0.07646206766366959
step: 760, loss: 0.05799689143896103
step: 770, loss: 0.06540369987487793
step: 780, loss: 0.02270939201116562
step: 790, loss: 0.066507987678051
step: 800, loss: 0.05076233670115471
step: 810, loss: 0.07665503770112991
step: 820, loss: 0.08621492236852646
step: 830, loss: 0.11049514263868332
step: 840, loss: 0.0598975233733654
step: 850, loss: 0.07110833376646042
step: 860, loss: 0.028738966211676598
step: 870, loss: 0.16494807600975037
step: 880, loss: 0.137269526720047
step: 890, loss: 0.04265065863728523
step: 900, loss: 0.021739918738603592
step: 910, loss: 0.257404625415802
step: 920, loss: 0.042811691761016846
step: 930, loss: 0.05297021567821503
step: 940, loss: 0.07089542597532272
step: 950, loss: 0.03231503441929817
step: 960, loss: 0.08585929125547409
step: 970, loss: 0.06721145659685135
epoch 13: dev_f1=0.9305747126436781, f1=0.9249084249084248, best_f1=0.9308118081180812
step: 0, loss: 0.07356175780296326
step: 10, loss: 0.08065110445022583
step: 20, loss: 0.08490914851427078
step: 30, loss: 0.032403405755758286
step: 40, loss: 0.01784287765622139
step: 50, loss: 0.013674118556082249
step: 60, loss: 0.04311969131231308
step: 70, loss: 0.025470100343227386
step: 80, loss: 0.025414951145648956
step: 90, loss: 0.02106359414756298
step: 100, loss: 0.04843898117542267
step: 110, loss: 0.04493655264377594
step: 120, loss: 0.03264102712273598
step: 130, loss: 0.09248585253953934
step: 140, loss: 0.04728486388921738
step: 150, loss: 0.0605926513671875
step: 160, loss: 0.0024652103893458843
step: 170, loss: 0.060120727866888046
step: 180, loss: 0.047551676630973816
step: 190, loss: 0.030240822583436966
step: 200, loss: 0.004838706459850073
step: 210, loss: 0.057738639414310455
step: 220, loss: 0.02410261332988739
step: 230, loss: 0.11536440253257751
step: 240, loss: 0.001645446289330721
step: 250, loss: 0.04922422766685486
step: 260, loss: 0.025974201038479805
step: 270, loss: 0.08935431391000748
step: 280, loss: 0.050768375396728516
step: 290, loss: 0.058761607855558395
step: 300, loss: 0.07357633858919144
step: 310, loss: 0.04980708286166191
step: 320, loss: 0.039060112088918686
step: 330, loss: 0.17842787504196167
step: 340, loss: 0.03385467827320099
step: 350, loss: 0.01978764683008194
step: 360, loss: 0.03365796059370041
step: 370, loss: 0.018635675311088562
step: 380, loss: 0.10018391162157059
step: 390, loss: 0.04204937815666199
step: 400, loss: 0.047901272773742676
step: 410, loss: 0.011412171646952629
step: 420, loss: 0.02071373164653778
step: 430, loss: 0.12268777191638947
step: 440, loss: 0.007599301636219025
step: 450, loss: 0.0625796690583229
step: 460, loss: 0.04621373489499092
step: 470, loss: 0.02467331290245056
step: 480, loss: 0.019578760489821434
step: 490, loss: 0.07374019920825958
step: 500, loss: 0.054731518030166626
step: 510, loss: 0.059099242091178894
step: 520, loss: 0.00642608106136322
step: 530, loss: 0.01609535701572895
step: 540, loss: 0.10645884275436401
step: 550, loss: 0.07495880126953125
step: 560, loss: 0.0308319553732872
step: 570, loss: 0.04806962236762047
step: 580, loss: 0.06650262326002121
step: 590, loss: 0.04274372011423111
step: 600, loss: 0.023307567462325096
step: 610, loss: 0.1135483831167221
step: 620, loss: 0.007788540795445442
step: 630, loss: 0.04943598061800003
step: 640, loss: 0.10644232481718063
step: 650, loss: 0.06368514150381088
step: 660, loss: 0.025640979409217834
step: 670, loss: 0.10068625211715698
step: 680, loss: 0.07441401481628418
step: 690, loss: 0.0642898678779602
step: 700, loss: 0.04137573018670082
step: 710, loss: 0.04615200683474541
step: 720, loss: 0.08285218477249146
step: 730, loss: 0.010281834751367569
step: 740, loss: 0.006639576982706785
step: 750, loss: 0.00918451976031065
step: 760, loss: 0.17849792540073395
step: 770, loss: 0.05644127354025841
step: 780, loss: 5.315395901561715e-05
step: 790, loss: 0.04818277433514595
step: 800, loss: 0.0815463736653328
step: 810, loss: 0.05290118604898453
step: 820, loss: 0.08356378227472305
step: 830, loss: 0.06623109430074692
step: 840, loss: 0.04059004783630371
step: 850, loss: 0.1190672218799591
step: 860, loss: 0.09508813172578812
step: 870, loss: 0.056722186505794525
step: 880, loss: 0.15763242542743683
step: 890, loss: 0.0771305039525032
step: 900, loss: 0.16738826036453247
step: 910, loss: 0.04075491428375244
step: 920, loss: 0.04631934314966202
step: 930, loss: 0.08094411343336105
step: 940, loss: 0.0004743855679407716
step: 950, loss: 0.054282959550619125
step: 960, loss: 0.060526855289936066
step: 970, loss: 0.02732706442475319
epoch 14: dev_f1=0.9294605809128631, f1=0.9239280774550485, best_f1=0.9308118081180812
step: 0, loss: 0.03361239656805992
step: 10, loss: 0.03047075867652893
step: 20, loss: 0.11323730647563934
step: 30, loss: 0.06378179788589478
step: 40, loss: 0.07833512872457504
step: 50, loss: 0.05423101782798767
step: 60, loss: 0.1539068967103958
step: 70, loss: 0.07547124475240707
step: 80, loss: 0.0227590911090374
step: 90, loss: 0.03241046890616417
step: 100, loss: 0.035866107791662216
step: 110, loss: 0.00012443342711776495
step: 120, loss: 0.07240042835474014
step: 130, loss: 0.029611550271511078
step: 140, loss: 0.009163115173578262
step: 150, loss: 0.041737958788871765
step: 160, loss: 1.2338049600657541e-05
step: 170, loss: 0.03325739502906799
step: 180, loss: 0.02555624581873417
step: 190, loss: 0.18962979316711426
step: 200, loss: 0.03986596688628197
step: 210, loss: 0.10512565821409225
step: 220, loss: 0.10273310542106628
step: 230, loss: 0.10623662918806076
step: 240, loss: 0.036595217883586884
step: 250, loss: 0.07701212167739868
step: 260, loss: 0.022802166640758514
step: 270, loss: 0.042972199618816376
step: 280, loss: 0.04547679424285889
step: 290, loss: 0.02455974742770195
step: 300, loss: 0.09044237434864044
step: 310, loss: 0.007664707489311695
step: 320, loss: 0.013377374038100243
step: 330, loss: 0.025282321497797966
step: 340, loss: 0.025195183232426643
step: 350, loss: 0.10547938197851181
step: 360, loss: 0.04402853175997734
step: 370, loss: 0.020820001140236855
step: 380, loss: 0.0894625261425972
step: 390, loss: 0.08636483550071716
step: 400, loss: 0.03444703295826912
step: 410, loss: 0.027346881106495857
step: 420, loss: 0.11409536004066467
step: 430, loss: 0.08414256572723389
step: 440, loss: 0.03071397729218006
step: 450, loss: 0.011765995062887669
step: 460, loss: 0.017924729734659195
step: 470, loss: 0.07719532400369644
step: 480, loss: 0.10468451678752899
step: 490, loss: 0.02444382756948471
step: 500, loss: 0.10056887567043304
step: 510, loss: 0.14759089052677155
step: 520, loss: 0.0072743999771773815
step: 530, loss: 0.08658523112535477
step: 540, loss: 0.05041957274079323
step: 550, loss: 0.09419765323400497
step: 560, loss: 0.03894897550344467
step: 570, loss: 0.06747553497552872
step: 580, loss: 0.0634293183684349
step: 590, loss: 0.03132953122258186
step: 600, loss: 0.00010078710329253227
step: 610, loss: 0.09408450126647949
step: 620, loss: 0.006574071478098631
step: 630, loss: 0.03909463435411453
step: 640, loss: 0.07807651907205582
step: 650, loss: 0.0350540392100811
step: 660, loss: 0.04604422673583031
step: 670, loss: 0.07272287458181381
step: 680, loss: 0.10365196317434311
step: 690, loss: 0.11615914851427078
step: 700, loss: 0.10682043433189392
step: 710, loss: 0.06133054569363594
step: 720, loss: 0.017828399315476418
step: 730, loss: 0.14477181434631348
step: 740, loss: 0.018763944506645203
step: 750, loss: 0.0701567530632019
step: 760, loss: 0.026415877044200897
step: 770, loss: 0.01991959661245346
step: 780, loss: 0.0892430990934372
step: 790, loss: 0.07767751812934875
step: 800, loss: 0.06454550474882126
step: 810, loss: 0.08168673515319824
step: 820, loss: 0.024696873500943184
step: 830, loss: 0.04145856201648712
step: 840, loss: 0.03167405351996422
step: 850, loss: 0.003045047167688608
step: 860, loss: 0.06380151957273483
step: 870, loss: 0.017436370253562927
step: 880, loss: 0.02526138350367546
step: 890, loss: 0.010296549648046494
step: 900, loss: 0.189261332154274
step: 910, loss: 0.031542882323265076
step: 920, loss: 0.09063242375850677
step: 930, loss: 0.043711964040994644
step: 940, loss: 0.08949832618236542
step: 950, loss: 0.04869995638728142
step: 960, loss: 0.17364349961280823
step: 970, loss: 0.06528163701295853
epoch 15: dev_f1=0.9335180055401663, f1=0.9278445883441258, best_f1=0.9308118081180812
step: 0, loss: 0.05741836875677109
step: 10, loss: 0.06350775063037872
step: 20, loss: 0.01820491999387741
step: 30, loss: 0.07761576771736145
step: 40, loss: 0.04527832567691803
step: 50, loss: 0.00010142679093405604
step: 60, loss: 0.0006245072581805289
step: 70, loss: 0.05347326397895813
step: 80, loss: 0.044406209141016006
step: 90, loss: 0.048281844705343246
step: 100, loss: 0.04638191685080528
step: 110, loss: 0.01559869572520256
step: 120, loss: 0.054655641317367554
step: 130, loss: 0.031879011541604996
step: 140, loss: 0.06518794596195221
step: 150, loss: 0.04517275094985962
step: 160, loss: 0.05927808955311775
step: 170, loss: 0.08674962818622589
step: 180, loss: 0.0706227496266365
step: 190, loss: 0.08459983766078949
step: 200, loss: 0.10674368590116501
step: 210, loss: 0.0002824482216965407
step: 220, loss: 0.008651971817016602
step: 230, loss: 0.04712647944688797
step: 240, loss: 0.04483361169695854
step: 250, loss: 0.08143309503793716
step: 260, loss: 0.06947322189807892
step: 270, loss: 0.052478935569524765
step: 280, loss: 0.07286903262138367
step: 290, loss: 0.06022976338863373
step: 300, loss: 0.09803134948015213
step: 310, loss: 0.016301482915878296
step: 320, loss: 0.08458320051431656
step: 330, loss: 0.0014401900116354227
step: 340, loss: 0.078868567943573
step: 350, loss: 0.06622594594955444
step: 360, loss: 0.010453620925545692
step: 370, loss: 0.018869180232286453
step: 380, loss: 0.0013171888422220945
step: 390, loss: 0.0893845185637474
step: 400, loss: 0.0967547744512558
step: 410, loss: 0.009233328513801098
step: 420, loss: 0.026174332946538925
step: 430, loss: 0.05805499479174614
step: 440, loss: 0.032496128231287
step: 450, loss: 8.492750203004107e-05
step: 460, loss: 0.034550923854112625
step: 470, loss: 0.014138385653495789
step: 480, loss: 0.005117716267704964
step: 490, loss: 0.07877153158187866
step: 500, loss: 0.06218256801366806
step: 510, loss: 0.05545482411980629
step: 520, loss: 0.05294473096728325
step: 530, loss: 0.0741589218378067
step: 540, loss: 0.09128284454345703
step: 550, loss: 0.13814449310302734
step: 560, loss: 0.05933716893196106
step: 570, loss: 0.03982444852590561
step: 580, loss: 0.0756065845489502
step: 590, loss: 0.05103399604558945
step: 600, loss: 0.008719487115740776
step: 610, loss: 0.03896570950746536
step: 620, loss: 0.00024975326959975064
step: 630, loss: 0.016814831644296646
step: 640, loss: 0.011887199245393276
step: 650, loss: 0.06628545373678207
step: 660, loss: 0.045239463448524475
step: 670, loss: 0.024554841220378876
step: 680, loss: 0.08323948085308075
step: 690, loss: 0.030664142221212387
step: 700, loss: 0.07544639706611633
step: 710, loss: 0.043527327477931976
step: 720, loss: 0.04460927098989487
step: 730, loss: 0.16266502439975739
step: 740, loss: 0.00013153442705515772
step: 750, loss: 0.08340998739004135
step: 760, loss: 0.051521725952625275
step: 770, loss: 0.004952945746481419
step: 780, loss: 0.08283644914627075
step: 790, loss: 0.04082819074392319
step: 800, loss: 0.038298413157463074
step: 810, loss: 0.03927048668265343
step: 820, loss: 0.08774251490831375
step: 830, loss: 0.037951331585645676
step: 840, loss: 0.043902844190597534
step: 850, loss: 0.11583451926708221
step: 860, loss: 0.03923901170492172
step: 870, loss: 0.1795847862958908
step: 880, loss: 0.12705132365226746
step: 890, loss: 0.02774105966091156
step: 900, loss: 0.0395287349820137
step: 910, loss: 0.02248552441596985
step: 920, loss: 0.0019395644776523113
step: 930, loss: 0.060456786304712296
step: 940, loss: 0.08855677396059036
step: 950, loss: 0.013753541745245457
step: 960, loss: 0.02209613472223282
step: 970, loss: 0.014306871220469475
epoch 16: dev_f1=0.9247015610651974, f1=0.9205328433624255, best_f1=0.9308118081180812
step: 0, loss: 0.06729254126548767
step: 10, loss: 0.04320545494556427
step: 20, loss: 0.028427675366401672
step: 30, loss: 0.02696433663368225
step: 40, loss: 0.0337393544614315
step: 50, loss: 0.02107354812324047
step: 60, loss: 0.0001047129844664596
step: 70, loss: 0.05011334642767906
step: 80, loss: 0.07086211442947388
step: 90, loss: 0.09258073568344116
step: 100, loss: 0.03643956035375595
step: 110, loss: 0.02175980433821678
step: 120, loss: 0.10111326724290848
step: 130, loss: 0.07678201794624329
step: 140, loss: 0.01860017515718937
step: 150, loss: 0.030808746814727783
step: 160, loss: 0.0002647687215358019
step: 170, loss: 0.015758756548166275
step: 180, loss: 0.11181703209877014
step: 190, loss: 0.0251283161342144
step: 200, loss: 0.022558890283107758
step: 210, loss: 0.09977145493030548
step: 220, loss: 0.05051887407898903
step: 230, loss: 0.051252011209726334
step: 240, loss: 0.06958917528390884
step: 250, loss: 0.02550417371094227
step: 260, loss: 0.05530620738863945
step: 270, loss: 0.14293114840984344
step: 280, loss: 0.1513952612876892
step: 290, loss: 0.008565463125705719
step: 300, loss: 0.01258538942784071
step: 310, loss: 0.014482902362942696
step: 320, loss: 0.054879993200302124
step: 330, loss: 0.10203973948955536
step: 340, loss: 0.0480399914085865
step: 350, loss: 0.07045849412679672
step: 360, loss: 0.030551936477422714
step: 370, loss: 0.061420731246471405
step: 380, loss: 0.001744582550600171
step: 390, loss: 0.06050622835755348
step: 400, loss: 0.05052584409713745
step: 410, loss: 0.058863405138254166
step: 420, loss: 0.045583050698041916
step: 430, loss: 0.008306448347866535
step: 440, loss: 0.13827142119407654
step: 450, loss: 0.02475542575120926
step: 460, loss: 0.00018510155496187508
step: 470, loss: 0.020612169057130814
step: 480, loss: 0.0954325944185257
step: 490, loss: 0.08884646743535995
step: 500, loss: 0.08266884833574295
step: 510, loss: 0.042264826595783234
step: 520, loss: 0.01449980866163969
step: 530, loss: 0.1013205498456955
step: 540, loss: 0.053161680698394775
step: 550, loss: 0.0743701383471489
step: 560, loss: 0.01993001438677311
step: 570, loss: 0.05872261896729469
step: 580, loss: 0.02675590291619301
step: 590, loss: 0.015308469533920288
step: 600, loss: 0.06268363445997238
step: 610, loss: 0.0022036386653780937
step: 620, loss: 0.032958004623651505
step: 630, loss: 0.028139252215623856
step: 640, loss: 0.007988006807863712
step: 650, loss: 0.06007956713438034
step: 660, loss: 0.10056395083665848
step: 670, loss: 0.06483594328165054
step: 680, loss: 0.04599504545331001
step: 690, loss: 0.03575574979186058
step: 700, loss: 0.07417768239974976
step: 710, loss: 0.042816467583179474
step: 720, loss: 0.059265296906232834
step: 730, loss: 0.060660187155008316
step: 740, loss: 0.0657319501042366
step: 750, loss: 0.07425493001937866
step: 760, loss: 0.041881728917360306
step: 770, loss: 0.05850491300225258
step: 780, loss: 0.0039032562635838985
step: 790, loss: 0.027454063296318054
step: 800, loss: 0.09169671684503555
step: 810, loss: 0.0765959620475769
step: 820, loss: 0.07499909400939941
step: 830, loss: 0.026826824992895126
step: 840, loss: 0.010328003205358982
step: 850, loss: 0.05242452025413513
step: 860, loss: 0.04967854171991348
step: 870, loss: 0.07073970884084702
step: 880, loss: 0.021994780749082565
step: 890, loss: 0.030521519482135773
step: 900, loss: 0.05158739537000656
step: 910, loss: 0.03510694578289986
step: 920, loss: 0.05325530841946602
step: 930, loss: 0.06075451150536537
step: 940, loss: 0.09227874130010605
step: 950, loss: 0.05728379637002945
step: 960, loss: 0.03627001494169235
step: 970, loss: 0.06790950894355774
epoch 17: dev_f1=0.9255813953488372, f1=0.9250116441546343, best_f1=0.9308118081180812
step: 0, loss: 0.030910465866327286
step: 10, loss: 0.05593768507242203
step: 20, loss: 0.10465626418590546
step: 30, loss: 0.03580174595117569
step: 40, loss: 0.07194425165653229
step: 50, loss: 0.08619228005409241
step: 60, loss: 0.010094637051224709
step: 70, loss: 0.02614353969693184
step: 80, loss: 0.045993346720933914
step: 90, loss: 0.02385445311665535
step: 100, loss: 0.06256160140037537
step: 110, loss: 0.03288491442799568
step: 120, loss: 2.59958778769942e-05
step: 130, loss: 0.008570889942348003
step: 140, loss: 0.015422462485730648
step: 150, loss: 8.873586011759471e-06
step: 160, loss: 0.056998543441295624
step: 170, loss: 0.1028689593076706
step: 180, loss: 0.18757884204387665
step: 190, loss: 0.02084505930542946
step: 200, loss: 0.08294063806533813
step: 210, loss: 0.043337736278772354
step: 220, loss: 0.09833121299743652
step: 230, loss: 0.12834429740905762
step: 240, loss: 0.00034530070843175054
step: 250, loss: 0.04687518998980522
step: 260, loss: 0.0317615382373333
step: 270, loss: 0.1014777347445488
step: 280, loss: 0.009756860323250294
step: 290, loss: 0.18144352734088898
step: 300, loss: 0.035824041813611984
step: 310, loss: 0.015868602320551872
step: 320, loss: 0.03772157058119774
step: 330, loss: 0.07567110657691956
step: 340, loss: 0.03841855004429817
step: 350, loss: 0.07776179164648056
step: 360, loss: 0.04326559230685234
step: 370, loss: 0.023757904767990112
step: 380, loss: 0.028683563694357872
step: 390, loss: 0.0692552998661995
step: 400, loss: 0.09108220040798187
step: 410, loss: 0.018556363880634308
step: 420, loss: 0.04037861153483391
step: 430, loss: 0.029856398701667786
step: 440, loss: 0.016876062378287315
step: 450, loss: 0.07252050936222076
step: 460, loss: 0.1531401127576828
step: 470, loss: 0.07245134562253952
step: 480, loss: 0.021388409659266472
step: 490, loss: 0.004547014366835356
step: 500, loss: 0.03532418608665466
step: 510, loss: 0.021673589944839478
step: 520, loss: 0.006082945503294468
step: 530, loss: 0.2457410842180252
step: 540, loss: 0.05103618651628494
step: 550, loss: 0.0709545910358429
step: 560, loss: 0.05440450832247734
step: 570, loss: 0.19417229294776917
step: 580, loss: 0.05843755230307579
step: 590, loss: 0.050031956285238266
step: 600, loss: 0.10754731297492981
step: 610, loss: 0.16157610714435577
step: 620, loss: 0.03682512044906616
step: 630, loss: 0.054292403161525726
step: 640, loss: 0.14205138385295868
step: 650, loss: 0.02906748279929161
step: 660, loss: 0.013625247403979301
step: 670, loss: 0.0011263089254498482
step: 680, loss: 7.069151615723968e-05
step: 690, loss: 0.00016485759988427162
step: 700, loss: 0.05477875843644142
step: 710, loss: 0.04010491818189621
step: 720, loss: 0.0294874906539917
step: 730, loss: 0.013053261674940586
step: 740, loss: 0.04721614718437195
step: 750, loss: 0.03931547328829765
step: 760, loss: 0.0090663256123662
step: 770, loss: 0.10696006566286087
step: 780, loss: 0.01792723871767521
step: 790, loss: 0.0004589426680468023
step: 800, loss: 0.016327494755387306
step: 810, loss: 0.021281439810991287
step: 820, loss: 0.04297550022602081
step: 830, loss: 0.015281375497579575
step: 840, loss: 0.022496603429317474
step: 850, loss: 0.05535202845931053
step: 860, loss: 0.11797674000263214
step: 870, loss: 0.043772902339696884
step: 880, loss: 0.12497404217720032
step: 890, loss: 0.008949839510023594
step: 900, loss: 0.011026537045836449
step: 910, loss: 0.10240591317415237
step: 920, loss: 7.653053035028279e-05
step: 930, loss: 5.950312697677873e-05
step: 940, loss: 0.000981273828074336
step: 950, loss: 0.04712611436843872
step: 960, loss: 0.10230324417352676
step: 970, loss: 0.024471499025821686
epoch 18: dev_f1=0.9263746505125815, f1=0.9265116279069768, best_f1=0.9308118081180812
step: 0, loss: 0.09194497019052505
step: 10, loss: 0.009233545511960983
step: 20, loss: 0.03158974274992943
step: 30, loss: 0.06762662529945374
step: 40, loss: 0.0810297429561615
step: 50, loss: 0.06159275025129318
step: 60, loss: 0.06133721396327019
step: 70, loss: 0.0430615209043026
step: 80, loss: 0.016846876591444016
step: 90, loss: 0.03872550651431084
step: 100, loss: 0.03798551857471466
step: 110, loss: 0.06684190779924393
step: 120, loss: 0.013212439604103565
step: 130, loss: 0.01742047816514969
step: 140, loss: 0.05435449630022049
step: 150, loss: 0.03832676634192467
step: 160, loss: 0.05142904445528984
step: 170, loss: 0.05854453518986702
step: 180, loss: 0.03991275280714035
step: 190, loss: 0.04303797706961632
step: 200, loss: 0.04747375473380089
step: 210, loss: 0.08564969897270203
step: 220, loss: 0.028020426630973816
step: 230, loss: 0.04371984302997589
step: 240, loss: 0.0778660997748375
step: 250, loss: 0.0044004484079778194
step: 260, loss: 8.262651135737542e-06
step: 270, loss: 0.09527093172073364
step: 280, loss: 0.0015754769556224346
step: 290, loss: 0.00886683352291584
step: 300, loss: 0.0011496308725327253
step: 310, loss: 0.0857640877366066
step: 320, loss: 0.000862899178173393
step: 330, loss: 0.07815021276473999
step: 340, loss: 0.06385156512260437
step: 350, loss: 7.228364120237529e-05
step: 360, loss: 0.024619879201054573
step: 370, loss: 0.04487829655408859
step: 380, loss: 3.987253148807213e-05
step: 390, loss: 0.10926219820976257
step: 400, loss: 0.012901956215500832
step: 410, loss: 0.03472115471959114
step: 420, loss: 0.03711028769612312
step: 430, loss: 0.020605113357305527
step: 440, loss: 0.17870719730854034
step: 450, loss: 0.01539189089089632
step: 460, loss: 0.0290023535490036
step: 470, loss: 0.02695729024708271
step: 480, loss: 0.03387989103794098
step: 490, loss: 0.022575194016098976
step: 500, loss: 0.05867660790681839
step: 510, loss: 0.01185076404362917
step: 520, loss: 0.04341915249824524
step: 530, loss: 0.01277363020926714
step: 540, loss: 2.272988058393821e-05
step: 550, loss: 0.0887485072016716
step: 560, loss: 0.040346622467041016
step: 570, loss: 0.00948434229940176
step: 580, loss: 0.05318547412753105
step: 590, loss: 0.06483906507492065
step: 600, loss: 0.011637724936008453
step: 610, loss: 0.05109716206789017
step: 620, loss: 0.035072844475507736
step: 630, loss: 0.05947315692901611
step: 640, loss: 0.08650585263967514
step: 650, loss: 0.0007680392009206116
step: 660, loss: 0.1355305314064026
step: 670, loss: 0.0538160614669323
step: 680, loss: 0.04927002266049385
step: 690, loss: 0.12035319209098816
step: 700, loss: 0.04899606108665466
step: 710, loss: 0.06275521963834763
step: 720, loss: 0.06180110573768616
step: 730, loss: 0.04558851197361946
step: 740, loss: 0.0007717914413660765
step: 750, loss: 0.037380073219537735
step: 760, loss: 5.661551404045895e-05
step: 770, loss: 0.03331812471151352
step: 780, loss: 0.02317255176603794
step: 790, loss: 0.04305218905210495
step: 800, loss: 0.025596357882022858
step: 810, loss: 0.02108634263277054
step: 820, loss: 0.06467723846435547
step: 830, loss: 0.07279150187969208
step: 840, loss: 0.06233997642993927
step: 850, loss: 0.06320850551128387
step: 860, loss: 0.016946513205766678
step: 870, loss: 0.09040182083845139
step: 880, loss: 0.03943061828613281
step: 890, loss: 0.04561854153871536
step: 900, loss: 0.03260364383459091
step: 910, loss: 0.022778576239943504
step: 920, loss: 0.03457251936197281
step: 930, loss: 0.025270789861679077
step: 940, loss: 0.01718677207827568
step: 950, loss: 0.07245049625635147
step: 960, loss: 0.047573693096637726
step: 970, loss: 0.11657044291496277
epoch 19: dev_f1=0.9255966307908282, f1=0.9253034547152195, best_f1=0.9308118081180812
step: 0, loss: 0.010530107654631138
step: 10, loss: 0.054294392466545105
step: 20, loss: 0.03487560525536537
step: 30, loss: 0.01313010323792696
step: 40, loss: 0.06757757067680359
step: 50, loss: 0.016449078917503357
step: 60, loss: 0.03147216513752937
step: 70, loss: 0.11648152768611908
step: 80, loss: 0.014277849346399307
step: 90, loss: 0.07898396998643875
step: 100, loss: 0.01615789532661438
step: 110, loss: 6.74221882945858e-05
step: 120, loss: 0.04627273604273796
step: 130, loss: 0.023069756105542183
step: 140, loss: 5.5828160839155316e-05
step: 150, loss: 0.01933191530406475
step: 160, loss: 0.033678531646728516
step: 170, loss: 0.0003891363739967346
step: 180, loss: 0.01094911340624094
step: 190, loss: 0.036558881402015686
step: 200, loss: 0.05048733949661255
step: 210, loss: 0.00020830232824664563
step: 220, loss: 0.03502430021762848
step: 230, loss: 0.03013869933784008
step: 240, loss: 0.030556872487068176
step: 250, loss: 0.037838079035282135
step: 260, loss: 0.04824655130505562
step: 270, loss: 0.009051136672496796
step: 280, loss: 0.016398433595895767
step: 290, loss: 0.057097937911748886
step: 300, loss: 0.0218959953635931
step: 310, loss: 0.08404122292995453
step: 320, loss: 0.06809963285923004
step: 330, loss: 7.6517098932527e-06
step: 340, loss: 0.05514649301767349
step: 350, loss: 0.009669234044849873
step: 360, loss: 0.014385799877345562
step: 370, loss: 0.09276966005563736
step: 380, loss: 0.07404939830303192
step: 390, loss: 0.06826270371675491
step: 400, loss: 0.07806266844272614
step: 410, loss: 0.08607261627912521
step: 420, loss: 0.036061838269233704
step: 430, loss: 3.0015426091267727e-05
step: 440, loss: 0.02502831257879734
step: 450, loss: 0.059972696006298065
step: 460, loss: 0.07777658104896545
step: 470, loss: 0.10482102632522583
step: 480, loss: 0.013549424707889557
step: 490, loss: 0.01826374977827072
step: 500, loss: 0.050528839230537415
step: 510, loss: 0.001912727253511548
step: 520, loss: 0.07763083279132843
step: 530, loss: 0.05274953693151474
step: 540, loss: 0.03530942648649216
step: 550, loss: 0.04496700316667557
step: 560, loss: 0.05483723059296608
step: 570, loss: 0.0498778373003006
step: 580, loss: 0.017731480300426483
step: 590, loss: 0.00013863263302482665
step: 600, loss: 0.040539149194955826
step: 610, loss: 0.02442992478609085
step: 620, loss: 0.06263333559036255
step: 630, loss: 0.0011147982440888882
step: 640, loss: 0.029422292485833168
step: 650, loss: 0.010144312866032124
step: 660, loss: 0.03288659080862999
step: 670, loss: 0.04996611177921295
step: 680, loss: 0.01589583232998848
step: 690, loss: 0.1085677519440651
step: 700, loss: 0.03706447035074234
step: 710, loss: 0.08920122683048248
step: 720, loss: 0.00014020931848790497
step: 730, loss: 0.01973186805844307
step: 740, loss: 0.09837669134140015
step: 750, loss: 0.02800862118601799
step: 760, loss: 0.02690551057457924
step: 770, loss: 0.1168559342622757
step: 780, loss: 0.025107331573963165
step: 790, loss: 0.02732987329363823
step: 800, loss: 0.0004289727075956762
step: 810, loss: 0.0749138817191124
step: 820, loss: 0.043347153812646866
step: 830, loss: 0.048460397869348526
step: 840, loss: 4.2235711589455605e-05
step: 850, loss: 0.05393841117620468
step: 860, loss: 0.0010252806823700666
step: 870, loss: 0.03007671982049942
step: 880, loss: 0.029501400887966156
step: 890, loss: 0.0026404743548482656
step: 900, loss: 0.0634717047214508
step: 910, loss: 0.08213837444782257
step: 920, loss: 0.06484004110097885
step: 930, loss: 0.0519668310880661
step: 940, loss: 0.000712320557795465
step: 950, loss: 0.06919732689857483
step: 960, loss: 6.321560067590326e-05
step: 970, loss: 0.02782473713159561
epoch 20: dev_f1=0.9250936329588015, f1=0.9261682242990654, best_f1=0.9308118081180812
