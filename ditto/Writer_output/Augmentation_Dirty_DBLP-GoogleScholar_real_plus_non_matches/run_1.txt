cuda
Device: cuda
step: 0, loss: 0.6424803137779236
step: 10, loss: 0.43479347229003906
step: 20, loss: 0.7232522964477539
step: 30, loss: 0.3635755479335785
step: 40, loss: 0.34904026985168457
step: 50, loss: 0.2132515013217926
step: 60, loss: 0.2871412932872772
step: 70, loss: 0.3829760253429413
step: 80, loss: 0.169793039560318
step: 90, loss: 0.1532479226589203
step: 100, loss: 0.14227208495140076
step: 110, loss: 0.11883225291967392
step: 120, loss: 0.14658746123313904
step: 130, loss: 0.18655024468898773
step: 140, loss: 0.22614000737667084
step: 150, loss: 0.13215145468711853
step: 160, loss: 0.12487844377756119
step: 170, loss: 0.04867493361234665
step: 180, loss: 0.12224338948726654
step: 190, loss: 0.1850263774394989
step: 200, loss: 0.24685154855251312
step: 210, loss: 0.13707628846168518
step: 220, loss: 0.27777737379074097
step: 230, loss: 0.050968993455171585
step: 240, loss: 0.1190418154001236
step: 250, loss: 0.05888079106807709
step: 260, loss: 0.152456134557724
step: 270, loss: 0.16647844016551971
step: 280, loss: 0.07937760651111603
step: 290, loss: 0.123401939868927
step: 300, loss: 0.12194666266441345
step: 310, loss: 0.15935546159744263
step: 320, loss: 0.3135221004486084
step: 330, loss: 0.38101938366889954
step: 340, loss: 0.08507134020328522
step: 350, loss: 0.16367961466312408
step: 360, loss: 0.1547587811946869
step: 370, loss: 0.10945823788642883
step: 380, loss: 0.10125845670700073
step: 390, loss: 0.08290629833936691
step: 400, loss: 0.08933582901954651
step: 410, loss: 0.1061435416340828
step: 420, loss: 0.16646051406860352
step: 430, loss: 0.25243324041366577
step: 440, loss: 0.11982426792383194
step: 450, loss: 0.20855280756950378
step: 460, loss: 0.11315152049064636
step: 470, loss: 0.10995859652757645
step: 480, loss: 0.1589926928281784
step: 490, loss: 0.2054092139005661
step: 500, loss: 0.13060279190540314
step: 510, loss: 0.17540974915027618
step: 520, loss: 0.08945321291685104
step: 530, loss: 0.2797149121761322
step: 540, loss: 0.17699259519577026
step: 550, loss: 0.08671897649765015
step: 560, loss: 0.1912071704864502
step: 570, loss: 0.1280592978000641
step: 580, loss: 0.17714433372020721
step: 590, loss: 0.15000197291374207
step: 600, loss: 0.2172885537147522
step: 610, loss: 0.20733889937400818
step: 620, loss: 0.07575322687625885
step: 630, loss: 0.1471264362335205
step: 640, loss: 0.061835501343011856
step: 650, loss: 0.17443658411502838
step: 660, loss: 0.31704914569854736
step: 670, loss: 0.09694037586450577
step: 680, loss: 0.120240218937397
step: 690, loss: 0.17727525532245636
step: 700, loss: 0.07172217220067978
step: 710, loss: 0.09054843336343765
step: 720, loss: 0.11850962787866592
step: 730, loss: 0.16110868752002716
step: 740, loss: 0.1408630609512329
step: 750, loss: 0.12349759787321091
step: 760, loss: 0.1424366980791092
step: 770, loss: 0.18141067028045654
step: 780, loss: 0.0662272498011589
step: 790, loss: 0.11310070008039474
step: 800, loss: 0.16832764446735382
step: 810, loss: 0.08377586305141449
step: 820, loss: 0.147720068693161
step: 830, loss: 0.19386805593967438
step: 840, loss: 0.1033686101436615
step: 850, loss: 0.12407157570123672
step: 860, loss: 0.14865146577358246
step: 870, loss: 0.1663319617509842
step: 880, loss: 0.12349634617567062
step: 890, loss: 0.12407232820987701
step: 900, loss: 0.18022121489048004
step: 910, loss: 0.1345958560705185
step: 920, loss: 0.04219125583767891
step: 930, loss: 0.17716287076473236
step: 940, loss: 0.1713138073682785
step: 950, loss: 0.17184700071811676
step: 960, loss: 0.10851972550153732
step: 970, loss: 0.14623214304447174
epoch 1: dev_f1=0.9128630705394191, f1=0.9081772498857926, best_f1=0.9081772498857926
step: 0, loss: 0.10528812557458878
step: 10, loss: 0.037800952792167664
step: 20, loss: 0.11494240909814835
step: 30, loss: 0.06605904549360275
step: 40, loss: 0.1277032047510147
step: 50, loss: 0.06312217563390732
step: 60, loss: 0.1577058881521225
step: 70, loss: 0.10319453477859497
step: 80, loss: 0.21703656017780304
step: 90, loss: 0.08245445042848587
step: 100, loss: 0.08048511296510696
step: 110, loss: 0.08531716465950012
step: 120, loss: 0.07809174805879593
step: 130, loss: 0.09405098110437393
step: 140, loss: 0.18763567507266998
step: 150, loss: 0.07307494431734085
step: 160, loss: 0.12437920272350311
step: 170, loss: 0.09146223217248917
step: 180, loss: 0.2376457303762436
step: 190, loss: 0.21911635994911194
step: 200, loss: 0.10249241441488266
step: 210, loss: 0.2605365216732025
step: 220, loss: 0.1601884812116623
step: 230, loss: 0.10488011687994003
step: 240, loss: 0.17989560961723328
step: 250, loss: 0.1781255602836609
step: 260, loss: 0.1278136521577835
step: 270, loss: 0.092159204185009
step: 280, loss: 0.20668089389801025
step: 290, loss: 0.18284179270267487
step: 300, loss: 0.21525803208351135
step: 310, loss: 0.20258744060993195
step: 320, loss: 0.07298727333545685
step: 330, loss: 0.024810465052723885
step: 340, loss: 0.13619598746299744
step: 350, loss: 0.11381524801254272
step: 360, loss: 0.21449466049671173
step: 370, loss: 0.09497407078742981
step: 380, loss: 0.06080104410648346
step: 390, loss: 0.17069649696350098
step: 400, loss: 0.08874630928039551
step: 410, loss: 0.08076679706573486
step: 420, loss: 0.15473011136054993
step: 430, loss: 0.11978107690811157
step: 440, loss: 0.06489549577236176
step: 450, loss: 0.08776471018791199
step: 460, loss: 0.12650549411773682
step: 470, loss: 0.1524031013250351
step: 480, loss: 0.10486415028572083
step: 490, loss: 0.13127486407756805
step: 500, loss: 0.08548720180988312
step: 510, loss: 0.06955509632825851
step: 520, loss: 0.07658617943525314
step: 530, loss: 0.09875825047492981
step: 540, loss: 0.07081671804189682
step: 550, loss: 0.14632748067378998
step: 560, loss: 0.06748129427433014
step: 570, loss: 0.06939131766557693
step: 580, loss: 0.0940823182463646
step: 590, loss: 0.18331415951251984
step: 600, loss: 0.09832848608493805
step: 610, loss: 0.17299970984458923
step: 620, loss: 0.15846045315265656
step: 630, loss: 0.07539043575525284
step: 640, loss: 0.24526192247867584
step: 650, loss: 0.08756139874458313
step: 660, loss: 0.16609904170036316
step: 670, loss: 0.12517689168453217
step: 680, loss: 0.08882268518209457
step: 690, loss: 0.15544231235980988
step: 700, loss: 0.08983708918094635
step: 710, loss: 0.1294959932565689
step: 720, loss: 0.13650041818618774
step: 730, loss: 0.16804461181163788
step: 740, loss: 0.12346317619085312
step: 750, loss: 0.06673524528741837
step: 760, loss: 0.04823603108525276
step: 770, loss: 0.09472054988145828
step: 780, loss: 0.0204666405916214
step: 790, loss: 0.0767502412199974
step: 800, loss: 0.10595348477363586
step: 810, loss: 0.12418670952320099
step: 820, loss: 0.08372387290000916
step: 830, loss: 0.03229987993836403
step: 840, loss: 0.15998093783855438
step: 850, loss: 0.23262162506580353
step: 860, loss: 0.11124642193317413
step: 870, loss: 0.11331132799386978
step: 880, loss: 0.0680256113409996
step: 890, loss: 0.29395243525505066
step: 900, loss: 0.15651661157608032
step: 910, loss: 0.146117702126503
step: 920, loss: 0.08617295324802399
step: 930, loss: 0.10392110049724579
step: 940, loss: 0.07058444619178772
step: 950, loss: 0.12023244053125381
step: 960, loss: 0.1630079597234726
step: 970, loss: 0.08553438633680344
epoch 2: dev_f1=0.9213793103448276, f1=0.9153776160145586, best_f1=0.9153776160145586
step: 0, loss: 0.10998079925775528
step: 10, loss: 0.13277095556259155
step: 20, loss: 0.0916442945599556
step: 30, loss: 0.11463923752307892
step: 40, loss: 0.084894098341465
step: 50, loss: 0.3401252031326294
step: 60, loss: 0.10987719893455505
step: 70, loss: 0.12004254013299942
step: 80, loss: 0.24715828895568848
step: 90, loss: 0.13180823624134064
step: 100, loss: 0.11370771378278732
step: 110, loss: 0.23288984596729279
step: 120, loss: 0.13906235992908478
step: 130, loss: 0.05753355100750923
step: 140, loss: 0.10842306166887283
step: 150, loss: 0.030147109180688858
step: 160, loss: 0.059903401881456375
step: 170, loss: 0.1382409930229187
step: 180, loss: 0.06576719135046005
step: 190, loss: 0.055402837693691254
step: 200, loss: 0.3606093227863312
step: 210, loss: 0.12247655540704727
step: 220, loss: 0.05457424744963646
step: 230, loss: 0.11910628527402878
step: 240, loss: 0.2587487995624542
step: 250, loss: 0.07706368714570999
step: 260, loss: 0.06489861756563187
step: 270, loss: 0.15737652778625488
step: 280, loss: 0.09156208485364914
step: 290, loss: 0.149225115776062
step: 300, loss: 0.12944236397743225
step: 310, loss: 0.08827605098485947
step: 320, loss: 0.10392114520072937
step: 330, loss: 0.07701405137777328
step: 340, loss: 0.20077009499073029
step: 350, loss: 0.267210453748703
step: 360, loss: 0.09787102043628693
step: 370, loss: 0.11551419645547867
step: 380, loss: 0.057831622660160065
step: 390, loss: 0.19582806527614594
step: 400, loss: 0.14374563097953796
step: 410, loss: 0.11946123838424683
step: 420, loss: 0.1263180673122406
step: 430, loss: 0.0365203320980072
step: 440, loss: 0.09623803198337555
step: 450, loss: 0.1135566458106041
step: 460, loss: 0.10641231387853622
step: 470, loss: 0.03980259224772453
step: 480, loss: 0.14056769013404846
step: 490, loss: 0.16714707016944885
step: 500, loss: 0.13978788256645203
step: 510, loss: 0.09282350540161133
step: 520, loss: 0.1702255755662918
step: 530, loss: 0.16990141570568085
step: 540, loss: 0.17745400965213776
step: 550, loss: 0.12042133510112762
step: 560, loss: 0.11417441815137863
step: 570, loss: 0.022871671244502068
step: 580, loss: 0.032493334263563156
step: 590, loss: 0.057562872767448425
step: 600, loss: 0.11127682030200958
step: 610, loss: 0.09812107682228088
step: 620, loss: 0.06235964596271515
step: 630, loss: 0.11684267222881317
step: 640, loss: 0.12809337675571442
step: 650, loss: 0.10935302823781967
step: 660, loss: 0.13104167580604553
step: 670, loss: 0.13406740128993988
step: 680, loss: 0.14430037140846252
step: 690, loss: 0.06053784862160683
step: 700, loss: 0.1234389916062355
step: 710, loss: 0.21295082569122314
step: 720, loss: 0.045596249401569366
step: 730, loss: 0.08047837764024734
step: 740, loss: 0.0558570921421051
step: 750, loss: 0.15374349057674408
step: 760, loss: 0.18745408952236176
step: 770, loss: 0.10065764933824539
step: 780, loss: 0.19355787336826324
step: 790, loss: 0.17064300179481506
step: 800, loss: 0.03185531869530678
step: 810, loss: 0.03778788819909096
step: 820, loss: 0.1033102422952652
step: 830, loss: 0.10520550608634949
step: 840, loss: 0.023959042504429817
step: 850, loss: 0.21328404545783997
step: 860, loss: 0.10507781058549881
step: 870, loss: 0.11229726672172546
step: 880, loss: 0.16499513387680054
step: 890, loss: 0.1824873387813568
step: 900, loss: 0.06335937976837158
step: 910, loss: 0.0808197408914566
step: 920, loss: 0.16228926181793213
step: 930, loss: 0.096586674451828
step: 940, loss: 0.10211999714374542
step: 950, loss: 0.09221230447292328
step: 960, loss: 0.1846851408481598
step: 970, loss: 0.32060718536376953
epoch 3: dev_f1=0.9193763919821827, f1=0.9205357142857143, best_f1=0.9153776160145586
step: 0, loss: 0.12957637012004852
step: 10, loss: 0.11819005757570267
step: 20, loss: 0.1138206198811531
step: 30, loss: 0.20902228355407715
step: 40, loss: 0.11091234534978867
step: 50, loss: 0.14725033938884735
step: 60, loss: 0.0763387680053711
step: 70, loss: 0.08825945109128952
step: 80, loss: 0.06435103714466095
step: 90, loss: 0.04670320078730583
step: 100, loss: 0.14896264672279358
step: 110, loss: 0.13214367628097534
step: 120, loss: 0.04694342240691185
step: 130, loss: 0.0415002815425396
step: 140, loss: 0.1724088042974472
step: 150, loss: 0.20714417099952698
step: 160, loss: 0.040593795478343964
step: 170, loss: 0.04462166130542755
step: 180, loss: 0.1921815276145935
step: 190, loss: 0.06864877790212631
step: 200, loss: 0.2104097157716751
step: 210, loss: 0.11770215630531311
step: 220, loss: 0.07885147631168365
step: 230, loss: 0.18776722252368927
step: 240, loss: 0.0628233477473259
step: 250, loss: 0.07475119084119797
step: 260, loss: 0.13153696060180664
step: 270, loss: 0.043053776025772095
step: 280, loss: 0.10598535090684891
step: 290, loss: 0.1504075676202774
step: 300, loss: 0.12835568189620972
step: 310, loss: 0.08111248165369034
step: 320, loss: 0.027059726417064667
step: 330, loss: 0.11867428570985794
step: 340, loss: 0.1285795122385025
step: 350, loss: 0.10903007537126541
step: 360, loss: 0.197331964969635
step: 370, loss: 0.11153530329465866
step: 380, loss: 0.021885626018047333
step: 390, loss: 0.10112062096595764
step: 400, loss: 0.08661770075559616
step: 410, loss: 0.10529740899801254
step: 420, loss: 0.08177022635936737
step: 430, loss: 0.13717687129974365
step: 440, loss: 0.09773100912570953
step: 450, loss: 0.11186973750591278
step: 460, loss: 0.029617520049214363
step: 470, loss: 0.15814127027988434
step: 480, loss: 0.15834970772266388
step: 490, loss: 0.2601519227027893
step: 500, loss: 0.11496131122112274
step: 510, loss: 0.09574738889932632
step: 520, loss: 0.03268832340836525
step: 530, loss: 0.15000958740711212
step: 540, loss: 0.034021757543087006
step: 550, loss: 0.060297273099422455
step: 560, loss: 0.11824355274438858
step: 570, loss: 0.18585945665836334
step: 580, loss: 0.08348169177770615
step: 590, loss: 0.036165349185466766
step: 600, loss: 0.15724754333496094
step: 610, loss: 0.1326926201581955
step: 620, loss: 0.14528939127922058
step: 630, loss: 0.18249624967575073
step: 640, loss: 0.026614949107170105
step: 650, loss: 0.06282811611890793
step: 660, loss: 0.12810039520263672
step: 670, loss: 0.1454567164182663
step: 680, loss: 0.11364361643791199
step: 690, loss: 0.12961237132549286
step: 700, loss: 0.07504138350486755
step: 710, loss: 0.10697578638792038
step: 720, loss: 0.10722291469573975
step: 730, loss: 0.23012010753154755
step: 740, loss: 0.07114732265472412
step: 750, loss: 0.10286703705787659
step: 760, loss: 0.07305988669395447
step: 770, loss: 0.14792414009571075
step: 780, loss: 0.13846327364444733
step: 790, loss: 0.11472522467374802
step: 800, loss: 0.1320621222257614
step: 810, loss: 0.0779733657836914
step: 820, loss: 0.13528507947921753
step: 830, loss: 0.10769284516572952
step: 840, loss: 0.07542743533849716
step: 850, loss: 0.25124672055244446
step: 860, loss: 0.10478372126817703
step: 870, loss: 0.0772453099489212
step: 880, loss: 0.32156771421432495
step: 890, loss: 0.007019514683634043
step: 900, loss: 0.1310657411813736
step: 910, loss: 0.07103550434112549
step: 920, loss: 0.09627245366573334
step: 930, loss: 0.059860967099666595
step: 940, loss: 0.08488184958696365
step: 950, loss: 0.06798572838306427
step: 960, loss: 0.16931866109371185
step: 970, loss: 0.09544482082128525
epoch 4: dev_f1=0.9196879302432308, f1=0.9151459854014599, best_f1=0.9153776160145586
step: 0, loss: 0.0729924812912941
step: 10, loss: 0.05563557520508766
step: 20, loss: 0.09936888515949249
step: 30, loss: 0.14038774371147156
step: 40, loss: 0.11441206932067871
step: 50, loss: 0.05595414340496063
step: 60, loss: 0.20235347747802734
step: 70, loss: 0.06303630769252777
step: 80, loss: 0.026776984333992004
step: 90, loss: 0.10870246589183807
step: 100, loss: 0.14685313403606415
step: 110, loss: 0.07218732684850693
step: 120, loss: 0.06594086438417435
step: 130, loss: 0.05335076153278351
step: 140, loss: 0.1216902807354927
step: 150, loss: 0.19351033866405487
step: 160, loss: 0.13936422765254974
step: 170, loss: 0.1429179310798645
step: 180, loss: 0.20705409348011017
step: 190, loss: 0.17315804958343506
step: 200, loss: 0.10806882381439209
step: 210, loss: 0.1097366139292717
step: 220, loss: 0.11465542018413544
step: 230, loss: 0.08655887097120285
step: 240, loss: 0.09832020848989487
step: 250, loss: 0.11465450376272202
step: 260, loss: 0.10849256068468094
step: 270, loss: 0.14832960069179535
step: 280, loss: 0.07119619846343994
step: 290, loss: 0.11521188169717789
step: 300, loss: 0.15654148161411285
step: 310, loss: 0.1665807068347931
step: 320, loss: 0.08059801161289215
step: 330, loss: 0.08906346559524536
step: 340, loss: 0.11350656300783157
step: 350, loss: 0.08152641355991364
step: 360, loss: 0.09576163440942764
step: 370, loss: 0.11239653825759888
step: 380, loss: 0.0803779810667038
step: 390, loss: 0.14512106776237488
step: 400, loss: 0.085604727268219
step: 410, loss: 0.012969128787517548
step: 420, loss: 0.03249604254961014
step: 430, loss: 0.06607859581708908
step: 440, loss: 0.1044783741235733
step: 450, loss: 0.10120002180337906
step: 460, loss: 0.0954589992761612
step: 470, loss: 0.13640564680099487
step: 480, loss: 0.14858075976371765
step: 490, loss: 0.057370323687791824
step: 500, loss: 0.05588621273636818
step: 510, loss: 0.1193016842007637
step: 520, loss: 0.12786678969860077
step: 530, loss: 0.0669529065489769
step: 540, loss: 0.14512218534946442
step: 550, loss: 0.08502388000488281
step: 560, loss: 0.1443435102701187
step: 570, loss: 0.04073026031255722
step: 580, loss: 0.11415962129831314
step: 590, loss: 0.07486454397439957
step: 600, loss: 0.274517685174942
step: 610, loss: 0.16886037588119507
step: 620, loss: 0.012208402156829834
step: 630, loss: 0.21430113911628723
step: 640, loss: 0.04396321251988411
step: 650, loss: 0.12322530150413513
step: 660, loss: 0.10642240941524506
step: 670, loss: 0.08071176707744598
step: 680, loss: 0.013418498449027538
step: 690, loss: 0.06496535986661911
step: 700, loss: 0.12572716176509857
step: 710, loss: 0.14209452271461487
step: 720, loss: 0.11184655129909515
step: 730, loss: 0.12272854894399643
step: 740, loss: 0.21465814113616943
step: 750, loss: 0.10952487587928772
step: 760, loss: 0.036984384059906006
step: 770, loss: 0.008842550218105316
step: 780, loss: 0.1001831516623497
step: 790, loss: 0.10234066843986511
step: 800, loss: 0.09554001688957214
step: 810, loss: 0.235849067568779
step: 820, loss: 0.10674910247325897
step: 830, loss: 0.12949858605861664
step: 840, loss: 0.05776374414563179
step: 850, loss: 0.1362999975681305
step: 860, loss: 0.029078735038638115
step: 870, loss: 0.12152554094791412
step: 880, loss: 0.08149075508117676
step: 890, loss: 0.07749614119529724
step: 900, loss: 0.1584712564945221
step: 910, loss: 0.07606317102909088
step: 920, loss: 0.017665641382336617
step: 930, loss: 0.013535049743950367
step: 940, loss: 0.035418957471847534
step: 950, loss: 0.0512578971683979
step: 960, loss: 0.11728043854236603
step: 970, loss: 0.09690254926681519
epoch 5: dev_f1=0.9339491916859123, f1=0.9227230910763569, best_f1=0.9227230910763569
step: 0, loss: 0.12567581236362457
step: 10, loss: 0.079536072909832
step: 20, loss: 0.057017236948013306
step: 30, loss: 0.06926242262125015
step: 40, loss: 0.04298703745007515
step: 50, loss: 0.15450794994831085
step: 60, loss: 0.0877188891172409
step: 70, loss: 0.11895765364170074
step: 80, loss: 0.1481952667236328
step: 90, loss: 0.05896288529038429
step: 100, loss: 0.09085053950548172
step: 110, loss: 0.025340382009744644
step: 120, loss: 0.11300121247768402
step: 130, loss: 0.15557841956615448
step: 140, loss: 0.05701563507318497
step: 150, loss: 0.03900118172168732
step: 160, loss: 0.09279350936412811
step: 170, loss: 0.09120596945285797
step: 180, loss: 0.06089949235320091
step: 190, loss: 0.12088079750537872
step: 200, loss: 0.12205511331558228
step: 210, loss: 0.009054054506123066
step: 220, loss: 0.014192173257470131
step: 230, loss: 0.02006753906607628
step: 240, loss: 0.11398901045322418
step: 250, loss: 0.07360631972551346
step: 260, loss: 0.10090719163417816
step: 270, loss: 0.04943963512778282
step: 280, loss: 0.06892193108797073
step: 290, loss: 0.15468427538871765
step: 300, loss: 0.023181980475783348
step: 310, loss: 0.055860381573438644
step: 320, loss: 0.17687493562698364
step: 330, loss: 0.02950488030910492
step: 340, loss: 0.0953763797879219
step: 350, loss: 0.05399496853351593
step: 360, loss: 0.16107778251171112
step: 370, loss: 0.1953931748867035
step: 380, loss: 0.10196025669574738
step: 390, loss: 0.06462042033672333
step: 400, loss: 0.10756759345531464
step: 410, loss: 0.09587360918521881
step: 420, loss: 0.03190305829048157
step: 430, loss: 0.04579155147075653
step: 440, loss: 0.07067238539457321
step: 450, loss: 0.0493244007229805
step: 460, loss: 0.1076894998550415
step: 470, loss: 0.03600994497537613
step: 480, loss: 0.060414694249629974
step: 490, loss: 0.04060554876923561
step: 500, loss: 0.061041392385959625
step: 510, loss: 0.16584806144237518
step: 520, loss: 0.03215622529387474
step: 530, loss: 0.0840381383895874
step: 540, loss: 0.10428859293460846
step: 550, loss: 0.1587381660938263
step: 560, loss: 0.14720161259174347
step: 570, loss: 0.07054144889116287
step: 580, loss: 0.035058457404375076
step: 590, loss: 0.03628750517964363
step: 600, loss: 0.12869620323181152
step: 610, loss: 0.11182999610900879
step: 620, loss: 0.08564258366823196
step: 630, loss: 0.025793947279453278
step: 640, loss: 0.06578400731086731
step: 650, loss: 0.10937730222940445
step: 660, loss: 0.12433367967605591
step: 670, loss: 0.06675291061401367
step: 680, loss: 0.11075873672962189
step: 690, loss: 0.1713096648454666
step: 700, loss: 0.1981869339942932
step: 710, loss: 0.07631629705429077
step: 720, loss: 0.011188782751560211
step: 730, loss: 0.0694270208477974
step: 740, loss: 0.14800520241260529
step: 750, loss: 0.044502221047878265
step: 760, loss: 0.11854241788387299
step: 770, loss: 0.1700340360403061
step: 780, loss: 0.11345187574625015
step: 790, loss: 0.16851532459259033
step: 800, loss: 0.07707184553146362
step: 810, loss: 0.09094106405973434
step: 820, loss: 0.06051984801888466
step: 830, loss: 0.07822830229997635
step: 840, loss: 0.095852792263031
step: 850, loss: 0.11332859098911285
step: 860, loss: 0.05697950720787048
step: 870, loss: 0.092649444937706
step: 880, loss: 0.04788050055503845
step: 890, loss: 0.13868875801563263
step: 900, loss: 0.25588542222976685
step: 910, loss: 0.10062147676944733
step: 920, loss: 0.04568140208721161
step: 930, loss: 0.040698591619729996
step: 940, loss: 0.053784146904945374
step: 950, loss: 0.2320556789636612
step: 960, loss: 0.053779277950525284
step: 970, loss: 0.049418870359659195
epoch 6: dev_f1=0.9309701492537312, f1=0.9212121212121211, best_f1=0.9227230910763569
step: 0, loss: 0.09624706953763962
step: 10, loss: 0.06379956752061844
step: 20, loss: 0.06652168929576874
step: 30, loss: 0.07681383937597275
step: 40, loss: 0.06169198453426361
step: 50, loss: 0.22132955491542816
step: 60, loss: 0.14886601269245148
step: 70, loss: 0.028079479932785034
step: 80, loss: 0.08397810161113739
step: 90, loss: 0.06878767907619476
step: 100, loss: 0.07566946744918823
step: 110, loss: 0.0723455622792244
step: 120, loss: 0.06018703430891037
step: 130, loss: 0.15432538092136383
step: 140, loss: 0.10661257058382034
step: 150, loss: 0.06068289652466774
step: 160, loss: 0.06957722455263138
step: 170, loss: 0.03445037826895714
step: 180, loss: 0.17595693469047546
step: 190, loss: 0.05068378895521164
step: 200, loss: 0.027398722246289253
step: 210, loss: 0.0842212662100792
step: 220, loss: 0.024542856961488724
step: 230, loss: 0.0800967812538147
step: 240, loss: 0.1309298723936081
step: 250, loss: 0.06883871555328369
step: 260, loss: 0.03852468729019165
step: 270, loss: 0.042517807334661484
step: 280, loss: 0.12544767558574677
step: 290, loss: 0.1140122041106224
step: 300, loss: 0.10589727014303207
step: 310, loss: 0.0624707005918026
step: 320, loss: 0.14435577392578125
step: 330, loss: 0.06964588165283203
step: 340, loss: 0.05772203579545021
step: 350, loss: 0.06780923157930374
step: 360, loss: 0.03377457708120346
step: 370, loss: 0.0648493841290474
step: 380, loss: 0.0779155045747757
step: 390, loss: 0.10888560861349106
step: 400, loss: 0.21760523319244385
step: 410, loss: 0.043080490082502365
step: 420, loss: 0.06014251708984375
step: 430, loss: 0.05237580090761185
step: 440, loss: 0.047999750822782516
step: 450, loss: 0.06973598897457123
step: 460, loss: 0.09155572950839996
step: 470, loss: 0.03145400434732437
step: 480, loss: 0.009310323745012283
step: 490, loss: 0.03839004039764404
step: 500, loss: 0.21275657415390015
step: 510, loss: 0.0002278746833326295
step: 520, loss: 0.12896695733070374
step: 530, loss: 0.17597855627536774
step: 540, loss: 0.14113929867744446
step: 550, loss: 0.024297794327139854
step: 560, loss: 0.17044423520565033
step: 570, loss: 0.12643025815486908
step: 580, loss: 0.12269493192434311
step: 590, loss: 0.09403679519891739
step: 600, loss: 0.23633992671966553
step: 610, loss: 0.12802466750144958
step: 620, loss: 0.02030758559703827
step: 630, loss: 0.15892335772514343
step: 640, loss: 0.06840337067842484
step: 650, loss: 0.06805871427059174
step: 660, loss: 0.05046292021870613
step: 670, loss: 0.05855334550142288
step: 680, loss: 0.193391814827919
step: 690, loss: 0.0297431331127882
step: 700, loss: 0.1258239895105362
step: 710, loss: 0.09978083521127701
step: 720, loss: 0.027018386870622635
step: 730, loss: 0.049131061881780624
step: 740, loss: 0.09061091393232346
step: 750, loss: 0.032556287944316864
step: 760, loss: 0.05421226844191551
step: 770, loss: 0.035709839314222336
step: 780, loss: 0.10442402958869934
step: 790, loss: 0.04913311079144478
step: 800, loss: 0.08198173344135284
step: 810, loss: 0.10115154832601547
step: 820, loss: 0.0791989266872406
step: 830, loss: 0.007091604173183441
step: 840, loss: 0.11705053597688675
step: 850, loss: 0.1387663334608078
step: 860, loss: 0.16230590641498566
step: 870, loss: 0.13047564029693604
step: 880, loss: 0.05549071729183197
step: 890, loss: 0.05488422140479088
step: 900, loss: 0.037262655794620514
step: 910, loss: 0.09426364302635193
step: 920, loss: 0.1446070373058319
step: 930, loss: 0.10244638472795486
step: 940, loss: 0.10951026529073715
step: 950, loss: 0.15210935473442078
step: 960, loss: 0.06752508878707886
step: 970, loss: 0.1287412941455841
epoch 7: dev_f1=0.9294336118848654, f1=0.9190453907346748, best_f1=0.9227230910763569
step: 0, loss: 0.10846730321645737
step: 10, loss: 0.09955035895109177
step: 20, loss: 0.05414476990699768
step: 30, loss: 0.07419578731060028
step: 40, loss: 0.13322030007839203
step: 50, loss: 0.022526223212480545
step: 60, loss: 0.033970627933740616
step: 70, loss: 0.1781664788722992
step: 80, loss: 0.0653722807765007
step: 90, loss: 0.08270759135484695
step: 100, loss: 0.1307382881641388
step: 110, loss: 0.06815102696418762
step: 120, loss: 0.07240866124629974
step: 130, loss: 0.015377933159470558
step: 140, loss: 0.23280298709869385
step: 150, loss: 0.11559299379587173
step: 160, loss: 0.11799478530883789
step: 170, loss: 0.09426699578762054
step: 180, loss: 0.05937327817082405
step: 190, loss: 0.1974477916955948
step: 200, loss: 0.08215975016355515
step: 210, loss: 0.03733455389738083
step: 220, loss: 0.03975183516740799
step: 230, loss: 0.0855509340763092
step: 240, loss: 0.11149659007787704
step: 250, loss: 0.11365247517824173
step: 260, loss: 0.2005346715450287
step: 270, loss: 0.18504604697227478
step: 280, loss: 0.06376954913139343
step: 290, loss: 0.0697777271270752
step: 300, loss: 0.08710520714521408
step: 310, loss: 0.07526344805955887
step: 320, loss: 0.07964863628149033
step: 330, loss: 0.11523166298866272
step: 340, loss: 0.0653737261891365
step: 350, loss: 0.14528760313987732
step: 360, loss: 0.09404463320970535
step: 370, loss: 0.07410312443971634
step: 380, loss: 0.08081410080194473
step: 390, loss: 0.15473505854606628
step: 400, loss: 0.05134601518511772
step: 410, loss: 0.08592946082353592
step: 420, loss: 0.11840377002954483
step: 430, loss: 0.04329938441514969
step: 440, loss: 0.0166490375995636
step: 450, loss: 0.09632173180580139
step: 460, loss: 0.10328605771064758
step: 470, loss: 0.09745069593191147
step: 480, loss: 0.11953816562891006
step: 490, loss: 0.06230562552809715
step: 500, loss: 0.08737343549728394
step: 510, loss: 0.050729237496852875
step: 520, loss: 0.10421191900968552
step: 530, loss: 0.01556331105530262
step: 540, loss: 0.10366244614124298
step: 550, loss: 0.09836593270301819
step: 560, loss: 0.16334272921085358
step: 570, loss: 0.044092096388339996
step: 580, loss: 0.0007482774672098458
step: 590, loss: 0.037247639149427414
step: 600, loss: 0.08169541507959366
step: 610, loss: 0.10008145868778229
step: 620, loss: 0.14525260031223297
step: 630, loss: 0.06594061851501465
step: 640, loss: 0.10533444583415985
step: 650, loss: 0.0880163386464119
step: 660, loss: 0.03166208043694496
step: 670, loss: 0.008245839737355709
step: 680, loss: 0.09040801227092743
step: 690, loss: 0.04301637038588524
step: 700, loss: 0.07941826432943344
step: 710, loss: 0.14540131390094757
step: 720, loss: 0.16096697747707367
step: 730, loss: 0.06796988844871521
step: 740, loss: 0.13594312965869904
step: 750, loss: 0.09584332257509232
step: 760, loss: 0.07937797158956528
step: 770, loss: 0.10806498676538467
step: 780, loss: 0.08557857573032379
step: 790, loss: 0.08087632805109024
step: 800, loss: 0.16018475592136383
step: 810, loss: 0.23003573715686798
step: 820, loss: 0.25301745533943176
step: 830, loss: 0.08267684280872345
step: 840, loss: 0.035453006625175476
step: 850, loss: 0.07908449321985245
step: 860, loss: 0.04360096529126167
step: 870, loss: 0.07070168852806091
step: 880, loss: 0.16378182172775269
step: 890, loss: 0.04649421200156212
step: 900, loss: 0.057768017053604126
step: 910, loss: 0.1314578354358673
step: 920, loss: 0.09942621737718582
step: 930, loss: 0.19885161519050598
step: 940, loss: 0.040257737040519714
step: 950, loss: 0.08139084279537201
step: 960, loss: 0.01926850713789463
step: 970, loss: 0.010784640908241272
epoch 8: dev_f1=0.9301895515487749, f1=0.9234317343173433, best_f1=0.9227230910763569
step: 0, loss: 0.08447631448507309
step: 10, loss: 0.1856478452682495
step: 20, loss: 0.024930540472269058
step: 30, loss: 0.03631896525621414
step: 40, loss: 0.06576910614967346
step: 50, loss: 0.18629126250743866
step: 60, loss: 0.07897371798753738
step: 70, loss: 0.02906164899468422
step: 80, loss: 0.01884964294731617
step: 90, loss: 0.0370502732694149
step: 100, loss: 0.033069904893636703
step: 110, loss: 0.0380629301071167
step: 120, loss: 0.06785815209150314
step: 130, loss: 0.052608832716941833
step: 140, loss: 0.11613988876342773
step: 150, loss: 0.14642202854156494
step: 160, loss: 0.0786137655377388
step: 170, loss: 0.08872561156749725
step: 180, loss: 0.07657387852668762
step: 190, loss: 0.07040140777826309
step: 200, loss: 0.014210669323801994
step: 210, loss: 0.08634410053491592
step: 220, loss: 0.10447601974010468
step: 230, loss: 0.10691619664430618
step: 240, loss: 0.10093457996845245
step: 250, loss: 0.13697084784507751
step: 260, loss: 0.010361554101109505
step: 270, loss: 0.04512524604797363
step: 280, loss: 0.22190269827842712
step: 290, loss: 0.09292858839035034
step: 300, loss: 0.06870563328266144
step: 310, loss: 0.15846435725688934
step: 320, loss: 0.07307843118906021
step: 330, loss: 0.0754862129688263
step: 340, loss: 0.03859834372997284
step: 350, loss: 0.04886886477470398
step: 360, loss: 0.15093450248241425
step: 370, loss: 0.06516716629266739
step: 380, loss: 0.033746182918548584
step: 390, loss: 0.005077520851045847
step: 400, loss: 0.06595131009817123
step: 410, loss: 0.11820460110902786
step: 420, loss: 0.12733012437820435
step: 430, loss: 0.08184734731912613
step: 440, loss: 0.04860861971974373
step: 450, loss: 0.08849768340587616
step: 460, loss: 0.12307456135749817
step: 470, loss: 0.06363926082849503
step: 480, loss: 0.09465852379798889
step: 490, loss: 0.09021668136119843
step: 500, loss: 0.09758162498474121
step: 510, loss: 0.08396726101636887
step: 520, loss: 0.022417213767766953
step: 530, loss: 0.10508008301258087
step: 540, loss: 0.04490349441766739
step: 550, loss: 0.125546395778656
step: 560, loss: 0.0350128598511219
step: 570, loss: 0.11725302040576935
step: 580, loss: 0.02018950693309307
step: 590, loss: 0.07548018544912338
step: 600, loss: 0.01919478550553322
step: 610, loss: 0.12998980283737183
step: 620, loss: 0.0812908336520195
step: 630, loss: 0.09459798783063889
step: 640, loss: 0.15282851457595825
step: 650, loss: 0.09199065715074539
step: 660, loss: 0.0030746585689485073
step: 670, loss: 0.08956200629472733
step: 680, loss: 0.12179835885763168
step: 690, loss: 0.067075714468956
step: 700, loss: 0.19235174357891083
step: 710, loss: 0.10787912458181381
step: 720, loss: 0.04790144041180611
step: 730, loss: 0.14443263411521912
step: 740, loss: 0.1713181734085083
step: 750, loss: 0.06816750019788742
step: 760, loss: 0.00759897381067276
step: 770, loss: 0.07611468434333801
step: 780, loss: 0.04363054782152176
step: 790, loss: 0.35575294494628906
step: 800, loss: 0.125590518116951
step: 810, loss: 0.05037492886185646
step: 820, loss: 0.09333796054124832
step: 830, loss: 0.12315525114536285
step: 840, loss: 0.059628892689943314
step: 850, loss: 0.0442357212305069
step: 860, loss: 0.047435663640499115
step: 870, loss: 0.11329120397567749
step: 880, loss: 0.0674467608332634
step: 890, loss: 0.13199609518051147
step: 900, loss: 0.1215691938996315
step: 910, loss: 0.04367532953619957
step: 920, loss: 0.07889670133590698
step: 930, loss: 0.05360933393239975
step: 940, loss: 0.0970846489071846
step: 950, loss: 0.09361863136291504
step: 960, loss: 0.08328107744455338
step: 970, loss: 0.08419913053512573
epoch 9: dev_f1=0.9352914180816887, f1=0.9316827143512151, best_f1=0.9316827143512151
step: 0, loss: 0.03343889117240906
step: 10, loss: 0.039291951805353165
step: 20, loss: 0.08039014041423798
step: 30, loss: 0.09724707901477814
step: 40, loss: 0.13754482567310333
step: 50, loss: 0.14380767941474915
step: 60, loss: 0.048425935208797455
step: 70, loss: 0.0659969374537468
step: 80, loss: 0.0821179449558258
step: 90, loss: 0.07339517772197723
step: 100, loss: 0.048771291971206665
step: 110, loss: 0.13189740478992462
step: 120, loss: 0.05490279570221901
step: 130, loss: 0.07024058699607849
step: 140, loss: 0.049112170934677124
step: 150, loss: 0.041796308010816574
step: 160, loss: 0.026269273832440376
step: 170, loss: 0.021209750324487686
step: 180, loss: 0.12796324491500854
step: 190, loss: 0.08320803195238113
step: 200, loss: 0.08171582967042923
step: 210, loss: 0.007432908751070499
step: 220, loss: 0.10789996385574341
step: 230, loss: 0.06220544874668121
step: 240, loss: 0.04849107190966606
step: 250, loss: 0.1277606040239334
step: 260, loss: 0.12232920527458191
step: 270, loss: 0.054146554321050644
step: 280, loss: 0.13384650647640228
step: 290, loss: 0.02336474508047104
step: 300, loss: 0.07709164917469025
step: 310, loss: 0.024066617712378502
step: 320, loss: 0.03747532516717911
step: 330, loss: 0.0005048238090239465
step: 340, loss: 0.07706570625305176
step: 350, loss: 0.052136246114969254
step: 360, loss: 0.05574781820178032
step: 370, loss: 0.03186533972620964
step: 380, loss: 0.16905982792377472
step: 390, loss: 0.0748475193977356
step: 400, loss: 0.09528900682926178
step: 410, loss: 0.05995122343301773
step: 420, loss: 0.11396127939224243
step: 430, loss: 0.08607953786849976
step: 440, loss: 0.10583782196044922
step: 450, loss: 0.06350896507501602
step: 460, loss: 0.0012816176749765873
step: 470, loss: 0.06371428072452545
step: 480, loss: 0.07112662494182587
step: 490, loss: 0.07581454515457153
step: 500, loss: 0.005206804722547531
step: 510, loss: 0.07521826028823853
step: 520, loss: 0.03202277049422264
step: 530, loss: 0.039771392941474915
step: 540, loss: 0.036214716732501984
step: 550, loss: 0.05323485657572746
step: 560, loss: 0.08789226412773132
step: 570, loss: 0.12976941466331482
step: 580, loss: 0.05434945598244667
step: 590, loss: 0.06414569169282913
step: 600, loss: 0.1107853353023529
step: 610, loss: 0.05763242393732071
step: 620, loss: 0.03247939795255661
step: 630, loss: 0.0788162350654602
step: 640, loss: 0.08721466362476349
step: 650, loss: 0.04889887571334839
step: 660, loss: 0.1330084651708603
step: 670, loss: 0.06953532248735428
step: 680, loss: 0.009865708649158478
step: 690, loss: 0.08104242384433746
step: 700, loss: 0.08158645778894424
step: 710, loss: 0.021062226966023445
step: 720, loss: 0.05842316895723343
step: 730, loss: 0.04720967635512352
step: 740, loss: 0.04558921605348587
step: 750, loss: 0.0747193694114685
step: 760, loss: 0.054097630083560944
step: 770, loss: 0.13386698067188263
step: 780, loss: 0.072762630879879
step: 790, loss: 0.0719914585351944
step: 800, loss: 0.08451984077692032
step: 810, loss: 0.005119835492223501
step: 820, loss: 0.08216899633407593
step: 830, loss: 0.07467420399188995
step: 840, loss: 0.017901483923196793
step: 850, loss: 0.05683184415102005
step: 860, loss: 0.018931008875370026
step: 870, loss: 0.018941029906272888
step: 880, loss: 0.051904186606407166
step: 890, loss: 0.09437822550535202
step: 900, loss: 0.0138556445017457
step: 910, loss: 0.13486580550670624
step: 920, loss: 0.005038807168602943
step: 930, loss: 0.07045885920524597
step: 940, loss: 0.07038780301809311
step: 950, loss: 0.1011633649468422
step: 960, loss: 0.06062425300478935
step: 970, loss: 0.0230206660926342
epoch 10: dev_f1=0.929472209248015, f1=0.921999065857076, best_f1=0.9316827143512151
step: 0, loss: 0.05759471282362938
step: 10, loss: 0.12856946885585785
step: 20, loss: 0.027295365929603577
step: 30, loss: 0.028657373040914536
step: 40, loss: 0.08351494371891022
step: 50, loss: 0.009205923415720463
step: 60, loss: 0.005285726860165596
step: 70, loss: 0.06110154092311859
step: 80, loss: 0.11605213582515717
step: 90, loss: 0.045277342200279236
step: 100, loss: 0.046114079654216766
step: 110, loss: 0.07759609818458557
step: 120, loss: 0.053938888013362885
step: 130, loss: 0.00809766910970211
step: 140, loss: 0.1134580597281456
step: 150, loss: 0.07339468598365784
step: 160, loss: 0.13783897459506989
step: 170, loss: 0.1296590119600296
step: 180, loss: 0.038229912519454956
step: 190, loss: 0.001060374896042049
step: 200, loss: 0.07714731246232986
step: 210, loss: 0.13155904412269592
step: 220, loss: 0.03754210099577904
step: 230, loss: 0.020975075662136078
step: 240, loss: 0.18458373844623566
step: 250, loss: 0.00807058997452259
step: 260, loss: 0.21132203936576843
step: 270, loss: 0.0570940338075161
step: 280, loss: 0.016844812780618668
step: 290, loss: 0.061942242085933685
step: 300, loss: 0.05418087914586067
step: 310, loss: 0.025360288098454475
step: 320, loss: 0.1691942811012268
step: 330, loss: 0.07482379674911499
step: 340, loss: 0.0024905921891331673
step: 350, loss: 0.022222023457288742
step: 360, loss: 0.035921625792980194
step: 370, loss: 0.032766059041023254
step: 380, loss: 0.08544208854436874
step: 390, loss: 0.11618959158658981
step: 400, loss: 0.09194225072860718
step: 410, loss: 0.08779575675725937
step: 420, loss: 0.049320053309202194
step: 430, loss: 0.07960256189107895
step: 440, loss: 0.049758877605199814
step: 450, loss: 0.0997471958398819
step: 460, loss: 0.08986705541610718
step: 470, loss: 0.029286153614521027
step: 480, loss: 0.059644915163517
step: 490, loss: 0.05664302036166191
step: 500, loss: 0.036959223449230194
step: 510, loss: 0.03367466479539871
step: 520, loss: 0.024840474128723145
step: 530, loss: 0.09961112588644028
step: 540, loss: 0.08505714684724808
step: 550, loss: 0.09446293115615845
step: 560, loss: 0.08848349004983902
step: 570, loss: 0.13820978999137878
step: 580, loss: 0.06702695786952972
step: 590, loss: 0.1698233038187027
step: 600, loss: 0.09099777042865753
step: 610, loss: 0.06369229406118393
step: 620, loss: 0.12615758180618286
step: 630, loss: 0.09392944723367691
step: 640, loss: 0.20390041172504425
step: 650, loss: 0.03123602829873562
step: 660, loss: 0.07762318849563599
step: 670, loss: 0.12470506131649017
step: 680, loss: 0.15343260765075684
step: 690, loss: 0.09432128071784973
step: 700, loss: 0.057898618280887604
step: 710, loss: 0.11475291103124619
step: 720, loss: 0.07867463678121567
step: 730, loss: 0.07459381222724915
step: 740, loss: 0.08009553700685501
step: 750, loss: 0.0852997750043869
step: 760, loss: 0.16629569232463837
step: 770, loss: 0.0942450612783432
step: 780, loss: 0.09373676776885986
step: 790, loss: 0.09178159385919571
step: 800, loss: 0.024992745369672775
step: 810, loss: 0.011334938928484917
step: 820, loss: 0.0716237798333168
step: 830, loss: 0.18403634428977966
step: 840, loss: 0.042327575385570526
step: 850, loss: 0.025983525440096855
step: 860, loss: 0.09338521957397461
step: 870, loss: 0.12324411422014236
step: 880, loss: 0.1599266678094864
step: 890, loss: 0.015947511419653893
step: 900, loss: 0.17075762152671814
step: 910, loss: 0.061697691679000854
step: 920, loss: 0.061683814972639084
step: 930, loss: 0.04335184395313263
step: 940, loss: 0.05939776822924614
step: 950, loss: 0.035707298666238785
step: 960, loss: 0.11266780644655228
step: 970, loss: 0.18780070543289185
epoch 11: dev_f1=0.9358736059479555, f1=0.9264909847434121, best_f1=0.9264909847434121
step: 0, loss: 0.018347499892115593
step: 10, loss: 0.01400035060942173
step: 20, loss: 0.14646756649017334
step: 30, loss: 0.06952186673879623
step: 40, loss: 0.08128299564123154
step: 50, loss: 0.05674244835972786
step: 60, loss: 0.05016249045729637
step: 70, loss: 0.022564657032489777
step: 80, loss: 0.013535511679947376
step: 90, loss: 0.05243998020887375
step: 100, loss: 0.09482032060623169
step: 110, loss: 0.15649375319480896
step: 120, loss: 0.12797115743160248
step: 130, loss: 0.052800923585891724
step: 140, loss: 0.019098050892353058
step: 150, loss: 0.03380950912833214
step: 160, loss: 0.11814358830451965
step: 170, loss: 0.05175979807972908
step: 180, loss: 0.04320923984050751
step: 190, loss: 0.05381903052330017
step: 200, loss: 0.13729442656040192
step: 210, loss: 0.031121689826250076
step: 220, loss: 0.05855612829327583
step: 230, loss: 0.06289545446634293
step: 240, loss: 0.04675207659602165
step: 250, loss: 0.08027827739715576
step: 260, loss: 0.04957815259695053
step: 270, loss: 0.05351912975311279
step: 280, loss: 0.11440353840589523
step: 290, loss: 0.06999808549880981
step: 300, loss: 0.06843073666095734
step: 310, loss: 0.05313728749752045
step: 320, loss: 0.056850600987672806
step: 330, loss: 0.1686805635690689
step: 340, loss: 0.11889664083719254
step: 350, loss: 0.002049132715910673
step: 360, loss: 0.05453003570437431
step: 370, loss: 0.0068252068012952805
step: 380, loss: 0.11030957102775574
step: 390, loss: 0.11348817497491837
step: 400, loss: 0.05286354199051857
step: 410, loss: 0.056189898401498795
step: 420, loss: 0.06179235503077507
step: 430, loss: 0.1505061835050583
step: 440, loss: 0.10273093730211258
step: 450, loss: 0.06790781021118164
step: 460, loss: 0.10325408726930618
step: 470, loss: 0.11129862070083618
step: 480, loss: 0.1551131010055542
step: 490, loss: 0.001635348773561418
step: 500, loss: 0.08377845585346222
step: 510, loss: 0.000193055733689107
step: 520, loss: 0.05604204162955284
step: 530, loss: 0.0668102502822876
step: 540, loss: 0.08542611449956894
step: 550, loss: 0.006791726686060429
step: 560, loss: 0.02413017861545086
step: 570, loss: 0.07062458246946335
step: 580, loss: 0.07145366817712784
step: 590, loss: 0.11950596421957016
step: 600, loss: 0.03597841039299965
step: 610, loss: 0.020775508135557175
step: 620, loss: 0.0599001944065094
step: 630, loss: 0.10264144092798233
step: 640, loss: 0.10894685238599777
step: 650, loss: 0.09924066811800003
step: 660, loss: 0.018136849626898766
step: 670, loss: 0.10685396939516068
step: 680, loss: 0.1440047025680542
step: 690, loss: 0.09297757595777512
step: 700, loss: 0.07175169140100479
step: 710, loss: 0.02837499976158142
step: 720, loss: 0.12148725986480713
step: 730, loss: 0.041842617094516754
step: 740, loss: 0.05748910829424858
step: 750, loss: 0.006807632278650999
step: 760, loss: 0.03917922452092171
step: 770, loss: 0.181134894490242
step: 780, loss: 0.0803445428609848
step: 790, loss: 0.050884757190942764
step: 800, loss: 0.033031903207302094
step: 810, loss: 0.06291109323501587
step: 820, loss: 0.009100133553147316
step: 830, loss: 0.1382131725549698
step: 840, loss: 0.15098221600055695
step: 850, loss: 0.040839746594429016
step: 860, loss: 0.0901583880186081
step: 870, loss: 0.13124442100524902
step: 880, loss: 0.014076825231313705
step: 890, loss: 0.04167567566037178
step: 900, loss: 0.07268522679805756
step: 910, loss: 0.08416737616062164
step: 920, loss: 0.21246813237667084
step: 930, loss: 0.12805834412574768
step: 940, loss: 0.05889973044395447
step: 950, loss: 0.04904036968946457
step: 960, loss: 0.09217026084661484
step: 970, loss: 0.05373192951083183
epoch 12: dev_f1=0.9319029850746268, f1=0.919605077574048, best_f1=0.9264909847434121
step: 0, loss: 0.06878386437892914
step: 10, loss: 0.15929681062698364
step: 20, loss: 0.023550573736429214
step: 30, loss: 0.022365910932421684
step: 40, loss: 0.06241974979639053
step: 50, loss: 0.07304996997117996
step: 60, loss: 0.1325470358133316
step: 70, loss: 0.008906502276659012
step: 80, loss: 0.13902227580547333
step: 90, loss: 0.10652322322130203
step: 100, loss: 0.0004603254492394626
step: 110, loss: 0.11851739883422852
step: 120, loss: 0.0682854950428009
step: 130, loss: 0.01390679832547903
step: 140, loss: 0.11028271168470383
step: 150, loss: 0.0115233538672328
step: 160, loss: 0.039296288043260574
step: 170, loss: 0.07716651260852814
step: 180, loss: 0.012072633020579815
step: 190, loss: 0.06709755957126617
step: 200, loss: 0.05253055691719055
step: 210, loss: 0.04281460866332054
step: 220, loss: 0.05019409954547882
step: 230, loss: 0.008733115158975124
step: 240, loss: 0.16014543175697327
step: 250, loss: 0.05774726718664169
step: 260, loss: 0.030606186017394066
step: 270, loss: 0.02318304032087326
step: 280, loss: 0.14456285536289215
step: 290, loss: 0.030738620087504387
step: 300, loss: 0.0008305527735501528
step: 310, loss: 0.008424410596489906
step: 320, loss: 0.05534658208489418
step: 330, loss: 0.05150673910975456
step: 340, loss: 0.048478856682777405
step: 350, loss: 0.09697765111923218
step: 360, loss: 0.10320845991373062
step: 370, loss: 0.043814871460199356
step: 380, loss: 0.007961951196193695
step: 390, loss: 0.018006371334195137
step: 400, loss: 0.11995794624090195
step: 410, loss: 0.01651543192565441
step: 420, loss: 0.0823919028043747
step: 430, loss: 0.043745625764131546
step: 440, loss: 0.04190842807292938
step: 450, loss: 0.06080814450979233
step: 460, loss: 0.00769051443785429
step: 470, loss: 0.045091331005096436
step: 480, loss: 0.11363287270069122
step: 490, loss: 0.09640722721815109
step: 500, loss: 0.021251967176795006
step: 510, loss: 0.026566827669739723
step: 520, loss: 0.05580933019518852
step: 530, loss: 0.021794334053993225
step: 540, loss: 0.1050039678812027
step: 550, loss: 0.01883305422961712
step: 560, loss: 0.10237252712249756
step: 570, loss: 0.020657271146774292
step: 580, loss: 0.01690090261399746
step: 590, loss: 0.060677479952573776
step: 600, loss: 2.943574872915633e-05
step: 610, loss: 0.15048182010650635
step: 620, loss: 0.03152034059166908
step: 630, loss: 0.07870017737150192
step: 640, loss: 0.04407532513141632
step: 650, loss: 0.043451402336359024
step: 660, loss: 0.0479184128344059
step: 670, loss: 0.08300039172172546
step: 680, loss: 0.16267718374729156
step: 690, loss: 0.16221483051776886
step: 700, loss: 0.04926237091422081
step: 710, loss: 0.022854475304484367
step: 720, loss: 0.09672392904758453
step: 730, loss: 0.0001087258497136645
step: 740, loss: 0.03939835727214813
step: 750, loss: 0.08890509605407715
step: 760, loss: 0.06481658667325974
step: 770, loss: 0.0734642744064331
step: 780, loss: 0.09906595945358276
step: 790, loss: 0.07372797280550003
step: 800, loss: 0.06553475558757782
step: 810, loss: 0.054853763431310654
step: 820, loss: 0.12086677551269531
step: 830, loss: 0.03140370920300484
step: 840, loss: 0.0012023269664496183
step: 850, loss: 0.11600351333618164
step: 860, loss: 0.036366574466228485
step: 870, loss: 0.009813542477786541
step: 880, loss: 0.07192962616682053
step: 890, loss: 0.05540123209357262
step: 900, loss: 0.003017871640622616
step: 910, loss: 0.022528279572725296
step: 920, loss: 0.03160739317536354
step: 930, loss: 0.05118132755160332
step: 940, loss: 0.10370883345603943
step: 950, loss: 0.2643967866897583
step: 960, loss: 0.08057449012994766
step: 970, loss: 0.05979694053530693
epoch 13: dev_f1=0.9312617702448212, f1=0.9202279202279201, best_f1=0.9264909847434121
step: 0, loss: 0.10200347006320953
step: 10, loss: 0.04256170615553856
step: 20, loss: 0.055441223084926605
step: 30, loss: 0.011474747210741043
step: 40, loss: 0.05774812400341034
step: 50, loss: 0.07785819470882416
step: 60, loss: 0.027749652042984962
step: 70, loss: 0.04581921920180321
step: 80, loss: 0.034116607159376144
step: 90, loss: 0.11400389671325684
step: 100, loss: 0.01180272363126278
step: 110, loss: 0.028541240841150284
step: 120, loss: 0.017383314669132233
step: 130, loss: 0.00017451243184041232
step: 140, loss: 0.04624386876821518
step: 150, loss: 0.06153113394975662
step: 160, loss: 0.016602961346507072
step: 170, loss: 0.016892138868570328
step: 180, loss: 0.039220020174980164
step: 190, loss: 0.03892281651496887
step: 200, loss: 0.0327114574611187
step: 210, loss: 0.003449509385973215
step: 220, loss: 0.038708947598934174
step: 230, loss: 0.1813773810863495
step: 240, loss: 0.018790194764733315
step: 250, loss: 0.09698204696178436
step: 260, loss: 0.06691589206457138
step: 270, loss: 0.06687182933092117
step: 280, loss: 0.11733026057481766
step: 290, loss: 0.05007873475551605
step: 300, loss: 0.058344628661870956
step: 310, loss: 0.0047101546078920364
step: 320, loss: 0.0609285868704319
step: 330, loss: 0.04468435049057007
step: 340, loss: 0.05308128148317337
step: 350, loss: 0.08652393519878387
step: 360, loss: 0.048631880432367325
step: 370, loss: 0.05854430049657822
step: 380, loss: 0.021175140514969826
step: 390, loss: 0.0931587815284729
step: 400, loss: 0.1412959098815918
step: 410, loss: 0.026708971709012985
step: 420, loss: 0.04423786327242851
step: 430, loss: 0.033888380974531174
step: 440, loss: 0.09708306193351746
step: 450, loss: 0.04600665345788002
step: 460, loss: 0.014772259630262852
step: 470, loss: 0.0268818698823452
step: 480, loss: 0.06125855818390846
step: 490, loss: 0.017871279269456863
step: 500, loss: 0.21472132205963135
step: 510, loss: 0.06430148333311081
step: 520, loss: 0.1236729696393013
step: 530, loss: 0.008498802781105042
step: 540, loss: 0.004641702398657799
step: 550, loss: 0.005641944706439972
step: 560, loss: 0.0030551657546311617
step: 570, loss: 0.07375096529722214
step: 580, loss: 0.11785956472158432
step: 590, loss: 0.11961700767278671
step: 600, loss: 0.04250162094831467
step: 610, loss: 0.021262692287564278
step: 620, loss: 0.025545690208673477
step: 630, loss: 0.0024567507207393646
step: 640, loss: 0.09224565327167511
step: 650, loss: 0.05268946662545204
step: 660, loss: 0.08582304418087006
step: 670, loss: 0.09308433532714844
step: 680, loss: 0.07125639170408249
step: 690, loss: 0.07534351199865341
step: 700, loss: 0.04520542919635773
step: 710, loss: 0.05661768838763237
step: 720, loss: 0.09301246702671051
step: 730, loss: 0.08467898517847061
step: 740, loss: 0.0527951642870903
step: 750, loss: 0.03729652985930443
step: 760, loss: 0.05517099052667618
step: 770, loss: 0.0002868729643523693
step: 780, loss: 0.008538116700947285
step: 790, loss: 0.0077034058049321175
step: 800, loss: 0.10072413086891174
step: 810, loss: 0.088997483253479
step: 820, loss: 0.07240422070026398
step: 830, loss: 0.05249004438519478
step: 840, loss: 0.0631457045674324
step: 850, loss: 0.04857819154858589
step: 860, loss: 2.1203588403295726e-05
step: 870, loss: 0.02825321815907955
step: 880, loss: 0.07878364622592926
step: 890, loss: 0.04303770512342453
step: 900, loss: 0.16672369837760925
step: 910, loss: 0.03522840142250061
step: 920, loss: 0.00017682276666164398
step: 930, loss: 0.11039378494024277
step: 940, loss: 0.17076972126960754
step: 950, loss: 0.0072022913955152035
step: 960, loss: 0.13623656332492828
step: 970, loss: 0.11720842123031616
epoch 14: dev_f1=0.9272137227630968, f1=0.908167974157822, best_f1=0.9264909847434121
step: 0, loss: 0.019318487495183945
step: 10, loss: 0.06860116869211197
step: 20, loss: 0.07362980395555496
step: 30, loss: 0.07220137119293213
step: 40, loss: 0.10672775655984879
step: 50, loss: 0.02527843788266182
step: 60, loss: 0.035630736500024796
step: 70, loss: 0.11486508697271347
step: 80, loss: 0.023804884403944016
step: 90, loss: 0.039385635405778885
step: 100, loss: 0.09967479854822159
step: 110, loss: 0.03245310112833977
step: 120, loss: 0.05801648274064064
step: 130, loss: 0.06733262538909912
step: 140, loss: 0.015166136436164379
step: 150, loss: 0.010630793869495392
step: 160, loss: 0.01685955934226513
step: 170, loss: 0.05070440098643303
step: 180, loss: 0.054802265018224716
step: 190, loss: 0.1291263848543167
step: 200, loss: 0.044567883014678955
step: 210, loss: 0.03460182622075081
step: 220, loss: 0.047789573669433594
step: 230, loss: 0.04721423238515854
step: 240, loss: 0.1063506230711937
step: 250, loss: 0.0458388514816761
step: 260, loss: 0.03450183570384979
step: 270, loss: 0.06087082251906395
step: 280, loss: 0.027530770748853683
step: 290, loss: 0.14044086635112762
step: 300, loss: 0.06853824853897095
step: 310, loss: 0.09177249670028687
step: 320, loss: 0.010711031034588814
step: 330, loss: 0.024464808404445648
step: 340, loss: 0.061827532947063446
step: 350, loss: 0.03533132001757622
step: 360, loss: 0.09287810325622559
step: 370, loss: 0.018204666674137115
step: 380, loss: 0.022759167477488518
step: 390, loss: 0.1116306334733963
step: 400, loss: 0.04734918847680092
step: 410, loss: 0.037947360426187515
step: 420, loss: 0.09858469665050507
step: 430, loss: 0.03853922337293625
step: 440, loss: 0.038101617246866226
step: 450, loss: 0.0123365493491292
step: 460, loss: 0.03758322075009346
step: 470, loss: 0.09608906507492065
step: 480, loss: 0.06152978911995888
step: 490, loss: 0.06487371027469635
step: 500, loss: 0.045964960008859634
step: 510, loss: 0.08918139338493347
step: 520, loss: 0.026384595781564713
step: 530, loss: 0.06959697604179382
step: 540, loss: 0.08233610540628433
step: 550, loss: 0.08739006519317627
step: 560, loss: 0.03291085362434387
step: 570, loss: 0.009361187927424908
step: 580, loss: 0.07160741835832596
step: 590, loss: 0.09086772799491882
step: 600, loss: 0.024423748254776
step: 610, loss: 0.031597357243299484
step: 620, loss: 0.10022696107625961
step: 630, loss: 0.08992340415716171
step: 640, loss: 0.07099702954292297
step: 650, loss: 0.06870313733816147
step: 660, loss: 0.03060217574238777
step: 670, loss: 0.011438761837780476
step: 680, loss: 0.09216811507940292
step: 690, loss: 0.19685541093349457
step: 700, loss: 0.04991509020328522
step: 710, loss: 0.046534691005945206
step: 720, loss: 0.11175186932086945
step: 730, loss: 0.07853052020072937
step: 740, loss: 0.04484431445598602
step: 750, loss: 0.02171514369547367
step: 760, loss: 0.25873324275016785
step: 770, loss: 0.08994384855031967
step: 780, loss: 0.08107609301805496
step: 790, loss: 0.014095681719481945
step: 800, loss: 0.03287271410226822
step: 810, loss: 0.14714328944683075
step: 820, loss: 0.016294024884700775
step: 830, loss: 0.06578680127859116
step: 840, loss: 0.12651126086711884
step: 850, loss: 0.08947081118822098
step: 860, loss: 0.009764264337718487
step: 870, loss: 0.00012837871327064931
step: 880, loss: 0.06870648264884949
step: 890, loss: 0.15848366916179657
step: 900, loss: 0.018972408026456833
step: 910, loss: 0.051387183368206024
step: 920, loss: 0.05309087038040161
step: 930, loss: 0.07570045441389084
step: 940, loss: 0.03284074366092682
step: 950, loss: 0.0635334849357605
step: 960, loss: 0.02542191930115223
step: 970, loss: 0.0815940573811531
epoch 15: dev_f1=0.9325267566309912, f1=0.9143389199255121, best_f1=0.9264909847434121
step: 0, loss: 0.07193822413682938
step: 10, loss: 0.030857672914862633
step: 20, loss: 0.057347431778907776
step: 30, loss: 0.028495751321315765
step: 40, loss: 0.0024496251717209816
step: 50, loss: 0.04474655166268349
step: 60, loss: 0.04966733232140541
step: 70, loss: 0.07656558603048325
step: 80, loss: 0.01816936954855919
step: 90, loss: 0.04898636043071747
step: 100, loss: 0.0780576765537262
step: 110, loss: 0.08800633251667023
step: 120, loss: 0.0018041683360934258
step: 130, loss: 0.037227366119623184
step: 140, loss: 0.0003538691671565175
step: 150, loss: 0.00044855696614831686
step: 160, loss: 0.01776237040758133
step: 170, loss: 0.04961185157299042
step: 180, loss: 0.014721525833010674
step: 190, loss: 0.056722573935985565
step: 200, loss: 0.00047654908848926425
step: 210, loss: 0.001078294008038938
step: 220, loss: 0.033927083015441895
step: 230, loss: 0.06516923755407333
step: 240, loss: 0.04318023845553398
step: 250, loss: 0.03653848543763161
step: 260, loss: 0.018483750522136688
step: 270, loss: 0.04385816678404808
step: 280, loss: 0.10491547733545303
step: 290, loss: 0.11835283786058426
step: 300, loss: 0.059018783271312714
step: 310, loss: 0.025161907076835632
step: 320, loss: 0.07361649721860886
step: 330, loss: 0.02733214572072029
step: 340, loss: 0.082636758685112
step: 350, loss: 0.00048369623254984617
step: 360, loss: 0.19790542125701904
step: 370, loss: 0.0817698985338211
step: 380, loss: 0.12270484864711761
step: 390, loss: 0.11896834522485733
step: 400, loss: 0.08782953768968582
step: 410, loss: 0.0521618016064167
step: 420, loss: 0.06689982116222382
step: 430, loss: 0.02007557637989521
step: 440, loss: 0.12259548157453537
step: 450, loss: 0.002920409431681037
step: 460, loss: 0.08307895064353943
step: 470, loss: 0.17414353787899017
step: 480, loss: 0.054940760135650635
step: 490, loss: 0.049536172300577164
step: 500, loss: 0.057293787598609924
step: 510, loss: 0.03418632969260216
step: 520, loss: 0.04336834326386452
step: 530, loss: 0.07265628129243851
step: 540, loss: 0.1087934598326683
step: 550, loss: 0.025560446083545685
step: 560, loss: 0.06756328791379929
step: 570, loss: 0.07484079897403717
step: 580, loss: 0.05645949766039848
step: 590, loss: 0.009885015897452831
step: 600, loss: 0.08109250664710999
step: 610, loss: 0.08512517064809799
step: 620, loss: 0.10960172861814499
step: 630, loss: 0.0788303092122078
step: 640, loss: 0.06322814524173737
step: 650, loss: 0.06386721879243851
step: 660, loss: 0.097071073949337
step: 670, loss: 0.04499077424407005
step: 680, loss: 0.03915318846702576
step: 690, loss: 0.06356929242610931
step: 700, loss: 0.09670621901750565
step: 710, loss: 0.04121645912528038
step: 720, loss: 0.0019577627535909414
step: 730, loss: 0.16418199241161346
step: 740, loss: 0.11781682074069977
step: 750, loss: 0.08617802709341049
step: 760, loss: 0.00997919775545597
step: 770, loss: 0.020771324634552002
step: 780, loss: 0.06251343339681625
step: 790, loss: 0.07189595699310303
step: 800, loss: 0.026778001338243484
step: 810, loss: 0.021480541676282883
step: 820, loss: 0.05658411234617233
step: 830, loss: 0.023449985310435295
step: 840, loss: 0.06642282754182816
step: 850, loss: 0.07266432046890259
step: 860, loss: 0.02260224148631096
step: 870, loss: 0.04910888150334358
step: 880, loss: 0.01690109260380268
step: 890, loss: 0.023687370121479034
step: 900, loss: 0.03088042512536049
step: 910, loss: 0.06348076462745667
step: 920, loss: 0.048077236860990524
step: 930, loss: 0.04821401834487915
step: 940, loss: 0.026806335896253586
step: 950, loss: 0.08454003930091858
step: 960, loss: 0.014000202529132366
step: 970, loss: 0.06529347598552704
epoch 16: dev_f1=0.9333333333333333, f1=0.915270018621974, best_f1=0.9264909847434121
step: 0, loss: 0.04770529642701149
step: 10, loss: 0.09627161175012589
step: 20, loss: 0.001616719295270741
step: 30, loss: 0.04750977084040642
step: 40, loss: 0.06264147162437439
step: 50, loss: 0.04122099652886391
step: 60, loss: 0.022905103862285614
step: 70, loss: 0.03357519954442978
step: 80, loss: 0.011015910655260086
step: 90, loss: 0.06042744964361191
step: 100, loss: 0.019944176077842712
step: 110, loss: 0.03862474858760834
step: 120, loss: 0.07800643146038055
step: 130, loss: 0.07466971129179001
step: 140, loss: 0.02265010215342045
step: 150, loss: 0.06458730250597
step: 160, loss: 0.042961012572050095
step: 170, loss: 0.012495111674070358
step: 180, loss: 0.10731782019138336
step: 190, loss: 0.03504771366715431
step: 200, loss: 0.0015605987282469869
step: 210, loss: 0.05161473527550697
step: 220, loss: 0.012844513170421124
step: 230, loss: 0.03017447143793106
step: 240, loss: 0.031184382736682892
step: 250, loss: 0.0819779708981514
step: 260, loss: 0.06202133744955063
step: 270, loss: 0.09767687320709229
step: 280, loss: 0.0392816886305809
step: 290, loss: 8.903088746592402e-05
step: 300, loss: 0.09948304295539856
step: 310, loss: 0.015884671360254288
step: 320, loss: 0.02231403812766075
step: 330, loss: 0.023909350857138634
step: 340, loss: 0.023851685225963593
step: 350, loss: 0.14868727326393127
step: 360, loss: 0.08356138318777084
step: 370, loss: 0.15217918157577515
step: 380, loss: 0.06967625766992569
step: 390, loss: 0.04671107977628708
step: 400, loss: 0.02591821178793907
step: 410, loss: 0.025408796966075897
step: 420, loss: 0.046775173395872116
step: 430, loss: 0.07653354108333588
step: 440, loss: 0.06087639927864075
step: 450, loss: 0.11731591075658798
step: 460, loss: 0.030843837186694145
step: 470, loss: 0.08624877780675888
step: 480, loss: 0.014660142362117767
step: 490, loss: 0.08260677009820938
step: 500, loss: 0.060246922075748444
step: 510, loss: 0.002444949932396412
step: 520, loss: 0.07482904940843582
step: 530, loss: 0.03654833510518074
step: 540, loss: 0.050591256469488144
step: 550, loss: 0.10196756571531296
step: 560, loss: 0.12638941407203674
step: 570, loss: 0.03787659481167793
step: 580, loss: 0.07519352436065674
step: 590, loss: 0.016299843788146973
step: 600, loss: 0.09101568162441254
step: 610, loss: 0.16284577548503876
step: 620, loss: 0.03197031468153
step: 630, loss: 0.05848605930805206
step: 640, loss: 0.11678873002529144
step: 650, loss: 0.03424917533993721
step: 660, loss: 4.968116263626143e-05
step: 670, loss: 0.05828629061579704
step: 680, loss: 0.02992212399840355
step: 690, loss: 0.00668666698038578
step: 700, loss: 0.03086627833545208
step: 710, loss: 0.11801531165838242
step: 720, loss: 0.09703376889228821
step: 730, loss: 0.05345166474580765
step: 740, loss: 0.03078272007405758
step: 750, loss: 0.008487626910209656
step: 760, loss: 0.002547740936279297
step: 770, loss: 0.07495204359292984
step: 780, loss: 0.0007119383662939072
step: 790, loss: 0.03059375286102295
step: 800, loss: 0.03567976504564285
step: 810, loss: 0.06533592194318771
step: 820, loss: 0.033956512808799744
step: 830, loss: 0.05886922404170036
step: 840, loss: 0.06544360518455505
step: 850, loss: 0.015408162027597427
step: 860, loss: 0.02793998457491398
step: 870, loss: 0.011180222034454346
step: 880, loss: 0.011478293687105179
step: 890, loss: 0.06892021000385284
step: 900, loss: 0.0879283994436264
step: 910, loss: 0.0748157799243927
step: 920, loss: 0.14103534817695618
step: 930, loss: 0.0700046718120575
step: 940, loss: 0.021339349448680878
step: 950, loss: 0.02468312904238701
step: 960, loss: 0.1354399174451828
step: 970, loss: 0.0016253154026344419
epoch 17: dev_f1=0.9359151682803135, f1=0.9204440333024977, best_f1=0.9204440333024977
step: 0, loss: 0.03681020066142082
step: 10, loss: 0.013890286907553673
step: 20, loss: 0.08332286030054092
step: 30, loss: 0.01848200336098671
step: 40, loss: 0.014875473454594612
step: 50, loss: 0.03237020596861839
step: 60, loss: 0.0023855813778936863
step: 70, loss: 0.07948949187994003
step: 80, loss: 0.07229946553707123
step: 90, loss: 0.061343155801296234
step: 100, loss: 0.050601910799741745
step: 110, loss: 0.020153671503067017
step: 120, loss: 0.08177368342876434
step: 130, loss: 0.014306679368019104
step: 140, loss: 0.1199246272444725
step: 150, loss: 0.03611714020371437
step: 160, loss: 0.07552137970924377
step: 170, loss: 0.043078575283288956
step: 180, loss: 0.030023010447621346
step: 190, loss: 0.02858727052807808
step: 200, loss: 0.07518955320119858
step: 210, loss: 0.0002270512341056019
step: 220, loss: 0.027356404811143875
step: 230, loss: 0.08008331060409546
step: 240, loss: 0.050561752170324326
step: 250, loss: 0.04330630972981453
step: 260, loss: 0.030735714361071587
step: 270, loss: 0.1111532673239708
step: 280, loss: 0.07429204881191254
step: 290, loss: 0.05403660237789154
step: 300, loss: 0.06159638613462448
step: 310, loss: 0.07368694990873337
step: 320, loss: 0.0008174639078788459
step: 330, loss: 0.1772044450044632
step: 340, loss: 0.06102113053202629
step: 350, loss: 0.006322619970887899
step: 360, loss: 0.06327586621046066
step: 370, loss: 0.016470707952976227
step: 380, loss: 0.09252703189849854
step: 390, loss: 0.03258262574672699
step: 400, loss: 0.023789968341588974
step: 410, loss: 0.014456390403211117
step: 420, loss: 0.12219176441431046
step: 430, loss: 0.03149612993001938
step: 440, loss: 0.1069389209151268
step: 450, loss: 0.030704166740179062
step: 460, loss: 0.0990951880812645
step: 470, loss: 0.03203403577208519
step: 480, loss: 0.01881984993815422
step: 490, loss: 0.1426020860671997
step: 500, loss: 3.372069841134362e-05
step: 510, loss: 0.08552788197994232
step: 520, loss: 0.046120189130306244
step: 530, loss: 0.05836031958460808
step: 540, loss: 0.058289140462875366
step: 550, loss: 0.031223835423588753
step: 560, loss: 0.04586018994450569
step: 570, loss: 0.037279412150382996
step: 580, loss: 0.03948330879211426
step: 590, loss: 0.06300710886716843
step: 600, loss: 0.06532743573188782
step: 610, loss: 0.03604225441813469
step: 620, loss: 0.0002438298106426373
step: 630, loss: 0.0854908898472786
step: 640, loss: 0.058118388056755066
step: 650, loss: 0.10044839233160019
step: 660, loss: 0.07271448522806168
step: 670, loss: 0.02121715433895588
step: 680, loss: 0.03336770460009575
step: 690, loss: 0.04362806677818298
step: 700, loss: 0.03644745796918869
step: 710, loss: 0.06117641180753708
step: 720, loss: 0.03353457897901535
step: 730, loss: 0.16757456958293915
step: 740, loss: 0.0694960206747055
step: 750, loss: 0.03647351264953613
step: 760, loss: 0.06082645431160927
step: 770, loss: 0.03340127319097519
step: 780, loss: 0.014877828769385815
step: 790, loss: 0.03897700086236
step: 800, loss: 0.044437240809202194
step: 810, loss: 0.11149345338344574
step: 820, loss: 0.05893436446785927
step: 830, loss: 0.03907401114702225
step: 840, loss: 0.0009633498848415911
step: 850, loss: 0.03973458334803581
step: 860, loss: 6.393858348019421e-05
step: 870, loss: 0.04523500055074692
step: 880, loss: 9.016387775773183e-05
step: 890, loss: 0.009376552887260914
step: 900, loss: 0.12926413118839264
step: 910, loss: 0.030115801841020584
step: 920, loss: 0.028203777968883514
step: 930, loss: 0.062445640563964844
step: 940, loss: 0.08325878530740738
step: 950, loss: 0.07743886858224869
step: 960, loss: 0.0615045428276062
step: 970, loss: 0.04376615956425667
epoch 18: dev_f1=0.9301675977653632, f1=0.9198508853681266, best_f1=0.9204440333024977
step: 0, loss: 0.032166488468647
step: 10, loss: 0.022639712318778038
step: 20, loss: 0.023757167160511017
step: 30, loss: 0.011045040562748909
step: 40, loss: 0.04277997463941574
step: 50, loss: 0.07436154782772064
step: 60, loss: 0.05825292691588402
step: 70, loss: 0.05016445368528366
step: 80, loss: 0.013896887190639973
step: 90, loss: 0.028568219393491745
step: 100, loss: 0.05936019495129585
step: 110, loss: 0.025642573833465576
step: 120, loss: 0.026309939101338387
step: 130, loss: 0.03919866308569908
step: 140, loss: 0.03445294499397278
step: 150, loss: 0.02812715619802475
step: 160, loss: 0.09153619408607483
step: 170, loss: 0.0009232079028151929
step: 180, loss: 0.009801092557609081
step: 190, loss: 0.010118981823325157
step: 200, loss: 0.02404698170721531
step: 210, loss: 0.07379459589719772
step: 220, loss: 0.03137596696615219
step: 230, loss: 0.012243200093507767
step: 240, loss: 0.10477949678897858
step: 250, loss: 0.04533187672495842
step: 260, loss: 0.06160314381122589
step: 270, loss: 0.000334207434207201
step: 280, loss: 0.046578481793403625
step: 290, loss: 0.10939110070466995
step: 300, loss: 0.02840627357363701
step: 310, loss: 0.01757633313536644
step: 320, loss: 0.06143980473279953
step: 330, loss: 0.03898726403713226
step: 340, loss: 0.03881864249706268
step: 350, loss: 0.12667612731456757
step: 360, loss: 0.019741244614124298
step: 370, loss: 0.10509870201349258
step: 380, loss: 0.055999934673309326
step: 390, loss: 0.00026924023404717445
step: 400, loss: 0.12421064078807831
step: 410, loss: 0.006695233751088381
step: 420, loss: 0.03424956277012825
step: 430, loss: 2.788359961414244e-05
step: 440, loss: 0.00030690693529322743
step: 450, loss: 0.025176623836159706
step: 460, loss: 0.06623180955648422
step: 470, loss: 0.09779570996761322
step: 480, loss: 0.045117732137441635
step: 490, loss: 0.09162630885839462
step: 500, loss: 0.027298161759972572
step: 510, loss: 0.034194353967905045
step: 520, loss: 0.057491399347782135
step: 530, loss: 0.012566640973091125
step: 540, loss: 0.06562947481870651
step: 550, loss: 0.0001623603020561859
step: 560, loss: 0.012269739992916584
step: 570, loss: 0.00027674829470925033
step: 580, loss: 0.04914505407214165
step: 590, loss: 0.0263248011469841
step: 600, loss: 0.06902863085269928
step: 610, loss: 0.04474678635597229
step: 620, loss: 0.04351597651839256
step: 630, loss: 0.006346190348267555
step: 640, loss: 0.07290338724851608
step: 650, loss: 0.1255786269903183
step: 660, loss: 0.037626832723617554
step: 670, loss: 0.06826724857091904
step: 680, loss: 0.024347642436623573
step: 690, loss: 0.007662785705178976
step: 700, loss: 0.036865703761577606
step: 710, loss: 5.7161258155247197e-05
step: 720, loss: 0.03678261861205101
step: 730, loss: 0.03189627081155777
step: 740, loss: 0.04037579894065857
step: 750, loss: 0.10882022976875305
step: 760, loss: 0.0009843248408287764
step: 770, loss: 0.04897153005003929
step: 780, loss: 0.029586244374513626
step: 790, loss: 0.00017236794519703835
step: 800, loss: 0.0696743056178093
step: 810, loss: 0.018046453595161438
step: 820, loss: 0.08000040054321289
step: 830, loss: 0.03685376048088074
step: 840, loss: 0.049376875162124634
step: 850, loss: 0.015074688009917736
step: 860, loss: 0.09963330626487732
step: 870, loss: 0.15005354583263397
step: 880, loss: 0.06002353876829147
step: 890, loss: 0.03958417847752571
step: 900, loss: 0.021015387028455734
step: 910, loss: 0.002349413465708494
step: 920, loss: 0.07405852526426315
step: 930, loss: 0.027698371559381485
step: 940, loss: 0.0380040779709816
step: 950, loss: 7.780646410537884e-05
step: 960, loss: 0.09890418499708176
step: 970, loss: 0.05582335963845253
epoch 19: dev_f1=0.9335192933519293, f1=0.9225023342670402, best_f1=0.9204440333024977
step: 0, loss: 0.007875876501202583
step: 10, loss: 0.11946751177310944
step: 20, loss: 7.29325256543234e-05
step: 30, loss: 0.02824385091662407
step: 40, loss: 0.07055346667766571
step: 50, loss: 0.023579642176628113
step: 60, loss: 0.0330352708697319
step: 70, loss: 0.08116426318883896
step: 80, loss: 0.041780583560466766
step: 90, loss: 0.009209233336150646
step: 100, loss: 0.07671032100915909
step: 110, loss: 0.00022240998805500567
step: 120, loss: 0.01931842230260372
step: 130, loss: 0.03676585108041763
step: 140, loss: 0.0825221911072731
step: 150, loss: 0.11887896806001663
step: 160, loss: 0.05381881073117256
step: 170, loss: 0.1534147709608078
step: 180, loss: 0.07110149413347244
step: 190, loss: 0.00028500184998847544
step: 200, loss: 0.03801429644227028
step: 210, loss: 0.0636977106332779
step: 220, loss: 0.027359796687960625
step: 230, loss: 0.02856985293328762
step: 240, loss: 0.044937413185834885
step: 250, loss: 0.009824934415519238
step: 260, loss: 0.015515723265707493
step: 270, loss: 0.06974688172340393
step: 280, loss: 0.08315198123455048
step: 290, loss: 0.05950183421373367
step: 300, loss: 0.02747335098683834
step: 310, loss: 0.11823224276304245
step: 320, loss: 0.022407829761505127
step: 330, loss: 0.04015723243355751
step: 340, loss: 0.19764024019241333
step: 350, loss: 0.02955254353582859
step: 360, loss: 0.044887296855449677
step: 370, loss: 0.0943257063627243
step: 380, loss: 0.13243351876735687
step: 390, loss: 0.029542045667767525
step: 400, loss: 2.7086445697932504e-05
step: 410, loss: 0.03484799340367317
step: 420, loss: 9.398059773957357e-05
step: 430, loss: 0.05893963575363159
step: 440, loss: 0.044511646032333374
step: 450, loss: 0.00797607284039259
step: 460, loss: 0.08753451704978943
step: 470, loss: 0.026173828169703484
step: 480, loss: 0.09226001799106598
step: 490, loss: 0.024943822994828224
step: 500, loss: 0.06680591404438019
step: 510, loss: 0.025836320593953133
step: 520, loss: 0.053317610174417496
step: 530, loss: 0.006626961752772331
step: 540, loss: 0.12677626311779022
step: 550, loss: 0.061343949288129807
step: 560, loss: 0.06163782998919487
step: 570, loss: 0.031580474227666855
step: 580, loss: 0.00011953548528254032
step: 590, loss: 0.03160831704735756
step: 600, loss: 0.0030109521467238665
step: 610, loss: 0.05417473986744881
step: 620, loss: 0.07158873975276947
step: 630, loss: 0.000458946218714118
step: 640, loss: 0.052872464060783386
step: 650, loss: 0.028109323233366013
step: 660, loss: 0.10826052725315094
step: 670, loss: 0.07245059311389923
step: 680, loss: 0.02701684646308422
step: 690, loss: 0.05243478715419769
step: 700, loss: 0.039648573845624924
step: 710, loss: 0.00026943953707814217
step: 720, loss: 0.0253062155097723
step: 730, loss: 0.03988805413246155
step: 740, loss: 0.07699372619390488
step: 750, loss: 0.05566244199872017
step: 760, loss: 0.03594830632209778
step: 770, loss: 0.07080980390310287
step: 780, loss: 0.01738753542304039
step: 790, loss: 0.022713718935847282
step: 800, loss: 0.028612352907657623
step: 810, loss: 0.031526241451501846
step: 820, loss: 0.012562989257276058
step: 830, loss: 0.05456181988120079
step: 840, loss: 0.12134493142366409
step: 850, loss: 0.03001897782087326
step: 860, loss: 0.013522510416805744
step: 870, loss: 0.09828057140111923
step: 880, loss: 0.0493997298181057
step: 890, loss: 0.019748985767364502
step: 900, loss: 0.021327614784240723
step: 910, loss: 0.021526440978050232
step: 920, loss: 0.024851061403751373
step: 930, loss: 0.027186835184693336
step: 940, loss: 0.04284629225730896
step: 950, loss: 0.05243820697069168
step: 960, loss: 0.01494878064841032
step: 970, loss: 0.037791766226291656
epoch 20: dev_f1=0.9312119794103885, f1=0.9191729323308271, best_f1=0.9204440333024977
