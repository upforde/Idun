cuda
Device: cuda
step: 0, loss: 0.7216890454292297
step: 10, loss: 0.32162556052207947
step: 20, loss: 0.4656866490840912
step: 30, loss: 0.6163533926010132
step: 40, loss: 0.3086458146572113
step: 50, loss: 0.3095014691352844
step: 60, loss: 0.37719112634658813
step: 70, loss: 0.23567070066928864
step: 80, loss: 0.26433485746383667
step: 90, loss: 0.18911097943782806
step: 100, loss: 0.40152350068092346
step: 110, loss: 0.36386343836784363
step: 120, loss: 0.3223297894001007
step: 130, loss: 0.22829227149486542
step: 140, loss: 0.34903889894485474
step: 150, loss: 0.36104121804237366
step: 160, loss: 0.2924734354019165
step: 170, loss: 0.1473160982131958
step: 180, loss: 0.19548936188220978
step: 190, loss: 0.15606744587421417
step: 200, loss: 0.16377897560596466
step: 210, loss: 0.25380939245224
step: 220, loss: 0.2661689817905426
step: 230, loss: 0.42880263924598694
step: 240, loss: 0.17306795716285706
step: 250, loss: 0.3414172828197479
step: 260, loss: 0.24554097652435303
step: 270, loss: 0.10083488374948502
step: 280, loss: 0.19390907883644104
step: 290, loss: 0.4361705183982849
step: 300, loss: 0.18909408152103424
step: 310, loss: 0.36328980326652527
step: 320, loss: 0.21587099134922028
step: 330, loss: 0.21423082053661346
step: 340, loss: 0.1670534312725067
step: 350, loss: 0.4108775854110718
epoch 1: dev_f1=0.5368171021377673, f1=0.46080760095011875, best_f1=0.46080760095011875
step: 0, loss: 0.3114605247974396
step: 10, loss: 0.20590685307979584
step: 20, loss: 0.10313747823238373
step: 30, loss: 0.1574184149503708
step: 40, loss: 0.3470441699028015
step: 50, loss: 0.3467872738838196
step: 60, loss: 0.27737802267074585
step: 70, loss: 0.3227578103542328
step: 80, loss: 0.18073120713233948
step: 90, loss: 0.09088396281003952
step: 100, loss: 0.2501823604106903
step: 110, loss: 0.4302757680416107
step: 120, loss: 0.11511969566345215
step: 130, loss: 0.04468138888478279
step: 140, loss: 0.1595476269721985
step: 150, loss: 0.2328757494688034
step: 160, loss: 0.13868078589439392
step: 170, loss: 0.3463524878025055
step: 180, loss: 0.4098440408706665
step: 190, loss: 0.2544820010662079
step: 200, loss: 0.1389170140028
step: 210, loss: 0.15385887026786804
step: 220, loss: 0.20999373495578766
step: 230, loss: 0.2559911608695984
step: 240, loss: 0.26665830612182617
step: 250, loss: 0.1441832333803177
step: 260, loss: 0.2671404182910919
step: 270, loss: 0.08705524355173111
step: 280, loss: 0.14620311558246613
step: 290, loss: 0.17681360244750977
step: 300, loss: 0.052899762988090515
step: 310, loss: 0.08588468283414841
step: 320, loss: 0.2090798020362854
step: 330, loss: 0.33200588822364807
step: 340, loss: 0.034340087324380875
step: 350, loss: 0.14081686735153198
epoch 2: dev_f1=0.722943722943723, f1=0.6822033898305084, best_f1=0.6822033898305084
step: 0, loss: 0.1269935965538025
step: 10, loss: 0.08268918097019196
step: 20, loss: 0.07968714833259583
step: 30, loss: 0.01621328853070736
step: 40, loss: 0.2850230038166046
step: 50, loss: 0.03776559233665466
step: 60, loss: 0.21203413605690002
step: 70, loss: 0.03649011626839638
step: 80, loss: 0.18497811257839203
step: 90, loss: 0.34803083539009094
step: 100, loss: 0.25590386986732483
step: 110, loss: 0.1338934302330017
step: 120, loss: 0.039376821368932724
step: 130, loss: 0.20304299890995026
step: 140, loss: 0.1592874377965927
step: 150, loss: 0.18510575592517853
step: 160, loss: 0.1452404409646988
step: 170, loss: 0.12293988466262817
step: 180, loss: 0.030250685289502144
step: 190, loss: 0.04969603195786476
step: 200, loss: 0.19464947283267975
step: 210, loss: 0.029168112203478813
step: 220, loss: 0.24979658424854279
step: 230, loss: 0.115098737180233
step: 240, loss: 0.17606639862060547
step: 250, loss: 0.09710231423377991
step: 260, loss: 0.07277152687311172
step: 270, loss: 0.029962241649627686
step: 280, loss: 0.20879705250263214
step: 290, loss: 0.16319164633750916
step: 300, loss: 0.3162994980812073
step: 310, loss: 0.03487284481525421
step: 320, loss: 0.01609889604151249
step: 330, loss: 0.04083119332790375
step: 340, loss: 0.1660975068807602
step: 350, loss: 0.08139914274215698
epoch 3: dev_f1=0.7358916478555305, f1=0.7217391304347827, best_f1=0.7217391304347827
step: 0, loss: 0.04439013823866844
step: 10, loss: 0.07594763487577438
step: 20, loss: 0.024747682735323906
step: 30, loss: 0.0839008316397667
step: 40, loss: 0.05180320143699646
step: 50, loss: 0.0813334584236145
step: 60, loss: 0.12747840583324432
step: 70, loss: 0.0495673231780529
step: 80, loss: 0.14227977395057678
step: 90, loss: 0.0707181841135025
step: 100, loss: 0.02993135154247284
step: 110, loss: 0.013183233328163624
step: 120, loss: 0.1633235067129135
step: 130, loss: 0.03189631924033165
step: 140, loss: 0.12188608199357986
step: 150, loss: 0.13471266627311707
step: 160, loss: 0.05533669516444206
step: 170, loss: 0.0479598268866539
step: 180, loss: 0.20702743530273438
step: 190, loss: 0.02371516078710556
step: 200, loss: 0.018248461186885834
step: 210, loss: 0.18245047330856323
step: 220, loss: 0.23885060846805573
step: 230, loss: 0.11283741891384125
step: 240, loss: 0.03710167855024338
step: 250, loss: 0.07715144008398056
step: 260, loss: 0.13776279985904694
step: 270, loss: 0.042279791086912155
step: 280, loss: 0.01900758408010006
step: 290, loss: 0.01961376704275608
step: 300, loss: 0.10904140025377274
step: 310, loss: 0.02566615119576454
step: 320, loss: 0.0681394711136818
step: 330, loss: 0.10755650699138641
step: 340, loss: 0.03909614682197571
step: 350, loss: 0.2036582976579666
epoch 4: dev_f1=0.7828282828282828, f1=0.7531806615776081, best_f1=0.7531806615776081
step: 0, loss: 0.04219605028629303
step: 10, loss: 0.03010503761470318
step: 20, loss: 0.055419113487005234
step: 30, loss: 0.003075371030718088
step: 40, loss: 0.018122689798474312
step: 50, loss: 0.03357639163732529
step: 60, loss: 0.16502580046653748
step: 70, loss: 0.06057509034872055
step: 80, loss: 0.010084614157676697
step: 90, loss: 0.03711091727018356
step: 100, loss: 0.056824591010808945
step: 110, loss: 0.05519350245594978
step: 120, loss: 0.07756955176591873
step: 130, loss: 0.041412364691495895
step: 140, loss: 0.015493281185626984
step: 150, loss: 0.016892312094569206
step: 160, loss: 0.009649960324168205
step: 170, loss: 0.07646176218986511
step: 180, loss: 0.013216142542660236
step: 190, loss: 0.021797621622681618
step: 200, loss: 0.081943079829216
step: 210, loss: 0.2250518649816513
step: 220, loss: 0.08158250153064728
step: 230, loss: 0.08822070062160492
step: 240, loss: 0.033947814255952835
step: 250, loss: 0.08295983076095581
step: 260, loss: 0.010787736624479294
step: 270, loss: 0.07978364825248718
step: 280, loss: 0.11876824498176575
step: 290, loss: 0.029227204620838165
step: 300, loss: 0.03855234012007713
step: 310, loss: 0.06840695440769196
step: 320, loss: 0.015039325691759586
step: 330, loss: 0.013582060113549232
step: 340, loss: 0.03589380532503128
step: 350, loss: 0.10374815762042999
epoch 5: dev_f1=0.7802469135802469, f1=0.7722772277227724, best_f1=0.7531806615776081
step: 0, loss: 0.010522921569645405
step: 10, loss: 0.04108745977282524
step: 20, loss: 0.027918267995119095
step: 30, loss: 0.010052165947854519
step: 40, loss: 0.011151636019349098
step: 50, loss: 0.23386843502521515
step: 60, loss: 0.0026223035529255867
step: 70, loss: 0.09842725843191147
step: 80, loss: 0.013041150756180286
step: 90, loss: 0.0003619235067162663
step: 100, loss: 0.0053468006663024426
step: 110, loss: 0.024127038195729256
step: 120, loss: 0.007588146720081568
step: 130, loss: 0.011036435142159462
step: 140, loss: 0.07740875333547592
step: 150, loss: 0.05417368933558464
step: 160, loss: 0.10491200536489487
step: 170, loss: 0.004944105166941881
step: 180, loss: 0.0006630742573179305
step: 190, loss: 0.002962411381304264
step: 200, loss: 0.005114778410643339
step: 210, loss: 0.005499367602169514
step: 220, loss: 0.027208680287003517
step: 230, loss: 0.11063157021999359
step: 240, loss: 0.0823289155960083
step: 250, loss: 0.025960983708500862
step: 260, loss: 0.01721886731684208
step: 270, loss: 0.05351513251662254
step: 280, loss: 0.002670923015102744
step: 290, loss: 0.002637814497575164
step: 300, loss: 0.03230065479874611
step: 310, loss: 0.0037383604794740677
step: 320, loss: 0.0035565136931836605
step: 330, loss: 0.0105799725279212
step: 340, loss: 0.02748219110071659
step: 350, loss: 0.02170265093445778
epoch 6: dev_f1=0.7741935483870966, f1=0.7584650112866818, best_f1=0.7531806615776081
step: 0, loss: 0.09350579231977463
step: 10, loss: 0.013271796517074108
step: 20, loss: 0.09943297505378723
step: 30, loss: 0.04581622779369354
step: 40, loss: 0.0008350495481863618
step: 50, loss: 0.012710346840322018
step: 60, loss: 0.03434884920716286
step: 70, loss: 0.00164856540504843
step: 80, loss: 0.07823299616575241
step: 90, loss: 0.0005792250158265233
step: 100, loss: 0.030338548123836517
step: 110, loss: 0.051245443522930145
step: 120, loss: 0.1126289963722229
step: 130, loss: 0.014292238280177116
step: 140, loss: 0.014026752673089504
step: 150, loss: 0.010227787308394909
step: 160, loss: 0.0394938662648201
step: 170, loss: 0.08462202548980713
step: 180, loss: 0.009744517505168915
step: 190, loss: 0.0059299166314303875
step: 200, loss: 0.022634807974100113
step: 210, loss: 0.040673982352018356
step: 220, loss: 0.0012196189491078258
step: 230, loss: 0.08189935237169266
step: 240, loss: 0.06970986723899841
step: 250, loss: 0.02005111053586006
step: 260, loss: 0.0019073931034654379
step: 270, loss: 0.010459324344992638
step: 280, loss: 0.0041138227097690105
step: 290, loss: 0.07953716069459915
step: 300, loss: 0.009217771701514721
step: 310, loss: 0.003933126572519541
step: 320, loss: 0.022692611441016197
step: 330, loss: 0.019523408263921738
step: 340, loss: 0.0031155571341514587
step: 350, loss: 0.004692106507718563
epoch 7: dev_f1=0.7650273224043717, f1=0.726775956284153, best_f1=0.7531806615776081
step: 0, loss: 0.004924047738313675
step: 10, loss: 0.0001389489189023152
step: 20, loss: 0.0007343458128161728
step: 30, loss: 0.04524930939078331
step: 40, loss: 0.005769266746938229
step: 50, loss: 0.0027899316046386957
step: 60, loss: 0.001062438590452075
step: 70, loss: 0.002800479531288147
step: 80, loss: 0.008350374177098274
step: 90, loss: 0.005694853141903877
step: 100, loss: 0.0014764067018404603
step: 110, loss: 0.012809524312615395
step: 120, loss: 0.01948980800807476
step: 130, loss: 0.016933176666498184
step: 140, loss: 0.02511407621204853
step: 150, loss: 0.00215941877104342
step: 160, loss: 0.1212923526763916
step: 170, loss: 0.1852855384349823
step: 180, loss: 0.0638580247759819
step: 190, loss: 0.009896471165120602
step: 200, loss: 0.001652571721933782
step: 210, loss: 0.002697786083444953
step: 220, loss: 0.0021346600260585546
step: 230, loss: 0.05558155104517937
step: 240, loss: 0.10285364836454391
step: 250, loss: 0.09476079046726227
step: 260, loss: 0.00041539472294971347
step: 270, loss: 0.018555015325546265
step: 280, loss: 0.10377693176269531
step: 290, loss: 0.014304732903838158
step: 300, loss: 0.0012939758598804474
step: 310, loss: 0.013191026635468006
step: 320, loss: 0.01566634513437748
step: 330, loss: 0.005362381227314472
step: 340, loss: 0.0015299090882763267
step: 350, loss: 0.009580958634614944
epoch 8: dev_f1=0.7674418604651163, f1=0.7494356659142212, best_f1=0.7531806615776081
step: 0, loss: 0.00047676460235379636
step: 10, loss: 0.06247572600841522
step: 20, loss: 0.00213093520142138
step: 30, loss: 0.022868545725941658
step: 40, loss: 0.011351454071700573
step: 50, loss: 0.006781446747481823
step: 60, loss: 0.08382827788591385
step: 70, loss: 0.00209532817825675
step: 80, loss: 0.0005219230079092085
step: 90, loss: 0.0027281884104013443
step: 100, loss: 0.019941510632634163
step: 110, loss: 0.014732609502971172
step: 120, loss: 0.0010928099509328604
step: 130, loss: 0.001864835387095809
step: 140, loss: 0.039869524538517
step: 150, loss: 0.002562904264777899
step: 160, loss: 0.001044530887156725
step: 170, loss: 0.18394619226455688
step: 180, loss: 0.016330743208527565
step: 190, loss: 0.0015549412928521633
step: 200, loss: 0.001028824015520513
step: 210, loss: 0.0005575845716521144
step: 220, loss: 0.006517102941870689
step: 230, loss: 0.0002575030957814306
step: 240, loss: 0.03526933118700981
step: 250, loss: 0.00027216144371777773
step: 260, loss: 0.024296129122376442
step: 270, loss: 0.005167805589735508
step: 280, loss: 0.0006163588259369135
step: 290, loss: 0.001609825063496828
step: 300, loss: 0.009951364248991013
step: 310, loss: 0.001161065767519176
step: 320, loss: 0.09416280686855316
step: 330, loss: 0.00016879320901352912
step: 340, loss: 0.0254934374243021
step: 350, loss: 0.00032802578061819077
epoch 9: dev_f1=0.7756097560975609, f1=0.745679012345679, best_f1=0.7531806615776081
step: 0, loss: 0.003222232684493065
step: 10, loss: 0.0008494241046719253
step: 20, loss: 0.001902384334243834
step: 30, loss: 0.005488850641995668
step: 40, loss: 0.13702131807804108
step: 50, loss: 0.0007148852455429733
step: 60, loss: 0.016995780169963837
step: 70, loss: 0.058679841458797455
step: 80, loss: 0.021892288699746132
step: 90, loss: 0.010499224066734314
step: 100, loss: 0.0002777934423647821
step: 110, loss: 0.0002495023945812136
step: 120, loss: 0.0003921331081073731
step: 130, loss: 0.007116571068763733
step: 140, loss: 0.0004384425701573491
step: 150, loss: 0.008453303948044777
step: 160, loss: 0.0010131055023521185
step: 170, loss: 0.0015499601140618324
step: 180, loss: 0.01006169430911541
step: 190, loss: 0.00016359776782337576
step: 200, loss: 0.12262943387031555
step: 210, loss: 6.294796185102314e-05
step: 220, loss: 0.001223703962750733
step: 230, loss: 0.001965475035831332
step: 240, loss: 0.00040609523421153426
step: 250, loss: 0.001439300482161343
step: 260, loss: 0.005671401973813772
step: 270, loss: 0.001641726354137063
step: 280, loss: 0.009459450840950012
step: 290, loss: 0.033149369060993195
step: 300, loss: 0.02995944768190384
step: 310, loss: 0.24412602186203003
step: 320, loss: 0.027041222900152206
step: 330, loss: 0.004660549573600292
step: 340, loss: 0.02849898859858513
step: 350, loss: 0.0014705646317452192
epoch 10: dev_f1=0.7835051546391752, f1=0.7672634271099745, best_f1=0.7672634271099745
step: 0, loss: 0.0005170814110897481
step: 10, loss: 0.004573445301502943
step: 20, loss: 0.00015558884479105473
step: 30, loss: 0.00032099659438245
step: 40, loss: 0.0019513434963300824
step: 50, loss: 0.0048036715015769005
step: 60, loss: 0.0016797399148344994
step: 70, loss: 0.00035351005499251187
step: 80, loss: 0.01363727729767561
step: 90, loss: 0.0010238582035526633
step: 100, loss: 0.0005809467402286828
step: 110, loss: 0.004005383234471083
step: 120, loss: 0.0025401839520782232
step: 130, loss: 0.000725140911526978
step: 140, loss: 0.00029767228988930583
step: 150, loss: 0.0008381829829886556
step: 160, loss: 0.0015165252843871713
step: 170, loss: 0.015840956941246986
step: 180, loss: 0.002593061188235879
step: 190, loss: 0.0013669813051819801
step: 200, loss: 0.00046415257384069264
step: 210, loss: 0.0034706334117799997
step: 220, loss: 0.0015193956205621362
step: 230, loss: 0.01639343984425068
step: 240, loss: 0.00027913961093872786
step: 250, loss: 0.013695316389203072
step: 260, loss: 0.002110630040988326
step: 270, loss: 0.00026365843950770795
step: 280, loss: 0.0010021522175520658
step: 290, loss: 0.0006902109598740935
step: 300, loss: 0.008130114525556564
step: 310, loss: 0.021422922611236572
step: 320, loss: 0.00104074040427804
step: 330, loss: 0.0029043895192444324
step: 340, loss: 0.0029560504481196404
step: 350, loss: 0.0003021410957444459
epoch 11: dev_f1=0.7783251231527093, f1=0.7447306791569087, best_f1=0.7672634271099745
step: 0, loss: 0.001414610305801034
step: 10, loss: 0.0005507985479198396
step: 20, loss: 0.00047326990170404315
step: 30, loss: 0.0008248674566857517
step: 40, loss: 0.0006095243152230978
step: 50, loss: 0.00023565780429635197
step: 60, loss: 0.0002911037008743733
step: 70, loss: 0.00011850134615087882
step: 80, loss: 0.0002940863196272403
step: 90, loss: 0.0006523270276375115
step: 100, loss: 0.06340282410383224
step: 110, loss: 0.009993528947234154
step: 120, loss: 0.0003362121933605522
step: 130, loss: 0.0006212525186128914
step: 140, loss: 0.0018631074344739318
step: 150, loss: 8.812265878077596e-05
step: 160, loss: 0.0036410873290151358
step: 170, loss: 0.0010534182656556368
step: 180, loss: 0.00016460538608953357
step: 190, loss: 0.0006261321832425892
step: 200, loss: 0.0011061513796448708
step: 210, loss: 0.003772033378481865
step: 220, loss: 0.0004912299918942153
step: 230, loss: 0.023310238495469093
step: 240, loss: 0.008398234844207764
step: 250, loss: 0.04818524792790413
step: 260, loss: 0.0002470683539286256
step: 270, loss: 0.0004609702154994011
step: 280, loss: 0.00023948152374941856
step: 290, loss: 0.002394553739577532
step: 300, loss: 0.003995117731392384
step: 310, loss: 0.016463370993733406
step: 320, loss: 0.00042508155456744134
step: 330, loss: 0.09031853824853897
step: 340, loss: 0.004627389833331108
step: 350, loss: 0.00044156695366837084
epoch 12: dev_f1=0.8042328042328043, f1=0.7454068241469816, best_f1=0.7454068241469816
step: 0, loss: 0.014847572892904282
step: 10, loss: 0.12730325758457184
step: 20, loss: 0.00012452408554963768
step: 30, loss: 0.007448733784258366
step: 40, loss: 0.0016970158321782947
step: 50, loss: 0.0005608811625279486
step: 60, loss: 0.0003466864873189479
step: 70, loss: 0.023450592532753944
step: 80, loss: 0.0004825893265660852
step: 90, loss: 0.0008708325331099331
step: 100, loss: 0.0019787303172051907
step: 110, loss: 0.0003819873963948339
step: 120, loss: 0.03513370826840401
step: 130, loss: 0.00461805472150445
step: 140, loss: 0.0007541062077507377
step: 150, loss: 0.0005576523835770786
step: 160, loss: 0.0007228016620501876
step: 170, loss: 0.005302085541188717
step: 180, loss: 0.035081896930933
step: 190, loss: 0.0006121331243775785
step: 200, loss: 0.01816730387508869
step: 210, loss: 0.0016590076265856624
step: 220, loss: 0.001498772413469851
step: 230, loss: 0.007215240504592657
step: 240, loss: 0.005738516338169575
step: 250, loss: 0.0033798839431256056
step: 260, loss: 0.04537292942404747
step: 270, loss: 0.00031870006932877004
step: 280, loss: 0.004574160557240248
step: 290, loss: 0.0003355986555106938
step: 300, loss: 0.0046515436843037605
step: 310, loss: 0.0016022222116589546
step: 320, loss: 0.0003676562337204814
step: 330, loss: 0.02769881673157215
step: 340, loss: 0.010454293340444565
step: 350, loss: 0.007913315668702126
epoch 13: dev_f1=0.772972972972973, f1=0.752577319587629, best_f1=0.7454068241469816
step: 0, loss: 0.005879633594304323
step: 10, loss: 0.014446709305047989
step: 20, loss: 0.000640009471680969
step: 30, loss: 0.005473933182656765
step: 40, loss: 0.010294132865965366
step: 50, loss: 0.058126725256443024
step: 60, loss: 0.0015480357687920332
step: 70, loss: 0.007632039021700621
step: 80, loss: 0.007639490999281406
step: 90, loss: 0.016444990411400795
step: 100, loss: 0.003037462942302227
step: 110, loss: 0.0004902309156022966
step: 120, loss: 0.00015186506789177656
step: 130, loss: 0.0007319432334043086
step: 140, loss: 0.004845551215112209
step: 150, loss: 0.06952135264873505
step: 160, loss: 0.003212457522749901
step: 170, loss: 0.000770239275880158
step: 180, loss: 0.00041043839883059263
step: 190, loss: 0.008427269756793976
step: 200, loss: 0.03434159606695175
step: 210, loss: 0.0032770356629043818
step: 220, loss: 0.0003516105061862618
step: 230, loss: 0.0002727805695030838
step: 240, loss: 0.0002432308392599225
step: 250, loss: 0.0003231993177905679
step: 260, loss: 0.0054031843319535255
step: 270, loss: 0.0004953910829499364
step: 280, loss: 0.00010667894093785435
step: 290, loss: 0.02165711112320423
step: 300, loss: 0.0009002640144899487
step: 310, loss: 0.00012589259131345898
step: 320, loss: 0.004670660477131605
step: 330, loss: 0.001523657701909542
step: 340, loss: 0.0006784084835089743
step: 350, loss: 0.027004459872841835
epoch 14: dev_f1=0.7837150127226464, f1=0.7347931873479318, best_f1=0.7454068241469816
step: 0, loss: 0.00016933957522269338
step: 10, loss: 0.00011700437607942149
step: 20, loss: 0.004153302870690823
step: 30, loss: 0.0031468262895941734
step: 40, loss: 0.005686609074473381
step: 50, loss: 0.0017147066537290812
step: 60, loss: 0.0006565405055880547
step: 70, loss: 0.0006152780842967331
step: 80, loss: 0.0007425142102874815
step: 90, loss: 0.00021577818552032113
step: 100, loss: 0.00027893774677067995
step: 110, loss: 0.006535266060382128
step: 120, loss: 8.316535240737721e-05
step: 130, loss: 0.0038408387918025255
step: 140, loss: 0.00033470275229774415
step: 150, loss: 0.00329166604205966
step: 160, loss: 0.005843046586960554
step: 170, loss: 7.966616249177605e-05
step: 180, loss: 0.0022621983662247658
step: 190, loss: 0.00033230165718123317
step: 200, loss: 0.0020207457710057497
step: 210, loss: 0.00017971449415199459
step: 220, loss: 0.0038621167186647654
step: 230, loss: 0.0009325058781541884
step: 240, loss: 0.00019095410243608057
step: 250, loss: 0.00017650345398578793
step: 260, loss: 0.00017964874859899282
step: 270, loss: 0.0001711151417111978
step: 280, loss: 0.008919146843254566
step: 290, loss: 0.0018735998310148716
step: 300, loss: 0.003732826095074415
step: 310, loss: 0.0010347893694415689
step: 320, loss: 0.00582087691873312
step: 330, loss: 0.00033051901846192777
step: 340, loss: 0.00024613869027234614
step: 350, loss: 0.006784538738429546
epoch 15: dev_f1=0.7772020725388602, f1=0.7409200968523002, best_f1=0.7454068241469816
step: 0, loss: 0.00023779245384503156
step: 10, loss: 0.05591287836432457
step: 20, loss: 0.005348911043256521
step: 30, loss: 0.0014684029156342149
step: 40, loss: 0.0004256578686181456
step: 50, loss: 0.010023216716945171
step: 60, loss: 0.0002121743600582704
step: 70, loss: 0.0015716934576630592
step: 80, loss: 0.010609185323119164
step: 90, loss: 8.467326551908627e-05
step: 100, loss: 0.00012611848069354892
step: 110, loss: 0.00030033811344765127
step: 120, loss: 6.956019205972552e-05
step: 130, loss: 0.00039161101449280977
step: 140, loss: 0.0006623843219131231
step: 150, loss: 6.433000089600682e-05
step: 160, loss: 0.0004650968767236918
step: 170, loss: 0.00010658911924110726
step: 180, loss: 7.581574027426541e-05
step: 190, loss: 0.0003845841274596751
step: 200, loss: 0.0069832331500947475
step: 210, loss: 0.004038176499307156
step: 220, loss: 0.006191353779286146
step: 230, loss: 0.0003579053736757487
step: 240, loss: 0.06413093209266663
step: 250, loss: 0.00019873947894666344
step: 260, loss: 0.00040522561175748706
step: 270, loss: 0.0001062813971657306
step: 280, loss: 0.006737386807799339
step: 290, loss: 0.00020954619685653597
step: 300, loss: 0.004053729586303234
step: 310, loss: 0.0011856394121423364
step: 320, loss: 0.0009055311675183475
step: 330, loss: 0.0008773818844929338
step: 340, loss: 0.0003467254573479295
step: 350, loss: 0.10659356415271759
epoch 16: dev_f1=0.7989821882951653, f1=0.754257907542579, best_f1=0.7454068241469816
step: 0, loss: 0.0011223794426769018
step: 10, loss: 0.0032624192535877228
step: 20, loss: 0.0035686956252902746
step: 30, loss: 0.00022272525529842824
step: 40, loss: 0.00044841840281151235
step: 50, loss: 5.4286741942632943e-05
step: 60, loss: 0.004743232391774654
step: 70, loss: 0.00038726491038687527
step: 80, loss: 6.993729766691104e-05
step: 90, loss: 0.0003152174467686564
step: 100, loss: 0.0006939948070794344
step: 110, loss: 0.00048559095012024045
step: 120, loss: 0.00013149275036994368
step: 130, loss: 0.00022360956063494086
step: 140, loss: 0.0002508301695343107
step: 150, loss: 0.0009216648759320378
step: 160, loss: 3.573458161554299e-05
step: 170, loss: 0.0003221894148737192
step: 180, loss: 0.0019011320546269417
step: 190, loss: 0.00011086062295362353
step: 200, loss: 0.06528370827436447
step: 210, loss: 7.508724229410291e-05
step: 220, loss: 0.0002590030198916793
step: 230, loss: 0.09565283358097076
step: 240, loss: 0.002315736375749111
step: 250, loss: 0.0002597720886114985
step: 260, loss: 0.000750475621316582
step: 270, loss: 0.0014815706526860595
step: 280, loss: 0.025454165413975716
step: 290, loss: 0.03977864608168602
step: 300, loss: 0.00015060481382533908
step: 310, loss: 0.00020025207777507603
step: 320, loss: 0.00011335700401104987
step: 330, loss: 0.0033568087965250015
step: 340, loss: 0.00013438450696412474
step: 350, loss: 6.0088230384280905e-05
epoch 17: dev_f1=0.8, f1=0.7470449172576832, best_f1=0.7454068241469816
step: 0, loss: 0.0005399009678512812
step: 10, loss: 0.0014951444463804364
step: 20, loss: 0.00017635342373978347
step: 30, loss: 0.00010155925701837987
step: 40, loss: 0.0006537523586302996
step: 50, loss: 0.0057165008038282394
step: 60, loss: 0.003136856248602271
step: 70, loss: 0.0005162513698451221
step: 80, loss: 0.00018971740792039782
step: 90, loss: 0.0025993669405579567
step: 100, loss: 0.004045509733259678
step: 110, loss: 0.000124216967378743
step: 120, loss: 0.0030853156931698322
step: 130, loss: 4.4240878196433187e-05
step: 140, loss: 0.0009226514375768602
step: 150, loss: 8.579856512369588e-05
step: 160, loss: 0.00027041014982387424
step: 170, loss: 0.0001977076317416504
step: 180, loss: 0.00010375657438999042
step: 190, loss: 0.0006677041528746486
step: 200, loss: 5.2320025133667514e-05
step: 210, loss: 0.0004013525031041354
step: 220, loss: 0.011055566370487213
step: 230, loss: 0.00039893033681437373
step: 240, loss: 0.0005988562479615211
step: 250, loss: 9.741050598677248e-05
step: 260, loss: 0.004443178419023752
step: 270, loss: 0.00013704616867471486
step: 280, loss: 0.04929478466510773
step: 290, loss: 0.0014879241352900863
step: 300, loss: 0.00010434125579195097
step: 310, loss: 9.580151527188718e-05
step: 320, loss: 0.0037031848914921284
step: 330, loss: 0.00023976511147338897
step: 340, loss: 0.00020086776930838823
step: 350, loss: 0.00013922466314397752
epoch 18: dev_f1=0.7774936061381074, f1=0.7445255474452555, best_f1=0.7454068241469816
step: 0, loss: 0.0001967529533430934
step: 10, loss: 6.67801359668374e-05
step: 20, loss: 0.00013779599976260215
step: 30, loss: 0.00017891079187393188
step: 40, loss: 0.0007453290745615959
step: 50, loss: 3.624178862082772e-05
step: 60, loss: 0.00040074001299217343
step: 70, loss: 0.0002958767581731081
step: 80, loss: 0.00043495677527971566
step: 90, loss: 9.182850044453517e-05
step: 100, loss: 0.00011076015653088689
step: 110, loss: 0.0021125294733792543
step: 120, loss: 8.565484313294291e-05
step: 130, loss: 6.265405681915581e-05
step: 140, loss: 0.0002736584283411503
step: 150, loss: 5.4042400734033436e-05
step: 160, loss: 6.335266516543925e-05
step: 170, loss: 0.00027494519599713385
step: 180, loss: 0.00011744094081223011
step: 190, loss: 2.2626869395026006e-05
step: 200, loss: 0.0001223292638314888
step: 210, loss: 0.0014121441636234522
step: 220, loss: 0.0007975168991833925
step: 230, loss: 3.360051414347254e-05
step: 240, loss: 0.00017374353774357587
step: 250, loss: 3.449353243922815e-05
step: 260, loss: 0.00012618565233424306
step: 270, loss: 0.000462059339042753
step: 280, loss: 0.006737765856087208
step: 290, loss: 4.721636651083827e-05
step: 300, loss: 8.601897570770234e-05
step: 310, loss: 0.0005883554113097489
step: 320, loss: 0.00012796020018868148
step: 330, loss: 8.843398245517164e-05
step: 340, loss: 0.0002421202079858631
step: 350, loss: 0.00029400712810456753
epoch 19: dev_f1=0.7804878048780489, f1=0.7305699481865284, best_f1=0.7454068241469816
step: 0, loss: 0.0010914928279817104
step: 10, loss: 0.00017644956824369729
step: 20, loss: 0.0004824075731448829
step: 30, loss: 0.00013763601600658149
step: 40, loss: 0.0001304455945501104
step: 50, loss: 5.8152010751655325e-05
step: 60, loss: 0.052383873611688614
step: 70, loss: 0.004418178461492062
step: 80, loss: 8.055430225795135e-05
step: 90, loss: 0.00028005894273519516
step: 100, loss: 0.00017090432811528444
step: 110, loss: 5.723495996790007e-05
step: 120, loss: 0.000144060657476075
step: 130, loss: 6.664003012701869e-05
step: 140, loss: 0.00016057379252742976
step: 150, loss: 0.0002254092978546396
step: 160, loss: 5.051858170190826e-05
step: 170, loss: 0.00019574834732338786
step: 180, loss: 9.239864448318258e-05
step: 190, loss: 0.0018466673791408539
step: 200, loss: 0.0007628737948834896
step: 210, loss: 7.947906851768494e-05
step: 220, loss: 0.027998005971312523
step: 230, loss: 0.0006703431135974824
step: 240, loss: 3.234471660107374e-05
step: 250, loss: 0.0008208695217035711
step: 260, loss: 4.149578671786003e-05
step: 270, loss: 0.00327002233825624
step: 280, loss: 0.012352528981864452
step: 290, loss: 0.00043811078649014235
step: 300, loss: 0.00013245934678707272
step: 310, loss: 0.00018533291586209089
step: 320, loss: 0.0007167515577748418
step: 330, loss: 0.002977903001010418
step: 340, loss: 5.6929504353320226e-05
step: 350, loss: 0.00016526573745068163
epoch 20: dev_f1=0.7804878048780489, f1=0.730077120822622, best_f1=0.7454068241469816
